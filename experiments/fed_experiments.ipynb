{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc3p7TAvGYSh"
   },
   "source": [
    "# Testing Federation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21289,
     "status": "ok",
     "timestamp": 1734443586618,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "xOvTXt_pXjcJ",
    "outputId": "e0a50c50-1b1c-44fd-9ea1-c3a0e7dce044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EP2MLr6MJ82-"
   },
   "source": [
    "## Setup + Load Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aLkPaxYTRJk"
   },
   "source": [
    "Since Google Colab has pre-loaded packages, we have found that using our `requirements.txt` file can cause conflicts if we do not have a clean slate. However, running our file locally does require the installations in the file, so we present two options. Option 1 is for running this notebook on Google Colab (recommended) and Option 2 is for running this notebook locally (not recommended unless you have access to nice GPUs). The demo code should not take excessively long on a local machine, but it is much fast on Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvPAGIHHTvt0"
   },
   "source": [
    "### Option 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4SS3cLETxuk"
   },
   "source": [
    "Assuming you have already mounted the git repo or uploaded the files to Google Drive, run the following two commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6585,
     "status": "ok",
     "timestamp": 1734443602438,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "C6uh2i1JT2zq",
    "outputId": "a8d4c5a4-cfb3-4ec6-e66b-45e116bc4409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting medmnist\n",
      "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.26.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.5.2)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.24.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.66.6)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (11.0.0)\n",
      "Collecting fire (from medmnist)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.20.1+cu121)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.13.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.4.2)\n",
      "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.36.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2024.9.20)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (24.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->medmnist) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medmnist) (3.0.2)\n",
      "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=53cda56d905fb41a3a758a53ccb8c694a02b4ec868a814ea4484ea6a05433ea1\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
      "Successfully built fire\n",
      "Installing collected packages: fire, medmnist\n",
      "Successfully installed fire-0.7.0 medmnist-3.0.2\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install medmnist\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Crpn12RT-4F"
   },
   "source": [
    "These should (in theory) provide all the necessary packages for the notebook given the pre-installed packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LW2FrWPTUH1A"
   },
   "source": [
    "### Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXx3wKFxUJv9"
   },
   "source": [
    "On a local machine (preferably in a virtual environment) within the project folder, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 996,
     "status": "ok",
     "timestamp": 1734308654257,
     "user": {
      "displayName": "Adam Jovine",
      "userId": "07868206669247318441"
     },
     "user_tz": 300
    },
    "id": "NxwM2mzNUJWL",
    "outputId": "17ebd6f6-f4b2-4175-d08b-1518d03fffde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3t6Z3QeKM_7"
   },
   "source": [
    "### For demo/normal use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c-MO3vaKD88"
   },
   "source": [
    "Ensure you are in the `src` folder of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1734393599404,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "xqsv0C5u6eNh",
    "outputId": "61a9b6c0-a609-4c05-e3ac-a046ed3d1927"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 566,
     "status": "ok",
     "timestamp": 1734393569751,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "TJ-718zvWjbC",
    "outputId": "f0633a46-3945-4413-98b9-2edccc5c84f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'CAP_The-Federation_asj53_ctc92/src'\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd OneFL/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1734443603091,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "hwL8JJoeZD64",
    "outputId": "6ff9947b-70b0-4e9e-8fa4-b2ac8e9b920d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/.shortcut-targets-by-id/1D1SxO5JoM4fHICMtrKkdp70SChR3NCos/CS4701_FL\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/OneFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1734443605329,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "MM3Ru_PY2xKZ"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 15311,
     "status": "ok",
     "timestamp": 1734443620767,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "PUv7vaoRWt-e"
   },
   "outputs": [],
   "source": [
    "import fedisca\n",
    "import train_loc\n",
    "from train_loc import LocalTrainer\n",
    "from resnet import ResNet50, ResNet18\n",
    "import medmnist\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnqGSRKeGS3y"
   },
   "source": [
    "## Testing training of single local classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSVq3cbSGqWy"
   },
   "source": [
    "We use the entire dataset (in this case breastmnist) to train and validate a single local classifier. breastmnist is a small dataset with binary labels, so it is ideal for speed and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1832,
     "status": "ok",
     "timestamp": 1734309533594,
     "user": {
      "displayName": "Adam Jovine",
      "userId": "07868206669247318441"
     },
     "user_tz": 300
    },
    "id": "0my8a4CGdDt2",
    "outputId": "7a94991e-4aa8-478c-ef84-6888fe2eade9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://zenodo.org/records/10519652/files/breastmnist.npz?download=1 to /root/.medmnist/breastmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560k/560k [00:00<00:00, 742kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /root/.medmnist/breastmnist.npz\n",
      "Using downloaded and verified file: /root/.medmnist/breastmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_flag = 'breastmnist'\n",
    "download = True\n",
    "\n",
    "info = medmnist.INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "train_dataset = DataClass(split='train', download=download)\n",
    "val_dataset = DataClass(split='val', download=download)\n",
    "test_dataset = DataClass(split='test', download=download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvARpW-xG1yA"
   },
   "outputs": [],
   "source": [
    "aug_list = []\n",
    "aug_list.append(transforms.RandomCrop(28, padding=4))\n",
    "aug_list.append(transforms.RandomHorizontalFlip())\n",
    "preprocess_list = [transforms.ToTensor(), transforms.Normalize(mean=[.5], std=[.5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhKloXStdZw1"
   },
   "outputs": [],
   "source": [
    "loc_trainer_test = LocalTrainer(ResNet18(in_channels=1, num_classes=2), DataClass, \"./loc_model_weights\", n_classes, epochs=100,  aug=aug_list, preprocess=preprocess_list, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70848,
     "status": "ok",
     "timestamp": 1734309743155,
     "user": {
      "displayName": "Adam Jovine",
      "userId": "07868206669247318441"
     },
     "user_tz": 300
    },
    "id": "fEU1GcLQp_ex",
    "outputId": "27251d90-666e-4632-b02e-55d0d08a0fd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch num: 1 \n",
      "Train loss: 0.044679 \n",
      "Val loss: 0.012910 \n",
      "Val acc: 0.294872\n",
      "Val balanced acc: 0.502506\n",
      "Epoch num: 2 \n",
      "Train loss: 0.024906 \n",
      "Val loss: 0.169277 \n",
      "Val acc: 0.384615\n",
      "Val balanced acc: 0.548872\n",
      "Epoch num: 3 \n",
      "Train loss: 0.021689 \n",
      "Val loss: 0.027551 \n",
      "Val acc: 0.730769\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 4 \n",
      "Train loss: 0.022055 \n",
      "Val loss: 0.016711 \n",
      "Val acc: 0.730769\n",
      "Val balanced acc: 0.515038\n",
      "Epoch num: 5 \n",
      "Train loss: 0.020043 \n",
      "Val loss: 0.007733 \n",
      "Val acc: 0.717949\n",
      "Val balanced acc: 0.686717\n",
      "Epoch num: 6 \n",
      "Train loss: 0.019952 \n",
      "Val loss: 0.007710 \n",
      "Val acc: 0.679487\n",
      "Val balanced acc: 0.494987\n",
      "Epoch num: 7 \n",
      "Train loss: 0.018884 \n",
      "Val loss: 0.006889 \n",
      "Val acc: 0.782051\n",
      "Val balanced acc: 0.625313\n",
      "Epoch num: 8 \n",
      "Train loss: 0.019230 \n",
      "Val loss: 0.006835 \n",
      "Val acc: 0.743590\n",
      "Val balanced acc: 0.614035\n",
      "Epoch num: 9 \n",
      "Train loss: 0.018679 \n",
      "Val loss: 0.006311 \n",
      "Val acc: 0.769231\n",
      "Val balanced acc: 0.691729\n",
      "Epoch num: 10 \n",
      "Train loss: 0.017160 \n",
      "Val loss: 0.006435 \n",
      "Val acc: 0.769231\n",
      "Val balanced acc: 0.691729\n",
      "Epoch num: 11 \n",
      "Train loss: 0.017507 \n",
      "Val loss: 0.006622 \n",
      "Val acc: 0.833333\n",
      "Val balanced acc: 0.690476\n",
      "Epoch num: 12 \n",
      "Train loss: 0.017004 \n",
      "Val loss: 0.005189 \n",
      "Val acc: 0.820513\n",
      "Val balanced acc: 0.741855\n",
      "Epoch num: 13 \n",
      "Train loss: 0.016662 \n",
      "Val loss: 0.007151 \n",
      "Val acc: 0.743590\n",
      "Val balanced acc: 0.553885\n",
      "Epoch num: 14 \n",
      "Train loss: 0.014917 \n",
      "Val loss: 0.028491 \n",
      "Val acc: 0.730769\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 15 \n",
      "Train loss: 0.016289 \n",
      "Val loss: 0.005649 \n",
      "Val acc: 0.833333\n",
      "Val balanced acc: 0.765664\n",
      "Epoch num: 16 \n",
      "Train loss: 0.015480 \n",
      "Val loss: 0.005676 \n",
      "Val acc: 0.833333\n",
      "Val balanced acc: 0.705514\n",
      "Epoch num: 17 \n",
      "Train loss: 0.016748 \n",
      "Val loss: 0.008656 \n",
      "Val acc: 0.794872\n",
      "Val balanced acc: 0.634085\n",
      "Epoch num: 18 \n",
      "Train loss: 0.016134 \n",
      "Val loss: 0.022804 \n",
      "Val acc: 0.730769\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 19 \n",
      "Train loss: 0.015813 \n",
      "Val loss: 0.014982 \n",
      "Val acc: 0.743590\n",
      "Val balanced acc: 0.523810\n",
      "Epoch num: 20 \n",
      "Train loss: 0.014144 \n",
      "Val loss: 0.005586 \n",
      "Val acc: 0.833333\n",
      "Val balanced acc: 0.705514\n",
      "Epoch num: 21 \n",
      "Train loss: 0.015240 \n",
      "Val loss: 0.006364 \n",
      "Val acc: 0.858974\n",
      "Val balanced acc: 0.768170\n",
      "Epoch num: 22 \n",
      "Train loss: 0.015687 \n",
      "Val loss: 0.005742 \n",
      "Val acc: 0.833333\n",
      "Val balanced acc: 0.720551\n",
      "Epoch num: 23 \n",
      "Train loss: 0.015249 \n",
      "Val loss: 0.005725 \n",
      "Val acc: 0.820513\n",
      "Val balanced acc: 0.696742\n",
      "Epoch num: 24 \n",
      "Train loss: 0.013579 \n",
      "Val loss: 0.005549 \n",
      "Val acc: 0.820513\n",
      "Val balanced acc: 0.696742\n",
      "Epoch num: 25 \n",
      "Train loss: 0.013597 \n",
      "Val loss: 0.005374 \n",
      "Val acc: 0.807692\n",
      "Val balanced acc: 0.703008\n",
      "Epoch num: 26 \n",
      "Train loss: 0.012312 \n",
      "Val loss: 0.005195 \n",
      "Val acc: 0.807692\n",
      "Val balanced acc: 0.703008\n",
      "Epoch num: 27 \n",
      "Train loss: 0.011960 \n",
      "Val loss: 0.005001 \n",
      "Val acc: 0.833333\n",
      "Val balanced acc: 0.735589\n",
      "Epoch num: 28 \n",
      "Train loss: 0.012188 \n",
      "Val loss: 0.004904 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.744361\n",
      "Epoch num: 29 \n",
      "Train loss: 0.011803 \n",
      "Val loss: 0.004738 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.744361\n",
      "Epoch num: 30 \n",
      "Train loss: 0.012227 \n",
      "Val loss: 0.004566 \n",
      "Val acc: 0.820513\n",
      "Val balanced acc: 0.726817\n",
      "Epoch num: 31 \n",
      "Train loss: 0.012018 \n",
      "Val loss: 0.004353 \n",
      "Val acc: 0.858974\n",
      "Val balanced acc: 0.768170\n",
      "Epoch num: 32 \n",
      "Train loss: 0.011254 \n",
      "Val loss: 0.004361 \n",
      "Val acc: 0.858974\n",
      "Val balanced acc: 0.768170\n",
      "Epoch num: 33 \n",
      "Train loss: 0.010600 \n",
      "Val loss: 0.004763 \n",
      "Val acc: 0.858974\n",
      "Val balanced acc: 0.753133\n",
      "Epoch num: 34 \n",
      "Train loss: 0.010459 \n",
      "Val loss: 0.004584 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.774436\n",
      "Epoch num: 35 \n",
      "Train loss: 0.010471 \n",
      "Val loss: 0.004303 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.774436\n",
      "Epoch num: 36 \n",
      "Train loss: 0.010702 \n",
      "Val loss: 0.004241 \n",
      "Val acc: 0.858974\n",
      "Val balanced acc: 0.783208\n",
      "Epoch num: 37 \n",
      "Train loss: 0.009542 \n",
      "Val loss: 0.004388 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.789474\n",
      "Epoch num: 38 \n",
      "Train loss: 0.009515 \n",
      "Val loss: 0.004367 \n",
      "Val acc: 0.833333\n",
      "Val balanced acc: 0.780702\n",
      "Epoch num: 39 \n",
      "Train loss: 0.010974 \n",
      "Val loss: 0.004475 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.744361\n",
      "Epoch num: 40 \n",
      "Train loss: 0.010035 \n",
      "Val loss: 0.004747 \n",
      "Val acc: 0.858974\n",
      "Val balanced acc: 0.753133\n",
      "Epoch num: 41 \n",
      "Train loss: 0.010160 \n",
      "Val loss: 0.004664 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.744361\n",
      "Epoch num: 42 \n",
      "Train loss: 0.009960 \n",
      "Val loss: 0.004956 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.744361\n",
      "Epoch num: 43 \n",
      "Train loss: 0.009250 \n",
      "Val loss: 0.005107 \n",
      "Val acc: 0.858974\n",
      "Val balanced acc: 0.753133\n",
      "Epoch num: 44 \n",
      "Train loss: 0.008826 \n",
      "Val loss: 0.004385 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.774436\n",
      "Epoch num: 45 \n",
      "Train loss: 0.008840 \n",
      "Val loss: 0.004325 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.789474\n",
      "Epoch num: 46 \n",
      "Train loss: 0.008133 \n",
      "Val loss: 0.004760 \n",
      "Val acc: 0.820513\n",
      "Val balanced acc: 0.741855\n",
      "Epoch num: 47 \n",
      "Train loss: 0.008614 \n",
      "Val loss: 0.004555 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.774436\n",
      "Epoch num: 48 \n",
      "Train loss: 0.008224 \n",
      "Val loss: 0.004517 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.774436\n",
      "Epoch num: 49 \n",
      "Train loss: 0.008197 \n",
      "Val loss: 0.004408 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.774436\n",
      "Epoch num: 50 \n",
      "Train loss: 0.007615 \n",
      "Val loss: 0.004340 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 51 \n",
      "Train loss: 0.006989 \n",
      "Val loss: 0.004329 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 52 \n",
      "Train loss: 0.008406 \n",
      "Val loss: 0.004329 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 53 \n",
      "Train loss: 0.007936 \n",
      "Val loss: 0.004371 \n",
      "Val acc: 0.833333\n",
      "Val balanced acc: 0.750627\n",
      "Epoch num: 54 \n",
      "Train loss: 0.007501 \n",
      "Val loss: 0.004402 \n",
      "Val acc: 0.833333\n",
      "Val balanced acc: 0.750627\n",
      "Epoch num: 55 \n",
      "Train loss: 0.007596 \n",
      "Val loss: 0.004482 \n",
      "Val acc: 0.858974\n",
      "Val balanced acc: 0.798246\n",
      "Epoch num: 56 \n",
      "Train loss: 0.007099 \n",
      "Val loss: 0.004516 \n",
      "Val acc: 0.858974\n",
      "Val balanced acc: 0.798246\n",
      "Epoch num: 57 \n",
      "Train loss: 0.007091 \n",
      "Val loss: 0.004497 \n",
      "Val acc: 0.871795\n",
      "Val balanced acc: 0.807018\n",
      "Epoch num: 58 \n",
      "Train loss: 0.007508 \n",
      "Val loss: 0.004461 \n",
      "Val acc: 0.858974\n",
      "Val balanced acc: 0.783208\n",
      "Epoch num: 59 \n",
      "Train loss: 0.007966 \n",
      "Val loss: 0.004430 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 60 \n",
      "Train loss: 0.007760 \n",
      "Val loss: 0.004433 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 61 \n",
      "Train loss: 0.007337 \n",
      "Val loss: 0.004485 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 62 \n",
      "Train loss: 0.007683 \n",
      "Val loss: 0.004489 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 63 \n",
      "Train loss: 0.008054 \n",
      "Val loss: 0.004498 \n",
      "Val acc: 0.858974\n",
      "Val balanced acc: 0.768170\n",
      "Epoch num: 64 \n",
      "Train loss: 0.009080 \n",
      "Val loss: 0.004583 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 65 \n",
      "Train loss: 0.006298 \n",
      "Val loss: 0.004525 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 66 \n",
      "Train loss: 0.007766 \n",
      "Val loss: 0.004468 \n",
      "Val acc: 0.858974\n",
      "Val balanced acc: 0.768170\n",
      "Epoch num: 67 \n",
      "Train loss: 0.006940 \n",
      "Val loss: 0.004421 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 68 \n",
      "Train loss: 0.007623 \n",
      "Val loss: 0.004438 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 69 \n",
      "Train loss: 0.008091 \n",
      "Val loss: 0.004475 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 70 \n",
      "Train loss: 0.006743 \n",
      "Val loss: 0.004467 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 71 \n",
      "Train loss: 0.007650 \n",
      "Val loss: 0.004496 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 72 \n",
      "Train loss: 0.008604 \n",
      "Val loss: 0.004571 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 73 \n",
      "Train loss: 0.007853 \n",
      "Val loss: 0.004501 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 74 \n",
      "Train loss: 0.006973 \n",
      "Val loss: 0.004463 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 75 \n",
      "Train loss: 0.007215 \n",
      "Val loss: 0.004488 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 76 \n",
      "Train loss: 0.007837 \n",
      "Val loss: 0.004486 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 77 \n",
      "Train loss: 0.007701 \n",
      "Val loss: 0.004489 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 78 \n",
      "Train loss: 0.008324 \n",
      "Val loss: 0.004460 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 79 \n",
      "Train loss: 0.008896 \n",
      "Val loss: 0.004464 \n",
      "Val acc: 0.858974\n",
      "Val balanced acc: 0.783208\n",
      "Epoch num: 80 \n",
      "Train loss: 0.007638 \n",
      "Val loss: 0.004551 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 81 \n",
      "Train loss: 0.007842 \n",
      "Val loss: 0.004540 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 82 \n",
      "Train loss: 0.007180 \n",
      "Val loss: 0.004508 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 83 \n",
      "Train loss: 0.006573 \n",
      "Val loss: 0.004525 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 84 \n",
      "Train loss: 0.008491 \n",
      "Val loss: 0.004509 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 85 \n",
      "Train loss: 0.008022 \n",
      "Val loss: 0.004520 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 86 \n",
      "Train loss: 0.008353 \n",
      "Val loss: 0.004529 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.774436\n",
      "Epoch num: 87 \n",
      "Train loss: 0.008706 \n",
      "Val loss: 0.004474 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 88 \n",
      "Train loss: 0.006977 \n",
      "Val loss: 0.004476 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 89 \n",
      "Train loss: 0.008338 \n",
      "Val loss: 0.004534 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 90 \n",
      "Train loss: 0.006752 \n",
      "Val loss: 0.004525 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 91 \n",
      "Train loss: 0.007237 \n",
      "Val loss: 0.004475 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 92 \n",
      "Train loss: 0.007648 \n",
      "Val loss: 0.004439 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 93 \n",
      "Train loss: 0.006630 \n",
      "Val loss: 0.004442 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 94 \n",
      "Train loss: 0.009288 \n",
      "Val loss: 0.004438 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 95 \n",
      "Train loss: 0.007975 \n",
      "Val loss: 0.004492 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 96 \n",
      "Train loss: 0.006813 \n",
      "Val loss: 0.004478 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 97 \n",
      "Train loss: 0.007194 \n",
      "Val loss: 0.004445 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 98 \n",
      "Train loss: 0.007416 \n",
      "Val loss: 0.004456 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 99 \n",
      "Train loss: 0.008351 \n",
      "Val loss: 0.004429 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "Epoch num: 100 \n",
      "Train loss: 0.008027 \n",
      "Val loss: 0.004464 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.759398\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "loc_trainer_test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNcBkh-ba3zO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1734309743268,
     "user": {
      "displayName": "Adam Jovine",
      "userId": "07868206669247318441"
     },
     "user_tz": 300
    },
    "id": "AjJxukYYan7Q",
    "outputId": "20391503-a4d0-4844-be6a-1c02b765a701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0059\n",
      "Test Accuracy: 0.8397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/CAP_The-Federation_asj53_ctc92/src/train_loc.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.state_dict = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "# Load the best performing model (assuming it was saved during training).\n",
    "loc_trainer_test.load_model(\"./loc_model_weights/best.pth\") # Replace with the actual path if different\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "test_data = DataClass(split='test', transform=test_transforms, download=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "# Evaluate the model on the test dataset.\n",
    "test_loss, test_acc, test_bacc = loc_trainer_test.validate(test_loader)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTu_0MbdUqT0"
   },
   "source": [
    "## Testing data partition for federation sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qWmKRIFXsHZ"
   },
   "source": [
    "We test the data partition method `utils.fl_partition` by verifying the splits are visually i.i.d (class balance matches, partition size is equal) when the i.i.d partition is specified and visually non i.i.d when it is not supposed to be. We use `pathmnist`, which has 9 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14428,
     "status": "ok",
     "timestamp": 1734309757695,
     "user": {
      "displayName": "Adam Jovine",
      "userId": "07868206669247318441"
     },
     "user_tz": 300
    },
    "id": "1MIjKAXpR-HE",
    "outputId": "90f83bae-8305-4563-8ce6-cbdde5c26c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://zenodo.org/records/10519652/files/pathmnist.npz?download=1 to ./pathmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206M/206M [00:11<00:00, 18.2MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset PathMNIST of size 28 (pathmnist)\n",
       "    Number of datapoints: 89996\n",
       "    Root location: ./\n",
       "    Split: train\n",
       "    Task: multi-class\n",
       "    Number of channels: 3\n",
       "    Meaning of labels: {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n",
       "    Number of samples: {'train': 89996, 'val': 10004, 'test': 7180}\n",
       "    Description: The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.\n",
       "    License: CC BY 4.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_flag = 'pathmnist'\n",
    "download = True\n",
    "\n",
    "info = medmnist.INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "train_dataset = DataClass(root=\"./\", split='train', download=download)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjRLCF9hYzny"
   },
   "source": [
    "We plot the distribution of labels of the original dataset. We use chatgpt to assist in the coding of this visualization using the prompt:\n",
    "\n",
    "\"Given a list of labels, plot the distribution of the labels using python and matplotlib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1734309757981,
     "user": {
      "displayName": "Adam Jovine",
      "userId": "07868206669247318441"
     },
     "user_tz": 300
    },
    "id": "m1iUEWuZYf-9",
    "outputId": "574082fa-8de0-421c-b295-923760051cae"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Example list of labels\n",
    "labels = train_dataset.labels.flatten().tolist()\n",
    "\n",
    "# Count the frequency of each label\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "# Extract labels and their counts\n",
    "categories = list(label_counts.keys())\n",
    "counts = list(label_counts.values())\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(categories, counts, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Labels', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Label Distribution', fontsize=16)\n",
    "plt.xticks(ticks = categories, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gbqW8NOZ9_g"
   },
   "source": [
    "Next, we test an i.i.d partition into 5 datasets and check if each of them display a similar distribution over the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWozNalkaXOs"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yCxmyjSaa1Y"
   },
   "outputs": [],
   "source": [
    "part_dir, list_name, split_ids = utils.fl_partition(train_dataset, 5, 9, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1734309759397,
     "user": {
      "displayName": "Adam Jovine",
      "userId": "07868206669247318441"
     },
     "user_tz": 300
    },
    "id": "nOq6CNlScbf5",
    "outputId": "079b26fb-1f1f-48a5-8f45-187d64bb78bc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5), sharey=True)\n",
    "\n",
    "for list_ind, ax in enumerate(axes):\n",
    "    f_name = list_name[list_ind]\n",
    "    data = np.load(f_name)\n",
    "    labels = data['train_labels'].flatten().tolist()\n",
    "    label_counts = Counter(labels)\n",
    "    categories = list(label_counts.keys())\n",
    "    counts = list(label_counts.values())\n",
    "\n",
    "    # Plot on the current subplot\n",
    "    ax.bar(categories, counts, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f\"Dataset {list_ind + 1}\", fontsize=14)\n",
    "    ax.set_xlabel('Labels', fontsize=12)\n",
    "    ax.set_xticks(ticks=categories)\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "fig.supylabel('Count', fontsize=14)\n",
    "fig.suptitle('Label distribution for iid partition', fontsize=16)\n",
    "fig.subplots_adjust(left=0.05, right=0.95, top=0.85, bottom=0.1, wspace=0.3)\n",
    "#plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHLA-7V1m0tK"
   },
   "source": [
    "Next, we want to make sure the dirichlet non-i.i.d splits are visually heterogenous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvL11M-Unmpq"
   },
   "outputs": [],
   "source": [
    "part_dir, list_name, split_ids = utils.fl_partition(train_dataset, 5, 10, beta=0.5, iid=False, min_size=1000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "Jkrlnh6Jon4b",
    "outputId": "f36780d2-647f-4315-8958-38e11c2bad36"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5), sharey=True)\n",
    "\n",
    "for list_ind, ax in enumerate(axes):\n",
    "    f_name = list_name[list_ind]\n",
    "    data = np.load(f_name)\n",
    "    labels = data['train_labels'].flatten().tolist()\n",
    "    label_counts = Counter(labels)\n",
    "    categories = list(label_counts.keys())\n",
    "    counts = list(label_counts.values())\n",
    "\n",
    "    # Plot on the current subplot\n",
    "    ax.bar(categories, counts, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f\"Dataset {list_ind + 1}\", fontsize=14)\n",
    "    ax.set_xlabel('Labels', fontsize=12)\n",
    "    ax.set_xticks(ticks=categories)\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "fig.supylabel('Count', fontsize=14)\n",
    "fig.suptitle('Label distribution for dirichlet partition', fontsize=16)\n",
    "fig.subplots_adjust(left=0.05, right=0.95, top=0.85, bottom=0.1, wspace=0.3)\n",
    "#plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxTyFXVgZc_L"
   },
   "source": [
    "## Testing FedISCA on i.i.d. data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgHhgHk-Zwl4"
   },
   "source": [
    "We need to test 3 different steps.\n",
    "\n",
    "1. Partition data in iid fashion\n",
    "2. Train local models on each partition of the data\n",
    "3. Federate local models with FedISCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kw1cbOjamFq"
   },
   "source": [
    "1. Partition iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2779,
     "status": "ok",
     "timestamp": 1734391839210,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "4r1avR6sfv1C",
    "outputId": "3db1dd1b-c8ad-4377-daef-30703d628b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./bloodmnist.npz\n",
      "Using downloaded and verified file: ./bloodmnist.npz\n",
      "Using downloaded and verified file: ./bloodmnist.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset BloodMNIST of size 28 (bloodmnist)\n",
       "    Number of datapoints: 11959\n",
       "    Root location: ./\n",
       "    Split: train\n",
       "    Task: multi-class\n",
       "    Number of channels: 3\n",
       "    Meaning of labels: {'0': 'basophil', '1': 'eosinophil', '2': 'erythroblast', '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)', '4': 'lymphocyte', '5': 'monocyte', '6': 'neutrophil', '7': 'platelet'}\n",
       "    Number of samples: {'train': 11959, 'val': 1712, 'test': 3421}\n",
       "    Description: The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3×360×363 pixels are center-cropped into 3×200×200, and then resized into 3×28×28.\n",
       "    License: CC BY 4.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_flag = 'bloodmnist'\n",
    "download = True\n",
    "\n",
    "info = medmnist.INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "glb_train_dataset = DataClass(root=\"./\", split='train', download=download)\n",
    "glb_val_dataset = DataClass(root=\"./\", split='val', download=download)\n",
    "glb_test_dataset = DataClass(root=\"./\", split='test', download=download)\n",
    "info[\"n_samples\"][\"train\"] = glb_train_dataset.imgs.shape[0]\n",
    "info[\"n_samples\"][\"val\"] = glb_val_dataset.imgs.shape[0]\n",
    "info[\"n_samples\"][\"test\"] = glb_test_dataset.imgs.shape[0]\n",
    "glb_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZS4MPgVRZhXw"
   },
   "outputs": [],
   "source": [
    "num_clients = 5\n",
    "num_classes = 8\n",
    "part_dir, list_name, split_ids = utils.fl_partition(glb_train_dataset, data_flag, num_clients, num_classes, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 193,
     "status": "ok",
     "timestamp": 1734368930821,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "6IGD4lvGcWgV",
    "outputId": "500da096-339f-42ca-9cf9-9d63533a70bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pathmnist_expr/sim_partitions_iid\n"
     ]
    }
   ],
   "source": [
    "print(part_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7h9HgsBObX6K"
   },
   "source": [
    "2. Train local models on each partition of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1353940,
     "status": "ok",
     "timestamp": 1734381997413,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "cSyz3rucbdEt",
    "outputId": "600414c8-f552-47e5-b4de-bfc383cb96eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch num: 1 \n",
      "Train loss: 0.171415 \n",
      "Val loss: 0.036534 \n",
      "Val acc: 0.313808\n",
      "Val balanced acc: 0.320942\n",
      "Epoch num: 2 \n",
      "Train loss: 0.087063 \n",
      "Val loss: 0.010793 \n",
      "Val acc: 0.631799\n",
      "Val balanced acc: 0.598843\n",
      "Epoch num: 3 \n",
      "Train loss: 0.070358 \n",
      "Val loss: 0.010469 \n",
      "Val acc: 0.602510\n",
      "Val balanced acc: 0.628109\n",
      "Epoch num: 4 \n",
      "Train loss: 0.065630 \n",
      "Val loss: 0.028277 \n",
      "Val acc: 0.336820\n",
      "Val balanced acc: 0.378057\n",
      "Epoch num: 5 \n",
      "Train loss: 0.056386 \n",
      "Val loss: 0.004226 \n",
      "Val acc: 0.824268\n",
      "Val balanced acc: 0.775830\n",
      "Epoch num: 6 \n",
      "Train loss: 0.051509 \n",
      "Val loss: 0.005905 \n",
      "Val acc: 0.728033\n",
      "Val balanced acc: 0.681233\n",
      "Epoch num: 7 \n",
      "Train loss: 0.051523 \n",
      "Val loss: 0.012275 \n",
      "Val acc: 0.589958\n",
      "Val balanced acc: 0.490417\n",
      "Epoch num: 8 \n",
      "Train loss: 0.053366 \n",
      "Val loss: 0.008233 \n",
      "Val acc: 0.690377\n",
      "Val balanced acc: 0.601304\n",
      "Epoch num: 9 \n",
      "Train loss: 0.046794 \n",
      "Val loss: 0.008647 \n",
      "Val acc: 0.640167\n",
      "Val balanced acc: 0.608475\n",
      "Epoch num: 10 \n",
      "Train loss: 0.044380 \n",
      "Val loss: 0.004139 \n",
      "Val acc: 0.832636\n",
      "Val balanced acc: 0.784485\n",
      "Epoch num: 11 \n",
      "Train loss: 0.043193 \n",
      "Val loss: 0.007453 \n",
      "Val acc: 0.698745\n",
      "Val balanced acc: 0.606960\n",
      "Epoch num: 12 \n",
      "Train loss: 0.044262 \n",
      "Val loss: 0.005258 \n",
      "Val acc: 0.778243\n",
      "Val balanced acc: 0.794181\n",
      "Epoch num: 13 \n",
      "Train loss: 0.041878 \n",
      "Val loss: 0.008180 \n",
      "Val acc: 0.684100\n",
      "Val balanced acc: 0.629537\n",
      "Epoch num: 14 \n",
      "Train loss: 0.037866 \n",
      "Val loss: 0.006701 \n",
      "Val acc: 0.751046\n",
      "Val balanced acc: 0.712463\n",
      "Epoch num: 15 \n",
      "Train loss: 0.034763 \n",
      "Val loss: 0.009003 \n",
      "Val acc: 0.719665\n",
      "Val balanced acc: 0.680702\n",
      "Epoch num: 16 \n",
      "Train loss: 0.034226 \n",
      "Val loss: 0.007717 \n",
      "Val acc: 0.755230\n",
      "Val balanced acc: 0.719302\n",
      "Epoch num: 17 \n",
      "Train loss: 0.033566 \n",
      "Val loss: 0.002721 \n",
      "Val acc: 0.882845\n",
      "Val balanced acc: 0.877455\n",
      "Epoch num: 18 \n",
      "Train loss: 0.031136 \n",
      "Val loss: 0.004158 \n",
      "Val acc: 0.836820\n",
      "Val balanced acc: 0.832654\n",
      "Epoch num: 19 \n",
      "Train loss: 0.031253 \n",
      "Val loss: 0.008440 \n",
      "Val acc: 0.765690\n",
      "Val balanced acc: 0.683111\n",
      "Epoch num: 20 \n",
      "Train loss: 0.032673 \n",
      "Val loss: 0.004958 \n",
      "Val acc: 0.803347\n",
      "Val balanced acc: 0.789231\n",
      "Epoch num: 21 \n",
      "Train loss: 0.028771 \n",
      "Val loss: 0.004489 \n",
      "Val acc: 0.805439\n",
      "Val balanced acc: 0.747697\n",
      "Epoch num: 22 \n",
      "Train loss: 0.026810 \n",
      "Val loss: 0.003809 \n",
      "Val acc: 0.870293\n",
      "Val balanced acc: 0.855728\n",
      "Epoch num: 23 \n",
      "Train loss: 0.024699 \n",
      "Val loss: 0.003765 \n",
      "Val acc: 0.853556\n",
      "Val balanced acc: 0.808510\n",
      "Epoch num: 24 \n",
      "Train loss: 0.027442 \n",
      "Val loss: 0.008658 \n",
      "Val acc: 0.732218\n",
      "Val balanced acc: 0.667867\n",
      "Epoch num: 25 \n",
      "Train loss: 0.023995 \n",
      "Val loss: 0.004942 \n",
      "Val acc: 0.811715\n",
      "Val balanced acc: 0.748967\n",
      "Epoch num: 26 \n",
      "Train loss: 0.028814 \n",
      "Val loss: 0.002991 \n",
      "Val acc: 0.897489\n",
      "Val balanced acc: 0.863213\n",
      "Epoch num: 27 \n",
      "Train loss: 0.024111 \n",
      "Val loss: 0.002970 \n",
      "Val acc: 0.884937\n",
      "Val balanced acc: 0.863721\n",
      "Epoch num: 28 \n",
      "Train loss: 0.020792 \n",
      "Val loss: 0.003360 \n",
      "Val acc: 0.874477\n",
      "Val balanced acc: 0.850014\n",
      "Epoch num: 29 \n",
      "Train loss: 0.018996 \n",
      "Val loss: 0.001834 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.923200\n",
      "Epoch num: 30 \n",
      "Train loss: 0.014991 \n",
      "Val loss: 0.001810 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.919603\n",
      "Epoch num: 31 \n",
      "Train loss: 0.014014 \n",
      "Val loss: 0.001819 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.935270\n",
      "Epoch num: 32 \n",
      "Train loss: 0.014577 \n",
      "Val loss: 0.001747 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.922959\n",
      "Epoch num: 33 \n",
      "Train loss: 0.012438 \n",
      "Val loss: 0.001724 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.927354\n",
      "Epoch num: 34 \n",
      "Train loss: 0.012859 \n",
      "Val loss: 0.001835 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.907607\n",
      "Epoch num: 35 \n",
      "Train loss: 0.012555 \n",
      "Val loss: 0.001930 \n",
      "Val acc: 0.914226\n",
      "Val balanced acc: 0.890376\n",
      "Epoch num: 36 \n",
      "Train loss: 0.010551 \n",
      "Val loss: 0.001849 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.913624\n",
      "Epoch num: 37 \n",
      "Train loss: 0.011494 \n",
      "Val loss: 0.001680 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.922755\n",
      "Epoch num: 38 \n",
      "Train loss: 0.011868 \n",
      "Val loss: 0.001720 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.920864\n",
      "Epoch num: 39 \n",
      "Train loss: 0.010695 \n",
      "Val loss: 0.001874 \n",
      "Val acc: 0.922594\n",
      "Val balanced acc: 0.909098\n",
      "Epoch num: 40 \n",
      "Train loss: 0.011246 \n",
      "Val loss: 0.001836 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.907953\n",
      "Epoch num: 41 \n",
      "Train loss: 0.012040 \n",
      "Val loss: 0.001937 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.925602\n",
      "Epoch num: 42 \n",
      "Train loss: 0.011438 \n",
      "Val loss: 0.001887 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.914441\n",
      "Epoch num: 43 \n",
      "Train loss: 0.010489 \n",
      "Val loss: 0.001784 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.918397\n",
      "Epoch num: 44 \n",
      "Train loss: 0.010433 \n",
      "Val loss: 0.002023 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.911972\n",
      "Epoch num: 45 \n",
      "Train loss: 0.010592 \n",
      "Val loss: 0.001800 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.926041\n",
      "Epoch num: 46 \n",
      "Train loss: 0.010885 \n",
      "Val loss: 0.001942 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.913833\n",
      "Epoch num: 47 \n",
      "Train loss: 0.010676 \n",
      "Val loss: 0.002113 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.902934\n",
      "Epoch num: 48 \n",
      "Train loss: 0.009390 \n",
      "Val loss: 0.001946 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.912523\n",
      "Epoch num: 49 \n",
      "Train loss: 0.008233 \n",
      "Val loss: 0.001778 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.924622\n",
      "Epoch num: 50 \n",
      "Train loss: 0.007903 \n",
      "Val loss: 0.001732 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.924652\n",
      "Epoch num: 51 \n",
      "Train loss: 0.009660 \n",
      "Val loss: 0.001714 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.922253\n",
      "Epoch num: 52 \n",
      "Train loss: 0.009299 \n",
      "Val loss: 0.001741 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.926041\n",
      "Epoch num: 53 \n",
      "Train loss: 0.007835 \n",
      "Val loss: 0.001738 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.926041\n",
      "Epoch num: 54 \n",
      "Train loss: 0.008215 \n",
      "Val loss: 0.001739 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.926041\n",
      "Epoch num: 55 \n",
      "Train loss: 0.007229 \n",
      "Val loss: 0.001761 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.922353\n",
      "Epoch num: 56 \n",
      "Train loss: 0.007772 \n",
      "Val loss: 0.001766 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.921274\n",
      "Epoch num: 57 \n",
      "Train loss: 0.007827 \n",
      "Val loss: 0.001725 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.924652\n",
      "Epoch num: 58 \n",
      "Train loss: 0.007652 \n",
      "Val loss: 0.001708 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927559\n",
      "Epoch num: 59 \n",
      "Train loss: 0.007835 \n",
      "Val loss: 0.001711 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927559\n",
      "Epoch num: 60 \n",
      "Train loss: 0.008302 \n",
      "Val loss: 0.001714 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927559\n",
      "Epoch num: 61 \n",
      "Train loss: 0.007691 \n",
      "Val loss: 0.001710 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.923771\n",
      "Epoch num: 62 \n",
      "Train loss: 0.007588 \n",
      "Val loss: 0.001691 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927150\n",
      "Epoch num: 63 \n",
      "Train loss: 0.007134 \n",
      "Val loss: 0.001702 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927559\n",
      "Epoch num: 64 \n",
      "Train loss: 0.007871 \n",
      "Val loss: 0.001725 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.928948\n",
      "Epoch num: 65 \n",
      "Train loss: 0.007630 \n",
      "Val loss: 0.001727 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.928948\n",
      "Epoch num: 66 \n",
      "Train loss: 0.007981 \n",
      "Val loss: 0.001720 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927559\n",
      "Epoch num: 67 \n",
      "Train loss: 0.008375 \n",
      "Val loss: 0.001731 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.928948\n",
      "Epoch num: 68 \n",
      "Train loss: 0.007709 \n",
      "Val loss: 0.001717 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.928948\n",
      "Epoch num: 69 \n",
      "Train loss: 0.007271 \n",
      "Val loss: 0.001730 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.925160\n",
      "Epoch num: 70 \n",
      "Train loss: 0.008744 \n",
      "Val loss: 0.001720 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.928948\n",
      "Epoch num: 71 \n",
      "Train loss: 0.007195 \n",
      "Val loss: 0.001713 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927559\n",
      "Epoch num: 72 \n",
      "Train loss: 0.007973 \n",
      "Val loss: 0.001720 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.923771\n",
      "Epoch num: 73 \n",
      "Train loss: 0.008699 \n",
      "Val loss: 0.001733 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927150\n",
      "Epoch num: 74 \n",
      "Train loss: 0.008203 \n",
      "Val loss: 0.001720 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927559\n",
      "Epoch num: 75 \n",
      "Train loss: 0.007316 \n",
      "Val loss: 0.001710 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927559\n",
      "Epoch num: 76 \n",
      "Train loss: 0.008196 \n",
      "Val loss: 0.001684 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.928948\n",
      "Epoch num: 77 \n",
      "Train loss: 0.008345 \n",
      "Val loss: 0.001708 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.928948\n",
      "Epoch num: 78 \n",
      "Train loss: 0.008656 \n",
      "Val loss: 0.001719 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.923771\n",
      "Epoch num: 79 \n",
      "Train loss: 0.008014 \n",
      "Val loss: 0.001712 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927559\n",
      "Epoch num: 80 \n",
      "Train loss: 0.008235 \n",
      "Val loss: 0.001693 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927150\n",
      "Epoch num: 81 \n",
      "Train loss: 0.008075 \n",
      "Val loss: 0.001732 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.928948\n",
      "Epoch num: 82 \n",
      "Train loss: 0.006719 \n",
      "Val loss: 0.001716 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.928948\n",
      "Epoch num: 83 \n",
      "Train loss: 0.008137 \n",
      "Val loss: 0.001727 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927150\n",
      "Epoch num: 84 \n",
      "Train loss: 0.008073 \n",
      "Val loss: 0.001697 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.925160\n",
      "Epoch num: 85 \n",
      "Train loss: 0.007682 \n",
      "Val loss: 0.001712 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.923771\n",
      "Epoch num: 86 \n",
      "Train loss: 0.008348 \n",
      "Val loss: 0.001715 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.923771\n",
      "Epoch num: 87 \n",
      "Train loss: 0.008084 \n",
      "Val loss: 0.001722 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.923771\n",
      "Epoch num: 88 \n",
      "Train loss: 0.008008 \n",
      "Val loss: 0.001734 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.923771\n",
      "Epoch num: 89 \n",
      "Train loss: 0.008340 \n",
      "Val loss: 0.001718 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.928948\n",
      "Epoch num: 90 \n",
      "Train loss: 0.007897 \n",
      "Val loss: 0.001728 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.925160\n",
      "Epoch num: 91 \n",
      "Train loss: 0.007268 \n",
      "Val loss: 0.001717 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.922253\n",
      "Epoch num: 92 \n",
      "Train loss: 0.008659 \n",
      "Val loss: 0.001710 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927559\n",
      "Epoch num: 93 \n",
      "Train loss: 0.007719 \n",
      "Val loss: 0.001700 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.928948\n",
      "Epoch num: 94 \n",
      "Train loss: 0.008575 \n",
      "Val loss: 0.001713 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.923771\n",
      "Epoch num: 95 \n",
      "Train loss: 0.008530 \n",
      "Val loss: 0.001732 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927559\n",
      "Epoch num: 96 \n",
      "Train loss: 0.006747 \n",
      "Val loss: 0.001721 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.923771\n",
      "Epoch num: 97 \n",
      "Train loss: 0.007374 \n",
      "Val loss: 0.001717 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.928948\n",
      "Epoch num: 98 \n",
      "Train loss: 0.008623 \n",
      "Val loss: 0.001716 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927150\n",
      "Epoch num: 99 \n",
      "Train loss: 0.008946 \n",
      "Val loss: 0.001710 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927150\n",
      "Epoch num: 100 \n",
      "Train loss: 0.007835 \n",
      "Val loss: 0.001739 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.927559\n",
      "All done!\n",
      "Epoch num: 1 \n",
      "Train loss: 0.161963 \n",
      "Val loss: 0.078657 \n",
      "Val acc: 0.202929\n",
      "Val balanced acc: 0.279167\n",
      "Epoch num: 2 \n",
      "Train loss: 0.085857 \n",
      "Val loss: 0.010295 \n",
      "Val acc: 0.610879\n",
      "Val balanced acc: 0.625893\n",
      "Epoch num: 3 \n",
      "Train loss: 0.068631 \n",
      "Val loss: 0.023370 \n",
      "Val acc: 0.451883\n",
      "Val balanced acc: 0.460289\n",
      "Epoch num: 4 \n",
      "Train loss: 0.060622 \n",
      "Val loss: 0.008472 \n",
      "Val acc: 0.705021\n",
      "Val balanced acc: 0.681627\n",
      "Epoch num: 5 \n",
      "Train loss: 0.051683 \n",
      "Val loss: 0.009580 \n",
      "Val acc: 0.646443\n",
      "Val balanced acc: 0.598740\n",
      "Epoch num: 6 \n",
      "Train loss: 0.053127 \n",
      "Val loss: 0.020975 \n",
      "Val acc: 0.485356\n",
      "Val balanced acc: 0.399403\n",
      "Epoch num: 7 \n",
      "Train loss: 0.048751 \n",
      "Val loss: 0.008940 \n",
      "Val acc: 0.698745\n",
      "Val balanced acc: 0.599455\n",
      "Epoch num: 8 \n",
      "Train loss: 0.049576 \n",
      "Val loss: 0.008411 \n",
      "Val acc: 0.707113\n",
      "Val balanced acc: 0.578802\n",
      "Epoch num: 9 \n",
      "Train loss: 0.046494 \n",
      "Val loss: 0.008493 \n",
      "Val acc: 0.721757\n",
      "Val balanced acc: 0.607579\n",
      "Epoch num: 10 \n",
      "Train loss: 0.045243 \n",
      "Val loss: 0.014429 \n",
      "Val acc: 0.615063\n",
      "Val balanced acc: 0.594877\n",
      "Epoch num: 11 \n",
      "Train loss: 0.042217 \n",
      "Val loss: 0.005064 \n",
      "Val acc: 0.799163\n",
      "Val balanced acc: 0.735150\n",
      "Epoch num: 12 \n",
      "Train loss: 0.043169 \n",
      "Val loss: 0.004910 \n",
      "Val acc: 0.797071\n",
      "Val balanced acc: 0.706759\n",
      "Epoch num: 13 \n",
      "Train loss: 0.042053 \n",
      "Val loss: 0.004364 \n",
      "Val acc: 0.841004\n",
      "Val balanced acc: 0.792732\n",
      "Epoch num: 14 \n",
      "Train loss: 0.039792 \n",
      "Val loss: 0.004961 \n",
      "Val acc: 0.799163\n",
      "Val balanced acc: 0.683008\n",
      "Epoch num: 15 \n",
      "Train loss: 0.033770 \n",
      "Val loss: 0.006515 \n",
      "Val acc: 0.774059\n",
      "Val balanced acc: 0.664245\n",
      "Epoch num: 16 \n",
      "Train loss: 0.036544 \n",
      "Val loss: 0.004137 \n",
      "Val acc: 0.841004\n",
      "Val balanced acc: 0.777276\n",
      "Epoch num: 17 \n",
      "Train loss: 0.034818 \n",
      "Val loss: 0.013106 \n",
      "Val acc: 0.675732\n",
      "Val balanced acc: 0.513828\n",
      "Epoch num: 18 \n",
      "Train loss: 0.038451 \n",
      "Val loss: 0.014047 \n",
      "Val acc: 0.631799\n",
      "Val balanced acc: 0.589890\n",
      "Epoch num: 19 \n",
      "Train loss: 0.035764 \n",
      "Val loss: 0.007723 \n",
      "Val acc: 0.736402\n",
      "Val balanced acc: 0.663904\n",
      "Epoch num: 20 \n",
      "Train loss: 0.032399 \n",
      "Val loss: 0.006224 \n",
      "Val acc: 0.792887\n",
      "Val balanced acc: 0.659057\n",
      "Epoch num: 21 \n",
      "Train loss: 0.030607 \n",
      "Val loss: 0.003891 \n",
      "Val acc: 0.851464\n",
      "Val balanced acc: 0.766315\n",
      "Epoch num: 22 \n",
      "Train loss: 0.030797 \n",
      "Val loss: 0.021173 \n",
      "Val acc: 0.661088\n",
      "Val balanced acc: 0.639046\n",
      "Epoch num: 23 \n",
      "Train loss: 0.036278 \n",
      "Val loss: 0.009768 \n",
      "Val acc: 0.734310\n",
      "Val balanced acc: 0.614050\n",
      "Epoch num: 24 \n",
      "Train loss: 0.029042 \n",
      "Val loss: 0.006247 \n",
      "Val acc: 0.761506\n",
      "Val balanced acc: 0.745034\n",
      "Epoch num: 25 \n",
      "Train loss: 0.026228 \n",
      "Val loss: 0.004760 \n",
      "Val acc: 0.830544\n",
      "Val balanced acc: 0.736707\n",
      "Epoch num: 26 \n",
      "Train loss: 0.023321 \n",
      "Val loss: 0.005592 \n",
      "Val acc: 0.809623\n",
      "Val balanced acc: 0.749304\n",
      "Epoch num: 27 \n",
      "Train loss: 0.022959 \n",
      "Val loss: 0.004723 \n",
      "Val acc: 0.857741\n",
      "Val balanced acc: 0.757640\n",
      "Epoch num: 28 \n",
      "Train loss: 0.023988 \n",
      "Val loss: 0.009932 \n",
      "Val acc: 0.757322\n",
      "Val balanced acc: 0.647954\n",
      "Epoch num: 29 \n",
      "Train loss: 0.027234 \n",
      "Val loss: 0.003672 \n",
      "Val acc: 0.853556\n",
      "Val balanced acc: 0.780111\n",
      "Epoch num: 30 \n",
      "Train loss: 0.024926 \n",
      "Val loss: 0.010408 \n",
      "Val acc: 0.748954\n",
      "Val balanced acc: 0.669490\n",
      "Epoch num: 31 \n",
      "Train loss: 0.024665 \n",
      "Val loss: 0.007004 \n",
      "Val acc: 0.767782\n",
      "Val balanced acc: 0.732291\n",
      "Epoch num: 32 \n",
      "Train loss: 0.022574 \n",
      "Val loss: 0.003844 \n",
      "Val acc: 0.868201\n",
      "Val balanced acc: 0.794784\n",
      "Epoch num: 33 \n",
      "Train loss: 0.022454 \n",
      "Val loss: 0.005083 \n",
      "Val acc: 0.843096\n",
      "Val balanced acc: 0.784077\n",
      "Epoch num: 34 \n",
      "Train loss: 0.020792 \n",
      "Val loss: 0.004226 \n",
      "Val acc: 0.824268\n",
      "Val balanced acc: 0.795419\n",
      "Epoch num: 35 \n",
      "Train loss: 0.022248 \n",
      "Val loss: 0.007286 \n",
      "Val acc: 0.786611\n",
      "Val balanced acc: 0.777753\n",
      "Epoch num: 36 \n",
      "Train loss: 0.019878 \n",
      "Val loss: 0.006173 \n",
      "Val acc: 0.786611\n",
      "Val balanced acc: 0.707018\n",
      "Epoch num: 37 \n",
      "Train loss: 0.019815 \n",
      "Val loss: 0.005822 \n",
      "Val acc: 0.811715\n",
      "Val balanced acc: 0.782658\n",
      "Epoch num: 38 \n",
      "Train loss: 0.017369 \n",
      "Val loss: 0.003795 \n",
      "Val acc: 0.870293\n",
      "Val balanced acc: 0.791400\n",
      "Epoch num: 39 \n",
      "Train loss: 0.021368 \n",
      "Val loss: 0.002910 \n",
      "Val acc: 0.895397\n",
      "Val balanced acc: 0.828091\n",
      "Epoch num: 40 \n",
      "Train loss: 0.017629 \n",
      "Val loss: 0.008959 \n",
      "Val acc: 0.753138\n",
      "Val balanced acc: 0.725975\n",
      "Epoch num: 41 \n",
      "Train loss: 0.017550 \n",
      "Val loss: 0.003524 \n",
      "Val acc: 0.872385\n",
      "Val balanced acc: 0.850779\n",
      "Epoch num: 42 \n",
      "Train loss: 0.016845 \n",
      "Val loss: 0.005893 \n",
      "Val acc: 0.824268\n",
      "Val balanced acc: 0.784903\n",
      "Epoch num: 43 \n",
      "Train loss: 0.017056 \n",
      "Val loss: 0.008498 \n",
      "Val acc: 0.736402\n",
      "Val balanced acc: 0.706275\n",
      "Epoch num: 44 \n",
      "Train loss: 0.015494 \n",
      "Val loss: 0.002082 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.903155\n",
      "Epoch num: 45 \n",
      "Train loss: 0.015366 \n",
      "Val loss: 0.003286 \n",
      "Val acc: 0.872385\n",
      "Val balanced acc: 0.818650\n",
      "Epoch num: 46 \n",
      "Train loss: 0.014340 \n",
      "Val loss: 0.003819 \n",
      "Val acc: 0.878661\n",
      "Val balanced acc: 0.873777\n",
      "Epoch num: 47 \n",
      "Train loss: 0.016731 \n",
      "Val loss: 0.005129 \n",
      "Val acc: 0.847280\n",
      "Val balanced acc: 0.797903\n",
      "Epoch num: 48 \n",
      "Train loss: 0.016036 \n",
      "Val loss: 0.007516 \n",
      "Val acc: 0.792887\n",
      "Val balanced acc: 0.690849\n",
      "Epoch num: 49 \n",
      "Train loss: 0.014600 \n",
      "Val loss: 0.008291 \n",
      "Val acc: 0.776151\n",
      "Val balanced acc: 0.752060\n",
      "Epoch num: 50 \n",
      "Train loss: 0.018154 \n",
      "Val loss: 0.005538 \n",
      "Val acc: 0.826360\n",
      "Val balanced acc: 0.810117\n",
      "Epoch num: 51 \n",
      "Train loss: 0.013313 \n",
      "Val loss: 0.004249 \n",
      "Val acc: 0.880753\n",
      "Val balanced acc: 0.806407\n",
      "Epoch num: 52 \n",
      "Train loss: 0.013813 \n",
      "Val loss: 0.004160 \n",
      "Val acc: 0.861925\n",
      "Val balanced acc: 0.804790\n",
      "Epoch num: 53 \n",
      "Train loss: 0.013220 \n",
      "Val loss: 0.007950 \n",
      "Val acc: 0.801255\n",
      "Val balanced acc: 0.752222\n",
      "Epoch num: 54 \n",
      "Train loss: 0.012799 \n",
      "Val loss: 0.002482 \n",
      "Val acc: 0.905858\n",
      "Val balanced acc: 0.888713\n",
      "Epoch num: 55 \n",
      "Train loss: 0.010957 \n",
      "Val loss: 0.004591 \n",
      "Val acc: 0.870293\n",
      "Val balanced acc: 0.778794\n",
      "Epoch num: 56 \n",
      "Train loss: 0.010619 \n",
      "Val loss: 0.002269 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.899095\n",
      "Epoch num: 57 \n",
      "Train loss: 0.006503 \n",
      "Val loss: 0.002032 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.910965\n",
      "Epoch num: 58 \n",
      "Train loss: 0.006328 \n",
      "Val loss: 0.001918 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.901992\n",
      "Epoch num: 59 \n",
      "Train loss: 0.005725 \n",
      "Val loss: 0.001780 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.904297\n",
      "Epoch num: 60 \n",
      "Train loss: 0.005911 \n",
      "Val loss: 0.001742 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.906405\n",
      "Epoch num: 61 \n",
      "Train loss: 0.005191 \n",
      "Val loss: 0.001777 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.906514\n",
      "Epoch num: 62 \n",
      "Train loss: 0.005509 \n",
      "Val loss: 0.001978 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.903274\n",
      "Epoch num: 63 \n",
      "Train loss: 0.004951 \n",
      "Val loss: 0.001768 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.912009\n",
      "Epoch num: 64 \n",
      "Train loss: 0.004357 \n",
      "Val loss: 0.001784 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.911034\n",
      "Epoch num: 65 \n",
      "Train loss: 0.004075 \n",
      "Val loss: 0.001878 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.910011\n",
      "Epoch num: 66 \n",
      "Train loss: 0.004258 \n",
      "Val loss: 0.001793 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.906819\n",
      "Epoch num: 67 \n",
      "Train loss: 0.005137 \n",
      "Val loss: 0.001839 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.921122\n",
      "Epoch num: 68 \n",
      "Train loss: 0.004578 \n",
      "Val loss: 0.001897 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.912119\n",
      "Epoch num: 69 \n",
      "Train loss: 0.003651 \n",
      "Val loss: 0.001701 \n",
      "Val acc: 0.949791\n",
      "Val balanced acc: 0.921268\n",
      "Epoch num: 70 \n",
      "Train loss: 0.004013 \n",
      "Val loss: 0.001732 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.914689\n",
      "Epoch num: 71 \n",
      "Train loss: 0.003893 \n",
      "Val loss: 0.001685 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.919063\n",
      "Epoch num: 72 \n",
      "Train loss: 0.004247 \n",
      "Val loss: 0.002022 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.916907\n",
      "Epoch num: 73 \n",
      "Train loss: 0.004140 \n",
      "Val loss: 0.001760 \n",
      "Val acc: 0.947699\n",
      "Val balanced acc: 0.917357\n",
      "Epoch num: 74 \n",
      "Train loss: 0.004113 \n",
      "Val loss: 0.001782 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.917625\n",
      "Epoch num: 75 \n",
      "Train loss: 0.003688 \n",
      "Val loss: 0.001892 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.916699\n",
      "Epoch num: 76 \n",
      "Train loss: 0.003833 \n",
      "Val loss: 0.001794 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.916334\n",
      "Epoch num: 77 \n",
      "Train loss: 0.003274 \n",
      "Val loss: 0.001876 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.910986\n",
      "Epoch num: 78 \n",
      "Train loss: 0.004205 \n",
      "Val loss: 0.001779 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.919684\n",
      "Epoch num: 79 \n",
      "Train loss: 0.004045 \n",
      "Val loss: 0.001888 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.922974\n",
      "Epoch num: 80 \n",
      "Train loss: 0.003280 \n",
      "Val loss: 0.001719 \n",
      "Val acc: 0.947699\n",
      "Val balanced acc: 0.922864\n",
      "Epoch num: 81 \n",
      "Train loss: 0.002721 \n",
      "Val loss: 0.001866 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.909804\n",
      "Epoch num: 82 \n",
      "Train loss: 0.004439 \n",
      "Val loss: 0.001791 \n",
      "Val acc: 0.947699\n",
      "Val balanced acc: 0.925642\n",
      "Epoch num: 83 \n",
      "Train loss: 0.003016 \n",
      "Val loss: 0.001777 \n",
      "Val acc: 0.949791\n",
      "Val balanced acc: 0.927494\n",
      "Epoch num: 84 \n",
      "Train loss: 0.003256 \n",
      "Val loss: 0.001750 \n",
      "Val acc: 0.947699\n",
      "Val balanced acc: 0.922864\n",
      "Epoch num: 85 \n",
      "Train loss: 0.002622 \n",
      "Val loss: 0.001782 \n",
      "Val acc: 0.945607\n",
      "Val balanced acc: 0.921219\n",
      "Epoch num: 86 \n",
      "Train loss: 0.003777 \n",
      "Val loss: 0.001776 \n",
      "Val acc: 0.947699\n",
      "Val balanced acc: 0.925849\n",
      "Epoch num: 87 \n",
      "Train loss: 0.003290 \n",
      "Val loss: 0.001750 \n",
      "Val acc: 0.945607\n",
      "Val balanced acc: 0.924204\n",
      "Epoch num: 88 \n",
      "Train loss: 0.003856 \n",
      "Val loss: 0.001754 \n",
      "Val acc: 0.947699\n",
      "Val balanced acc: 0.928371\n",
      "Epoch num: 89 \n",
      "Train loss: 0.003642 \n",
      "Val loss: 0.001768 \n",
      "Val acc: 0.945607\n",
      "Val balanced acc: 0.926726\n",
      "Epoch num: 90 \n",
      "Train loss: 0.003496 \n",
      "Val loss: 0.001740 \n",
      "Val acc: 0.947699\n",
      "Val balanced acc: 0.928371\n",
      "Epoch num: 91 \n",
      "Train loss: 0.004167 \n",
      "Val loss: 0.001715 \n",
      "Val acc: 0.947699\n",
      "Val balanced acc: 0.925849\n",
      "Epoch num: 92 \n",
      "Train loss: 0.003063 \n",
      "Val loss: 0.001726 \n",
      "Val acc: 0.949791\n",
      "Val balanced acc: 0.927494\n",
      "Epoch num: 93 \n",
      "Train loss: 0.003806 \n",
      "Val loss: 0.001729 \n",
      "Val acc: 0.949791\n",
      "Val balanced acc: 0.928164\n",
      "Epoch num: 94 \n",
      "Train loss: 0.002872 \n",
      "Val loss: 0.001721 \n",
      "Val acc: 0.947699\n",
      "Val balanced acc: 0.925849\n",
      "Epoch num: 95 \n",
      "Train loss: 0.002910 \n",
      "Val loss: 0.001730 \n",
      "Val acc: 0.949791\n",
      "Val balanced acc: 0.930016\n",
      "Epoch num: 96 \n",
      "Train loss: 0.002809 \n",
      "Val loss: 0.001715 \n",
      "Val acc: 0.951883\n",
      "Val balanced acc: 0.931660\n",
      "Epoch num: 97 \n",
      "Train loss: 0.003304 \n",
      "Val loss: 0.001727 \n",
      "Val acc: 0.947699\n",
      "Val balanced acc: 0.925849\n",
      "Epoch num: 98 \n",
      "Train loss: 0.003775 \n",
      "Val loss: 0.001720 \n",
      "Val acc: 0.947699\n",
      "Val balanced acc: 0.925849\n",
      "Epoch num: 99 \n",
      "Train loss: 0.002426 \n",
      "Val loss: 0.001739 \n",
      "Val acc: 0.947699\n",
      "Val balanced acc: 0.928371\n",
      "Epoch num: 100 \n",
      "Train loss: 0.002659 \n",
      "Val loss: 0.001750 \n",
      "Val acc: 0.945607\n",
      "Val balanced acc: 0.924204\n",
      "All done!\n",
      "Epoch num: 1 \n",
      "Train loss: 0.143021 \n",
      "Val loss: 0.023782 \n",
      "Val acc: 0.466527\n",
      "Val balanced acc: 0.374862\n",
      "Epoch num: 2 \n",
      "Train loss: 0.069640 \n",
      "Val loss: 0.045962 \n",
      "Val acc: 0.317992\n",
      "Val balanced acc: 0.334164\n",
      "Epoch num: 3 \n",
      "Train loss: 0.066827 \n",
      "Val loss: 0.019339 \n",
      "Val acc: 0.443515\n",
      "Val balanced acc: 0.444080\n",
      "Epoch num: 4 \n",
      "Train loss: 0.052880 \n",
      "Val loss: 0.011887 \n",
      "Val acc: 0.594142\n",
      "Val balanced acc: 0.603534\n",
      "Epoch num: 5 \n",
      "Train loss: 0.051415 \n",
      "Val loss: 0.006032 \n",
      "Val acc: 0.765690\n",
      "Val balanced acc: 0.730482\n",
      "Epoch num: 6 \n",
      "Train loss: 0.050063 \n",
      "Val loss: 0.005865 \n",
      "Val acc: 0.755230\n",
      "Val balanced acc: 0.705145\n",
      "Epoch num: 7 \n",
      "Train loss: 0.049229 \n",
      "Val loss: 0.004295 \n",
      "Val acc: 0.820084\n",
      "Val balanced acc: 0.770077\n",
      "Epoch num: 8 \n",
      "Train loss: 0.048696 \n",
      "Val loss: 0.005565 \n",
      "Val acc: 0.761506\n",
      "Val balanced acc: 0.710482\n",
      "Epoch num: 9 \n",
      "Train loss: 0.042552 \n",
      "Val loss: 0.003601 \n",
      "Val acc: 0.847280\n",
      "Val balanced acc: 0.812994\n",
      "Epoch num: 10 \n",
      "Train loss: 0.037686 \n",
      "Val loss: 0.014962 \n",
      "Val acc: 0.633891\n",
      "Val balanced acc: 0.638596\n",
      "Epoch num: 11 \n",
      "Train loss: 0.046488 \n",
      "Val loss: 0.034356 \n",
      "Val acc: 0.401674\n",
      "Val balanced acc: 0.344405\n",
      "Epoch num: 12 \n",
      "Train loss: 0.043959 \n",
      "Val loss: 0.007585 \n",
      "Val acc: 0.709205\n",
      "Val balanced acc: 0.660595\n",
      "Epoch num: 13 \n",
      "Train loss: 0.038246 \n",
      "Val loss: 0.006818 \n",
      "Val acc: 0.738494\n",
      "Val balanced acc: 0.720701\n",
      "Epoch num: 14 \n",
      "Train loss: 0.034326 \n",
      "Val loss: 0.004572 \n",
      "Val acc: 0.807531\n",
      "Val balanced acc: 0.823812\n",
      "Epoch num: 15 \n",
      "Train loss: 0.035140 \n",
      "Val loss: 0.020256 \n",
      "Val acc: 0.610879\n",
      "Val balanced acc: 0.518653\n",
      "Epoch num: 16 \n",
      "Train loss: 0.039116 \n",
      "Val loss: 0.027730 \n",
      "Val acc: 0.464435\n",
      "Val balanced acc: 0.446013\n",
      "Epoch num: 17 \n",
      "Train loss: 0.034742 \n",
      "Val loss: 0.005572 \n",
      "Val acc: 0.792887\n",
      "Val balanced acc: 0.740973\n",
      "Epoch num: 18 \n",
      "Train loss: 0.029881 \n",
      "Val loss: 0.005639 \n",
      "Val acc: 0.765690\n",
      "Val balanced acc: 0.741113\n",
      "Epoch num: 19 \n",
      "Train loss: 0.031374 \n",
      "Val loss: 0.002779 \n",
      "Val acc: 0.884937\n",
      "Val balanced acc: 0.865858\n",
      "Epoch num: 20 \n",
      "Train loss: 0.030493 \n",
      "Val loss: 0.004360 \n",
      "Val acc: 0.861925\n",
      "Val balanced acc: 0.830399\n",
      "Epoch num: 21 \n",
      "Train loss: 0.028937 \n",
      "Val loss: 0.004510 \n",
      "Val acc: 0.853556\n",
      "Val balanced acc: 0.852343\n",
      "Epoch num: 22 \n",
      "Train loss: 0.029918 \n",
      "Val loss: 0.002934 \n",
      "Val acc: 0.878661\n",
      "Val balanced acc: 0.877537\n",
      "Epoch num: 23 \n",
      "Train loss: 0.029387 \n",
      "Val loss: 0.003618 \n",
      "Val acc: 0.859833\n",
      "Val balanced acc: 0.863801\n",
      "Epoch num: 24 \n",
      "Train loss: 0.023336 \n",
      "Val loss: 0.006692 \n",
      "Val acc: 0.771967\n",
      "Val balanced acc: 0.704565\n",
      "Epoch num: 25 \n",
      "Train loss: 0.022971 \n",
      "Val loss: 0.002457 \n",
      "Val acc: 0.918410\n",
      "Val balanced acc: 0.892591\n",
      "Epoch num: 26 \n",
      "Train loss: 0.024970 \n",
      "Val loss: 0.005713 \n",
      "Val acc: 0.803347\n",
      "Val balanced acc: 0.758578\n",
      "Epoch num: 27 \n",
      "Train loss: 0.026867 \n",
      "Val loss: 0.006387 \n",
      "Val acc: 0.807531\n",
      "Val balanced acc: 0.746801\n",
      "Epoch num: 28 \n",
      "Train loss: 0.023342 \n",
      "Val loss: 0.004494 \n",
      "Val acc: 0.857741\n",
      "Val balanced acc: 0.843655\n",
      "Epoch num: 29 \n",
      "Train loss: 0.024269 \n",
      "Val loss: 0.002345 \n",
      "Val acc: 0.912134\n",
      "Val balanced acc: 0.898408\n",
      "Epoch num: 30 \n",
      "Train loss: 0.021585 \n",
      "Val loss: 0.007141 \n",
      "Val acc: 0.778243\n",
      "Val balanced acc: 0.796999\n",
      "Epoch num: 31 \n",
      "Train loss: 0.023440 \n",
      "Val loss: 0.004667 \n",
      "Val acc: 0.834728\n",
      "Val balanced acc: 0.813248\n",
      "Epoch num: 32 \n",
      "Train loss: 0.022252 \n",
      "Val loss: 0.002875 \n",
      "Val acc: 0.882845\n",
      "Val balanced acc: 0.863061\n",
      "Epoch num: 33 \n",
      "Train loss: 0.019528 \n",
      "Val loss: 0.010076 \n",
      "Val acc: 0.740586\n",
      "Val balanced acc: 0.703525\n",
      "Epoch num: 34 \n",
      "Train loss: 0.020953 \n",
      "Val loss: 0.005047 \n",
      "Val acc: 0.803347\n",
      "Val balanced acc: 0.783559\n",
      "Epoch num: 35 \n",
      "Train loss: 0.024161 \n",
      "Val loss: 0.006705 \n",
      "Val acc: 0.820084\n",
      "Val balanced acc: 0.759709\n",
      "Epoch num: 36 \n",
      "Train loss: 0.020346 \n",
      "Val loss: 0.004749 \n",
      "Val acc: 0.826360\n",
      "Val balanced acc: 0.766065\n",
      "Epoch num: 37 \n",
      "Train loss: 0.019522 \n",
      "Val loss: 0.004021 \n",
      "Val acc: 0.841004\n",
      "Val balanced acc: 0.828147\n",
      "Epoch num: 38 \n",
      "Train loss: 0.015968 \n",
      "Val loss: 0.006555 \n",
      "Val acc: 0.828452\n",
      "Val balanced acc: 0.832046\n",
      "Epoch num: 39 \n",
      "Train loss: 0.014668 \n",
      "Val loss: 0.002704 \n",
      "Val acc: 0.893305\n",
      "Val balanced acc: 0.886579\n",
      "Epoch num: 40 \n",
      "Train loss: 0.017292 \n",
      "Val loss: 0.003625 \n",
      "Val acc: 0.893305\n",
      "Val balanced acc: 0.883462\n",
      "Epoch num: 41 \n",
      "Train loss: 0.014039 \n",
      "Val loss: 0.001897 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.915760\n",
      "Epoch num: 42 \n",
      "Train loss: 0.010644 \n",
      "Val loss: 0.001784 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.924931\n",
      "Epoch num: 43 \n",
      "Train loss: 0.009854 \n",
      "Val loss: 0.001859 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.917371\n",
      "Epoch num: 44 \n",
      "Train loss: 0.010156 \n",
      "Val loss: 0.001879 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.925223\n",
      "Epoch num: 45 \n",
      "Train loss: 0.009758 \n",
      "Val loss: 0.001884 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.916663\n",
      "Epoch num: 46 \n",
      "Train loss: 0.008881 \n",
      "Val loss: 0.001747 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.919572\n",
      "Epoch num: 47 \n",
      "Train loss: 0.009061 \n",
      "Val loss: 0.001756 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.919081\n",
      "Epoch num: 48 \n",
      "Train loss: 0.007831 \n",
      "Val loss: 0.001725 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.919860\n",
      "Epoch num: 49 \n",
      "Train loss: 0.008865 \n",
      "Val loss: 0.001728 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.920518\n",
      "Epoch num: 50 \n",
      "Train loss: 0.007695 \n",
      "Val loss: 0.001671 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.919077\n",
      "Epoch num: 51 \n",
      "Train loss: 0.007836 \n",
      "Val loss: 0.001707 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.919216\n",
      "Epoch num: 52 \n",
      "Train loss: 0.007790 \n",
      "Val loss: 0.001796 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.915539\n",
      "Epoch num: 53 \n",
      "Train loss: 0.007191 \n",
      "Val loss: 0.001808 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.919717\n",
      "Epoch num: 54 \n",
      "Train loss: 0.007522 \n",
      "Val loss: 0.001674 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.928568\n",
      "Epoch num: 55 \n",
      "Train loss: 0.008322 \n",
      "Val loss: 0.001732 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.916203\n",
      "Epoch num: 56 \n",
      "Train loss: 0.007395 \n",
      "Val loss: 0.001775 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.915198\n",
      "Epoch num: 57 \n",
      "Train loss: 0.006947 \n",
      "Val loss: 0.001749 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.921840\n",
      "Epoch num: 58 \n",
      "Train loss: 0.006483 \n",
      "Val loss: 0.001847 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.918864\n",
      "Epoch num: 59 \n",
      "Train loss: 0.007152 \n",
      "Val loss: 0.001795 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.915379\n",
      "Epoch num: 60 \n",
      "Train loss: 0.007169 \n",
      "Val loss: 0.001833 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.921620\n",
      "Epoch num: 61 \n",
      "Train loss: 0.006666 \n",
      "Val loss: 0.001825 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.914561\n",
      "Epoch num: 62 \n",
      "Train loss: 0.006528 \n",
      "Val loss: 0.001779 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.912616\n",
      "Epoch num: 63 \n",
      "Train loss: 0.006127 \n",
      "Val loss: 0.001779 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.914049\n",
      "Epoch num: 64 \n",
      "Train loss: 0.006177 \n",
      "Val loss: 0.001794 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.915592\n",
      "Epoch num: 65 \n",
      "Train loss: 0.005757 \n",
      "Val loss: 0.001777 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.914049\n",
      "Epoch num: 66 \n",
      "Train loss: 0.006137 \n",
      "Val loss: 0.001785 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.915592\n",
      "Epoch num: 67 \n",
      "Train loss: 0.005745 \n",
      "Val loss: 0.001766 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.921951\n",
      "Epoch num: 68 \n",
      "Train loss: 0.005895 \n",
      "Val loss: 0.001765 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.917135\n",
      "Epoch num: 69 \n",
      "Train loss: 0.006721 \n",
      "Val loss: 0.001755 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.921951\n",
      "Epoch num: 70 \n",
      "Train loss: 0.006035 \n",
      "Val loss: 0.001778 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.917029\n",
      "Epoch num: 71 \n",
      "Train loss: 0.005836 \n",
      "Val loss: 0.001789 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.917427\n",
      "Epoch num: 72 \n",
      "Train loss: 0.005282 \n",
      "Val loss: 0.001756 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.921840\n",
      "Epoch num: 73 \n",
      "Train loss: 0.005817 \n",
      "Val loss: 0.001751 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.923384\n",
      "Epoch num: 74 \n",
      "Train loss: 0.006664 \n",
      "Val loss: 0.001751 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.923384\n",
      "Epoch num: 75 \n",
      "Train loss: 0.006176 \n",
      "Val loss: 0.001753 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.920407\n",
      "Epoch num: 76 \n",
      "Train loss: 0.006232 \n",
      "Val loss: 0.001759 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.923384\n",
      "Epoch num: 77 \n",
      "Train loss: 0.005550 \n",
      "Val loss: 0.001754 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.923384\n",
      "Epoch num: 78 \n",
      "Train loss: 0.006229 \n",
      "Val loss: 0.001763 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.921840\n",
      "Epoch num: 79 \n",
      "Train loss: 0.005879 \n",
      "Val loss: 0.001771 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.921840\n",
      "Epoch num: 80 \n",
      "Train loss: 0.005571 \n",
      "Val loss: 0.001789 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.921840\n",
      "Epoch num: 81 \n",
      "Train loss: 0.005917 \n",
      "Val loss: 0.001774 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.923384\n",
      "Epoch num: 82 \n",
      "Train loss: 0.005913 \n",
      "Val loss: 0.001771 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.923384\n",
      "Epoch num: 83 \n",
      "Train loss: 0.006512 \n",
      "Val loss: 0.001766 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.921947\n",
      "Epoch num: 84 \n",
      "Train loss: 0.005619 \n",
      "Val loss: 0.001769 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.920404\n",
      "Epoch num: 85 \n",
      "Train loss: 0.005544 \n",
      "Val loss: 0.001754 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.923384\n",
      "Epoch num: 86 \n",
      "Train loss: 0.006458 \n",
      "Val loss: 0.001739 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.923384\n",
      "Epoch num: 87 \n",
      "Train loss: 0.005868 \n",
      "Val loss: 0.001796 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.921840\n",
      "Epoch num: 88 \n",
      "Train loss: 0.005362 \n",
      "Val loss: 0.001772 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.924927\n",
      "Epoch num: 89 \n",
      "Train loss: 0.005725 \n",
      "Val loss: 0.001750 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.924927\n",
      "Epoch num: 90 \n",
      "Train loss: 0.006286 \n",
      "Val loss: 0.001751 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.923384\n",
      "Epoch num: 91 \n",
      "Train loss: 0.005609 \n",
      "Val loss: 0.001751 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.923384\n",
      "Epoch num: 92 \n",
      "Train loss: 0.006592 \n",
      "Val loss: 0.001742 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.924927\n",
      "Epoch num: 93 \n",
      "Train loss: 0.006078 \n",
      "Val loss: 0.001777 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.918864\n",
      "Epoch num: 94 \n",
      "Train loss: 0.006192 \n",
      "Val loss: 0.001779 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.921840\n",
      "Epoch num: 95 \n",
      "Train loss: 0.005641 \n",
      "Val loss: 0.001749 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.923384\n",
      "Epoch num: 96 \n",
      "Train loss: 0.005876 \n",
      "Val loss: 0.001776 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.921840\n",
      "Epoch num: 97 \n",
      "Train loss: 0.005652 \n",
      "Val loss: 0.001754 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.916023\n",
      "Epoch num: 98 \n",
      "Train loss: 0.005670 \n",
      "Val loss: 0.001763 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.921840\n",
      "Epoch num: 99 \n",
      "Train loss: 0.005376 \n",
      "Val loss: 0.001754 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.917563\n",
      "Epoch num: 100 \n",
      "Train loss: 0.005921 \n",
      "Val loss: 0.001762 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.921947\n",
      "All done!\n",
      "Epoch num: 1 \n",
      "Train loss: 0.151419 \n",
      "Val loss: 0.080364 \n",
      "Val acc: 0.326360\n",
      "Val balanced acc: 0.255155\n",
      "Epoch num: 2 \n",
      "Train loss: 0.077842 \n",
      "Val loss: 0.033487 \n",
      "Val acc: 0.481172\n",
      "Val balanced acc: 0.405992\n",
      "Epoch num: 3 \n",
      "Train loss: 0.062175 \n",
      "Val loss: 0.013048 \n",
      "Val acc: 0.612971\n",
      "Val balanced acc: 0.522737\n",
      "Epoch num: 4 \n",
      "Train loss: 0.058684 \n",
      "Val loss: 0.010478 \n",
      "Val acc: 0.589958\n",
      "Val balanced acc: 0.535630\n",
      "Epoch num: 5 \n",
      "Train loss: 0.060997 \n",
      "Val loss: 0.017444 \n",
      "Val acc: 0.466527\n",
      "Val balanced acc: 0.385172\n",
      "Epoch num: 6 \n",
      "Train loss: 0.056612 \n",
      "Val loss: 0.008881 \n",
      "Val acc: 0.629707\n",
      "Val balanced acc: 0.582713\n",
      "Epoch num: 7 \n",
      "Train loss: 0.047513 \n",
      "Val loss: 0.010627 \n",
      "Val acc: 0.556485\n",
      "Val balanced acc: 0.530635\n",
      "Epoch num: 8 \n",
      "Train loss: 0.045761 \n",
      "Val loss: 0.006078 \n",
      "Val acc: 0.761506\n",
      "Val balanced acc: 0.719199\n",
      "Epoch num: 9 \n",
      "Train loss: 0.043936 \n",
      "Val loss: 0.003890 \n",
      "Val acc: 0.830544\n",
      "Val balanced acc: 0.794906\n",
      "Epoch num: 10 \n",
      "Train loss: 0.039302 \n",
      "Val loss: 0.003482 \n",
      "Val acc: 0.845188\n",
      "Val balanced acc: 0.811331\n",
      "Epoch num: 11 \n",
      "Train loss: 0.046295 \n",
      "Val loss: 0.006067 \n",
      "Val acc: 0.751046\n",
      "Val balanced acc: 0.741215\n",
      "Epoch num: 12 \n",
      "Train loss: 0.042044 \n",
      "Val loss: 0.007357 \n",
      "Val acc: 0.732218\n",
      "Val balanced acc: 0.676662\n",
      "Epoch num: 13 \n",
      "Train loss: 0.040483 \n",
      "Val loss: 0.014821 \n",
      "Val acc: 0.633891\n",
      "Val balanced acc: 0.560428\n",
      "Epoch num: 14 \n",
      "Train loss: 0.038440 \n",
      "Val loss: 0.006967 \n",
      "Val acc: 0.742678\n",
      "Val balanced acc: 0.667957\n",
      "Epoch num: 15 \n",
      "Train loss: 0.034900 \n",
      "Val loss: 0.018226 \n",
      "Val acc: 0.606695\n",
      "Val balanced acc: 0.533679\n",
      "Epoch num: 16 \n",
      "Train loss: 0.035832 \n",
      "Val loss: 0.004294 \n",
      "Val acc: 0.813807\n",
      "Val balanced acc: 0.743098\n",
      "Epoch num: 17 \n",
      "Train loss: 0.033266 \n",
      "Val loss: 0.003272 \n",
      "Val acc: 0.851464\n",
      "Val balanced acc: 0.790168\n",
      "Epoch num: 18 \n",
      "Train loss: 0.035090 \n",
      "Val loss: 0.007617 \n",
      "Val acc: 0.700837\n",
      "Val balanced acc: 0.639487\n",
      "Epoch num: 19 \n",
      "Train loss: 0.033555 \n",
      "Val loss: 0.008328 \n",
      "Val acc: 0.715481\n",
      "Val balanced acc: 0.632606\n",
      "Epoch num: 20 \n",
      "Train loss: 0.037729 \n",
      "Val loss: 0.004651 \n",
      "Val acc: 0.807531\n",
      "Val balanced acc: 0.815935\n",
      "Epoch num: 21 \n",
      "Train loss: 0.031362 \n",
      "Val loss: 0.009077 \n",
      "Val acc: 0.692469\n",
      "Val balanced acc: 0.633003\n",
      "Epoch num: 22 \n",
      "Train loss: 0.031978 \n",
      "Val loss: 0.003637 \n",
      "Val acc: 0.859833\n",
      "Val balanced acc: 0.805673\n",
      "Epoch num: 23 \n",
      "Train loss: 0.028986 \n",
      "Val loss: 0.007685 \n",
      "Val acc: 0.746862\n",
      "Val balanced acc: 0.640626\n",
      "Epoch num: 24 \n",
      "Train loss: 0.032837 \n",
      "Val loss: 0.016581 \n",
      "Val acc: 0.562761\n",
      "Val balanced acc: 0.510962\n",
      "Epoch num: 25 \n",
      "Train loss: 0.030355 \n",
      "Val loss: 0.003675 \n",
      "Val acc: 0.857741\n",
      "Val balanced acc: 0.804781\n",
      "Epoch num: 26 \n",
      "Train loss: 0.029483 \n",
      "Val loss: 0.003338 \n",
      "Val acc: 0.859833\n",
      "Val balanced acc: 0.823329\n",
      "Epoch num: 27 \n",
      "Train loss: 0.030156 \n",
      "Val loss: 0.003151 \n",
      "Val acc: 0.864017\n",
      "Val balanced acc: 0.827274\n",
      "Epoch num: 28 \n",
      "Train loss: 0.026220 \n",
      "Val loss: 0.002926 \n",
      "Val acc: 0.880753\n",
      "Val balanced acc: 0.869857\n",
      "Epoch num: 29 \n",
      "Train loss: 0.028170 \n",
      "Val loss: 0.006649 \n",
      "Val acc: 0.759414\n",
      "Val balanced acc: 0.687313\n",
      "Epoch num: 30 \n",
      "Train loss: 0.029353 \n",
      "Val loss: 0.002472 \n",
      "Val acc: 0.901674\n",
      "Val balanced acc: 0.873820\n",
      "Epoch num: 31 \n",
      "Train loss: 0.025234 \n",
      "Val loss: 0.004290 \n",
      "Val acc: 0.836820\n",
      "Val balanced acc: 0.777087\n",
      "Epoch num: 32 \n",
      "Train loss: 0.023216 \n",
      "Val loss: 0.006708 \n",
      "Val acc: 0.782427\n",
      "Val balanced acc: 0.744478\n",
      "Epoch num: 33 \n",
      "Train loss: 0.020794 \n",
      "Val loss: 0.011445 \n",
      "Val acc: 0.669456\n",
      "Val balanced acc: 0.664627\n",
      "Epoch num: 34 \n",
      "Train loss: 0.023615 \n",
      "Val loss: 0.003322 \n",
      "Val acc: 0.872385\n",
      "Val balanced acc: 0.808885\n",
      "Epoch num: 35 \n",
      "Train loss: 0.021306 \n",
      "Val loss: 0.005185 \n",
      "Val acc: 0.836820\n",
      "Val balanced acc: 0.784219\n",
      "Epoch num: 36 \n",
      "Train loss: 0.020740 \n",
      "Val loss: 0.005210 \n",
      "Val acc: 0.828452\n",
      "Val balanced acc: 0.789333\n",
      "Epoch num: 37 \n",
      "Train loss: 0.020839 \n",
      "Val loss: 0.002553 \n",
      "Val acc: 0.893305\n",
      "Val balanced acc: 0.848335\n",
      "Epoch num: 38 \n",
      "Train loss: 0.020330 \n",
      "Val loss: 0.002519 \n",
      "Val acc: 0.912134\n",
      "Val balanced acc: 0.894240\n",
      "Epoch num: 39 \n",
      "Train loss: 0.019184 \n",
      "Val loss: 0.009947 \n",
      "Val acc: 0.707113\n",
      "Val balanced acc: 0.708658\n",
      "Epoch num: 40 \n",
      "Train loss: 0.019500 \n",
      "Val loss: 0.002946 \n",
      "Val acc: 0.876569\n",
      "Val balanced acc: 0.843900\n",
      "Epoch num: 41 \n",
      "Train loss: 0.023218 \n",
      "Val loss: 0.005699 \n",
      "Val acc: 0.788703\n",
      "Val balanced acc: 0.748952\n",
      "Epoch num: 42 \n",
      "Train loss: 0.016687 \n",
      "Val loss: 0.002075 \n",
      "Val acc: 0.916318\n",
      "Val balanced acc: 0.894507\n",
      "Epoch num: 43 \n",
      "Train loss: 0.013420 \n",
      "Val loss: 0.002240 \n",
      "Val acc: 0.916318\n",
      "Val balanced acc: 0.892191\n",
      "Epoch num: 44 \n",
      "Train loss: 0.012836 \n",
      "Val loss: 0.002204 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.905453\n",
      "Epoch num: 45 \n",
      "Train loss: 0.010917 \n",
      "Val loss: 0.002218 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.913657\n",
      "Epoch num: 46 \n",
      "Train loss: 0.012143 \n",
      "Val loss: 0.002175 \n",
      "Val acc: 0.922594\n",
      "Val balanced acc: 0.901614\n",
      "Epoch num: 47 \n",
      "Train loss: 0.010083 \n",
      "Val loss: 0.002168 \n",
      "Val acc: 0.922594\n",
      "Val balanced acc: 0.898316\n",
      "Epoch num: 48 \n",
      "Train loss: 0.011069 \n",
      "Val loss: 0.002197 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.905720\n",
      "Epoch num: 49 \n",
      "Train loss: 0.010333 \n",
      "Val loss: 0.001990 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.909298\n",
      "Epoch num: 50 \n",
      "Train loss: 0.008784 \n",
      "Val loss: 0.002137 \n",
      "Val acc: 0.922594\n",
      "Val balanced acc: 0.896119\n",
      "Epoch num: 51 \n",
      "Train loss: 0.009234 \n",
      "Val loss: 0.002293 \n",
      "Val acc: 0.922594\n",
      "Val balanced acc: 0.898034\n",
      "Epoch num: 52 \n",
      "Train loss: 0.009354 \n",
      "Val loss: 0.002000 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.899720\n",
      "Epoch num: 53 \n",
      "Train loss: 0.008136 \n",
      "Val loss: 0.002143 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.908185\n",
      "Epoch num: 54 \n",
      "Train loss: 0.007821 \n",
      "Val loss: 0.002288 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.903240\n",
      "Epoch num: 55 \n",
      "Train loss: 0.008101 \n",
      "Val loss: 0.002066 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.901250\n",
      "Epoch num: 56 \n",
      "Train loss: 0.008548 \n",
      "Val loss: 0.001858 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.921661\n",
      "Epoch num: 57 \n",
      "Train loss: 0.007743 \n",
      "Val loss: 0.002019 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.897707\n",
      "Epoch num: 58 \n",
      "Train loss: 0.007609 \n",
      "Val loss: 0.001919 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.918899\n",
      "Epoch num: 59 \n",
      "Train loss: 0.009155 \n",
      "Val loss: 0.002169 \n",
      "Val acc: 0.922594\n",
      "Val balanced acc: 0.901629\n",
      "Epoch num: 60 \n",
      "Train loss: 0.007839 \n",
      "Val loss: 0.002081 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.911756\n",
      "Epoch num: 61 \n",
      "Train loss: 0.007132 \n",
      "Val loss: 0.002322 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.908976\n",
      "Epoch num: 62 \n",
      "Train loss: 0.007699 \n",
      "Val loss: 0.001893 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.903469\n",
      "Epoch num: 63 \n",
      "Train loss: 0.006865 \n",
      "Val loss: 0.002133 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.907903\n",
      "Epoch num: 64 \n",
      "Train loss: 0.007088 \n",
      "Val loss: 0.001938 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.922471\n",
      "Epoch num: 65 \n",
      "Train loss: 0.006358 \n",
      "Val loss: 0.002197 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.920815\n",
      "Epoch num: 66 \n",
      "Train loss: 0.005459 \n",
      "Val loss: 0.002216 \n",
      "Val acc: 0.922594\n",
      "Val balanced acc: 0.901911\n",
      "Epoch num: 67 \n",
      "Train loss: 0.007094 \n",
      "Val loss: 0.002068 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.912228\n",
      "Epoch num: 68 \n",
      "Train loss: 0.007061 \n",
      "Val loss: 0.002027 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.918540\n",
      "Epoch num: 69 \n",
      "Train loss: 0.006480 \n",
      "Val loss: 0.001971 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.919913\n",
      "Epoch num: 70 \n",
      "Train loss: 0.006496 \n",
      "Val loss: 0.001996 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.916624\n",
      "Epoch num: 71 \n",
      "Train loss: 0.005825 \n",
      "Val loss: 0.002013 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.911679\n",
      "Epoch num: 72 \n",
      "Train loss: 0.006433 \n",
      "Val loss: 0.001961 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.918264\n",
      "Epoch num: 73 \n",
      "Train loss: 0.005508 \n",
      "Val loss: 0.001902 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.924843\n",
      "Epoch num: 74 \n",
      "Train loss: 0.006008 \n",
      "Val loss: 0.001963 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.913862\n",
      "Epoch num: 75 \n",
      "Train loss: 0.006060 \n",
      "Val loss: 0.001987 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.911664\n",
      "Epoch num: 76 \n",
      "Train loss: 0.005884 \n",
      "Val loss: 0.002032 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.910290\n",
      "Epoch num: 77 \n",
      "Train loss: 0.005060 \n",
      "Val loss: 0.002060 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.913022\n",
      "Epoch num: 78 \n",
      "Train loss: 0.006160 \n",
      "Val loss: 0.002031 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.910290\n",
      "Epoch num: 79 \n",
      "Train loss: 0.005264 \n",
      "Val loss: 0.002059 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.907001\n",
      "Epoch num: 80 \n",
      "Train loss: 0.005286 \n",
      "Val loss: 0.002021 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.913022\n",
      "Epoch num: 81 \n",
      "Train loss: 0.005047 \n",
      "Val loss: 0.002012 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.911649\n",
      "Epoch num: 82 \n",
      "Train loss: 0.004923 \n",
      "Val loss: 0.002051 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.910290\n",
      "Epoch num: 83 \n",
      "Train loss: 0.005768 \n",
      "Val loss: 0.002008 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.915770\n",
      "Epoch num: 84 \n",
      "Train loss: 0.005881 \n",
      "Val loss: 0.002017 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.914411\n",
      "Epoch num: 85 \n",
      "Train loss: 0.005225 \n",
      "Val loss: 0.002006 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.911664\n",
      "Epoch num: 86 \n",
      "Train loss: 0.006187 \n",
      "Val loss: 0.002033 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.911649\n",
      "Epoch num: 87 \n",
      "Train loss: 0.006455 \n",
      "Val loss: 0.001938 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.917433\n",
      "Epoch num: 88 \n",
      "Train loss: 0.005112 \n",
      "Val loss: 0.001948 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.918807\n",
      "Epoch num: 89 \n",
      "Train loss: 0.005328 \n",
      "Val loss: 0.001981 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.910290\n",
      "Epoch num: 90 \n",
      "Train loss: 0.005979 \n",
      "Val loss: 0.002022 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.910290\n",
      "Epoch num: 91 \n",
      "Train loss: 0.005582 \n",
      "Val loss: 0.002003 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.910290\n",
      "Epoch num: 92 \n",
      "Train loss: 0.005415 \n",
      "Val loss: 0.002015 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.910290\n",
      "Epoch num: 93 \n",
      "Train loss: 0.005329 \n",
      "Val loss: 0.001960 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.917433\n",
      "Epoch num: 94 \n",
      "Train loss: 0.006791 \n",
      "Val loss: 0.001993 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.924271\n",
      "Epoch num: 95 \n",
      "Train loss: 0.005102 \n",
      "Val loss: 0.002004 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.918792\n",
      "Epoch num: 96 \n",
      "Train loss: 0.006427 \n",
      "Val loss: 0.002012 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.910290\n",
      "Epoch num: 97 \n",
      "Train loss: 0.005301 \n",
      "Val loss: 0.001985 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.911649\n",
      "Epoch num: 98 \n",
      "Train loss: 0.004586 \n",
      "Val loss: 0.002011 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.920700\n",
      "Epoch num: 99 \n",
      "Train loss: 0.005540 \n",
      "Val loss: 0.001984 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.920165\n",
      "Epoch num: 100 \n",
      "Train loss: 0.005102 \n",
      "Val loss: 0.001964 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.910290\n",
      "All done!\n",
      "Epoch num: 1 \n",
      "Train loss: 0.166450 \n",
      "Val loss: 0.045349 \n",
      "Val acc: 0.292887\n",
      "Val balanced acc: 0.252632\n",
      "Epoch num: 2 \n",
      "Train loss: 0.081009 \n",
      "Val loss: 0.018502 \n",
      "Val acc: 0.476987\n",
      "Val balanced acc: 0.396009\n",
      "Epoch num: 3 \n",
      "Train loss: 0.066917 \n",
      "Val loss: 0.013528 \n",
      "Val acc: 0.596234\n",
      "Val balanced acc: 0.598980\n",
      "Epoch num: 4 \n",
      "Train loss: 0.060520 \n",
      "Val loss: 0.006372 \n",
      "Val acc: 0.763598\n",
      "Val balanced acc: 0.706213\n",
      "Epoch num: 5 \n",
      "Train loss: 0.059660 \n",
      "Val loss: 0.007249 \n",
      "Val acc: 0.723849\n",
      "Val balanced acc: 0.668018\n",
      "Epoch num: 6 \n",
      "Train loss: 0.054480 \n",
      "Val loss: 0.007184 \n",
      "Val acc: 0.707113\n",
      "Val balanced acc: 0.692742\n",
      "Epoch num: 7 \n",
      "Train loss: 0.046754 \n",
      "Val loss: 0.007228 \n",
      "Val acc: 0.700837\n",
      "Val balanced acc: 0.622316\n",
      "Epoch num: 8 \n",
      "Train loss: 0.043386 \n",
      "Val loss: 0.009685 \n",
      "Val acc: 0.686192\n",
      "Val balanced acc: 0.673658\n",
      "Epoch num: 9 \n",
      "Train loss: 0.047087 \n",
      "Val loss: 0.012303 \n",
      "Val acc: 0.573222\n",
      "Val balanced acc: 0.568798\n",
      "Epoch num: 10 \n",
      "Train loss: 0.039218 \n",
      "Val loss: 0.005375 \n",
      "Val acc: 0.799163\n",
      "Val balanced acc: 0.758540\n",
      "Epoch num: 11 \n",
      "Train loss: 0.039380 \n",
      "Val loss: 0.011792 \n",
      "Val acc: 0.665272\n",
      "Val balanced acc: 0.596156\n",
      "Epoch num: 12 \n",
      "Train loss: 0.040964 \n",
      "Val loss: 0.011152 \n",
      "Val acc: 0.669456\n",
      "Val balanced acc: 0.623494\n",
      "Epoch num: 13 \n",
      "Train loss: 0.035036 \n",
      "Val loss: 0.013020 \n",
      "Val acc: 0.690377\n",
      "Val balanced acc: 0.648682\n",
      "Epoch num: 14 \n",
      "Train loss: 0.034976 \n",
      "Val loss: 0.007315 \n",
      "Val acc: 0.753138\n",
      "Val balanced acc: 0.708674\n",
      "Epoch num: 15 \n",
      "Train loss: 0.037502 \n",
      "Val loss: 0.004676 \n",
      "Val acc: 0.830544\n",
      "Val balanced acc: 0.808802\n",
      "Epoch num: 16 \n",
      "Train loss: 0.032229 \n",
      "Val loss: 0.004551 \n",
      "Val acc: 0.826360\n",
      "Val balanced acc: 0.795448\n",
      "Epoch num: 17 \n",
      "Train loss: 0.030638 \n",
      "Val loss: 0.007457 \n",
      "Val acc: 0.713389\n",
      "Val balanced acc: 0.683564\n",
      "Epoch num: 18 \n",
      "Train loss: 0.030281 \n",
      "Val loss: 0.004319 \n",
      "Val acc: 0.838912\n",
      "Val balanced acc: 0.810919\n",
      "Epoch num: 19 \n",
      "Train loss: 0.029131 \n",
      "Val loss: 0.011802 \n",
      "Val acc: 0.673640\n",
      "Val balanced acc: 0.676761\n",
      "Epoch num: 20 \n",
      "Train loss: 0.027849 \n",
      "Val loss: 0.006328 \n",
      "Val acc: 0.790795\n",
      "Val balanced acc: 0.723160\n",
      "Epoch num: 21 \n",
      "Train loss: 0.026653 \n",
      "Val loss: 0.012467 \n",
      "Val acc: 0.692469\n",
      "Val balanced acc: 0.658359\n",
      "Epoch num: 22 \n",
      "Train loss: 0.029797 \n",
      "Val loss: 0.005296 \n",
      "Val acc: 0.803347\n",
      "Val balanced acc: 0.809502\n",
      "Epoch num: 23 \n",
      "Train loss: 0.026793 \n",
      "Val loss: 0.004306 \n",
      "Val acc: 0.834728\n",
      "Val balanced acc: 0.800658\n",
      "Epoch num: 24 \n",
      "Train loss: 0.026424 \n",
      "Val loss: 0.005592 \n",
      "Val acc: 0.809623\n",
      "Val balanced acc: 0.751363\n",
      "Epoch num: 25 \n",
      "Train loss: 0.021973 \n",
      "Val loss: 0.002879 \n",
      "Val acc: 0.903766\n",
      "Val balanced acc: 0.904676\n",
      "Epoch num: 26 \n",
      "Train loss: 0.023167 \n",
      "Val loss: 0.004380 \n",
      "Val acc: 0.845188\n",
      "Val balanced acc: 0.819330\n",
      "Epoch num: 27 \n",
      "Train loss: 0.025419 \n",
      "Val loss: 0.011681 \n",
      "Val acc: 0.742678\n",
      "Val balanced acc: 0.690712\n",
      "Epoch num: 28 \n",
      "Train loss: 0.025098 \n",
      "Val loss: 0.004547 \n",
      "Val acc: 0.847280\n",
      "Val balanced acc: 0.850076\n",
      "Epoch num: 29 \n",
      "Train loss: 0.024298 \n",
      "Val loss: 0.004695 \n",
      "Val acc: 0.838912\n",
      "Val balanced acc: 0.853682\n",
      "Epoch num: 30 \n",
      "Train loss: 0.023788 \n",
      "Val loss: 0.005980 \n",
      "Val acc: 0.817992\n",
      "Val balanced acc: 0.790847\n",
      "Epoch num: 31 \n",
      "Train loss: 0.024576 \n",
      "Val loss: 0.003830 \n",
      "Val acc: 0.864017\n",
      "Val balanced acc: 0.835504\n",
      "Epoch num: 32 \n",
      "Train loss: 0.022974 \n",
      "Val loss: 0.011181 \n",
      "Val acc: 0.719665\n",
      "Val balanced acc: 0.662947\n",
      "Epoch num: 33 \n",
      "Train loss: 0.020075 \n",
      "Val loss: 0.004765 \n",
      "Val acc: 0.838912\n",
      "Val balanced acc: 0.811153\n",
      "Epoch num: 34 \n",
      "Train loss: 0.018965 \n",
      "Val loss: 0.005157 \n",
      "Val acc: 0.811715\n",
      "Val balanced acc: 0.769286\n",
      "Epoch num: 35 \n",
      "Train loss: 0.016555 \n",
      "Val loss: 0.002880 \n",
      "Val acc: 0.903766\n",
      "Val balanced acc: 0.895606\n",
      "Epoch num: 36 \n",
      "Train loss: 0.017862 \n",
      "Val loss: 0.007851 \n",
      "Val acc: 0.761506\n",
      "Val balanced acc: 0.729920\n",
      "Epoch num: 37 \n",
      "Train loss: 0.015545 \n",
      "Val loss: 0.002440 \n",
      "Val acc: 0.916318\n",
      "Val balanced acc: 0.900426\n",
      "Epoch num: 38 \n",
      "Train loss: 0.014593 \n",
      "Val loss: 0.002341 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.917639\n",
      "Epoch num: 39 \n",
      "Train loss: 0.012692 \n",
      "Val loss: 0.002432 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.913602\n",
      "Epoch num: 40 \n",
      "Train loss: 0.011637 \n",
      "Val loss: 0.002431 \n",
      "Val acc: 0.922594\n",
      "Val balanced acc: 0.906838\n",
      "Epoch num: 41 \n",
      "Train loss: 0.010369 \n",
      "Val loss: 0.002441 \n",
      "Val acc: 0.916318\n",
      "Val balanced acc: 0.898851\n",
      "Epoch num: 42 \n",
      "Train loss: 0.010242 \n",
      "Val loss: 0.002462 \n",
      "Val acc: 0.920502\n",
      "Val balanced acc: 0.905497\n",
      "Epoch num: 43 \n",
      "Train loss: 0.009602 \n",
      "Val loss: 0.002336 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.916735\n",
      "Epoch num: 44 \n",
      "Train loss: 0.008687 \n",
      "Val loss: 0.002366 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.913544\n",
      "Epoch num: 45 \n",
      "Train loss: 0.009634 \n",
      "Val loss: 0.002384 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.924655\n",
      "Epoch num: 46 \n",
      "Train loss: 0.008378 \n",
      "Val loss: 0.002367 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.921877\n",
      "Epoch num: 47 \n",
      "Train loss: 0.008992 \n",
      "Val loss: 0.002487 \n",
      "Val acc: 0.918410\n",
      "Val balanced acc: 0.906072\n",
      "Epoch num: 48 \n",
      "Train loss: 0.009133 \n",
      "Val loss: 0.002431 \n",
      "Val acc: 0.933054\n",
      "Val balanced acc: 0.921732\n",
      "Epoch num: 49 \n",
      "Train loss: 0.007580 \n",
      "Val loss: 0.002512 \n",
      "Val acc: 0.926778\n",
      "Val balanced acc: 0.916113\n",
      "Epoch num: 50 \n",
      "Train loss: 0.007356 \n",
      "Val loss: 0.002404 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.917989\n",
      "Epoch num: 51 \n",
      "Train loss: 0.009516 \n",
      "Val loss: 0.002442 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.918316\n",
      "Epoch num: 52 \n",
      "Train loss: 0.007560 \n",
      "Val loss: 0.002664 \n",
      "Val acc: 0.920502\n",
      "Val balanced acc: 0.906493\n",
      "Epoch num: 53 \n",
      "Train loss: 0.008456 \n",
      "Val loss: 0.002514 \n",
      "Val acc: 0.928870\n",
      "Val balanced acc: 0.920562\n",
      "Epoch num: 54 \n",
      "Train loss: 0.008721 \n",
      "Val loss: 0.003357 \n",
      "Val acc: 0.897489\n",
      "Val balanced acc: 0.883364\n",
      "Epoch num: 55 \n",
      "Train loss: 0.008835 \n",
      "Val loss: 0.002526 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.928932\n",
      "Epoch num: 56 \n",
      "Train loss: 0.006995 \n",
      "Val loss: 0.002451 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.929675\n",
      "Epoch num: 57 \n",
      "Train loss: 0.006909 \n",
      "Val loss: 0.002448 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.931320\n",
      "Epoch num: 58 \n",
      "Train loss: 0.007109 \n",
      "Val loss: 0.002448 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.924447\n",
      "Epoch num: 59 \n",
      "Train loss: 0.007618 \n",
      "Val loss: 0.002455 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.926091\n",
      "Epoch num: 60 \n",
      "Train loss: 0.006603 \n",
      "Val loss: 0.002491 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.935121\n",
      "Epoch num: 61 \n",
      "Train loss: 0.008242 \n",
      "Val loss: 0.002457 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.933867\n",
      "Epoch num: 62 \n",
      "Train loss: 0.006192 \n",
      "Val loss: 0.002507 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.927862\n",
      "Epoch num: 63 \n",
      "Train loss: 0.006627 \n",
      "Val loss: 0.002479 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.927799\n",
      "Epoch num: 64 \n",
      "Train loss: 0.007378 \n",
      "Val loss: 0.002466 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.928248\n",
      "Epoch num: 65 \n",
      "Train loss: 0.007082 \n",
      "Val loss: 0.002446 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.931832\n",
      "Epoch num: 66 \n",
      "Train loss: 0.006598 \n",
      "Val loss: 0.002433 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.936254\n",
      "Epoch num: 67 \n",
      "Train loss: 0.007003 \n",
      "Val loss: 0.002461 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.927858\n",
      "Epoch num: 68 \n",
      "Train loss: 0.006510 \n",
      "Val loss: 0.002458 \n",
      "Val acc: 0.945607\n",
      "Val balanced acc: 0.937899\n",
      "Epoch num: 69 \n",
      "Train loss: 0.007155 \n",
      "Val loss: 0.002452 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.937450\n",
      "Epoch num: 70 \n",
      "Train loss: 0.006195 \n",
      "Val loss: 0.002464 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.933867\n",
      "Epoch num: 71 \n",
      "Train loss: 0.006243 \n",
      "Val loss: 0.002462 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.933867\n",
      "Epoch num: 72 \n",
      "Train loss: 0.007312 \n",
      "Val loss: 0.002485 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.926666\n",
      "Epoch num: 73 \n",
      "Train loss: 0.006270 \n",
      "Val loss: 0.002453 \n",
      "Val acc: 0.945607\n",
      "Val balanced acc: 0.939095\n",
      "Epoch num: 74 \n",
      "Train loss: 0.007373 \n",
      "Val loss: 0.002464 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.936317\n",
      "Epoch num: 75 \n",
      "Train loss: 0.005512 \n",
      "Val loss: 0.002458 \n",
      "Val acc: 0.947699\n",
      "Val balanced acc: 0.940740\n",
      "Epoch num: 76 \n",
      "Train loss: 0.006937 \n",
      "Val loss: 0.002451 \n",
      "Val acc: 0.945607\n",
      "Val balanced acc: 0.937899\n",
      "Epoch num: 77 \n",
      "Train loss: 0.007219 \n",
      "Val loss: 0.002444 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.935063\n",
      "Epoch num: 78 \n",
      "Train loss: 0.005848 \n",
      "Val loss: 0.002461 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.928311\n",
      "Epoch num: 79 \n",
      "Train loss: 0.007366 \n",
      "Val loss: 0.002449 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.926666\n",
      "Epoch num: 80 \n",
      "Train loss: 0.006712 \n",
      "Val loss: 0.002457 \n",
      "Val acc: 0.945607\n",
      "Val balanced acc: 0.936708\n",
      "Epoch num: 81 \n",
      "Train loss: 0.006677 \n",
      "Val loss: 0.002486 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.933539\n",
      "Epoch num: 82 \n",
      "Train loss: 0.007935 \n",
      "Val loss: 0.002483 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.927858\n",
      "Epoch num: 83 \n",
      "Train loss: 0.007221 \n",
      "Val loss: 0.002474 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.926666\n",
      "Epoch num: 84 \n",
      "Train loss: 0.006442 \n",
      "Val loss: 0.002462 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.931089\n",
      "Epoch num: 85 \n",
      "Train loss: 0.007010 \n",
      "Val loss: 0.002441 \n",
      "Val acc: 0.945607\n",
      "Val balanced acc: 0.937899\n",
      "Epoch num: 86 \n",
      "Train loss: 0.006546 \n",
      "Val loss: 0.002457 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.923825\n",
      "Epoch num: 87 \n",
      "Train loss: 0.006912 \n",
      "Val loss: 0.002447 \n",
      "Val acc: 0.945607\n",
      "Val balanced acc: 0.939095\n",
      "Epoch num: 88 \n",
      "Train loss: 0.007334 \n",
      "Val loss: 0.002477 \n",
      "Val acc: 0.945607\n",
      "Val balanced acc: 0.937962\n",
      "Epoch num: 89 \n",
      "Train loss: 0.006177 \n",
      "Val loss: 0.002454 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.934673\n",
      "Epoch num: 90 \n",
      "Train loss: 0.006841 \n",
      "Val loss: 0.002447 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.931089\n",
      "Epoch num: 91 \n",
      "Train loss: 0.007533 \n",
      "Val loss: 0.002437 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.934609\n",
      "Epoch num: 92 \n",
      "Train loss: 0.007339 \n",
      "Val loss: 0.002435 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.932222\n",
      "Epoch num: 93 \n",
      "Train loss: 0.006390 \n",
      "Val loss: 0.002480 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.929444\n",
      "Epoch num: 94 \n",
      "Train loss: 0.006473 \n",
      "Val loss: 0.002455 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.936254\n",
      "Epoch num: 95 \n",
      "Train loss: 0.007338 \n",
      "Val loss: 0.002467 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.923825\n",
      "Epoch num: 96 \n",
      "Train loss: 0.006363 \n",
      "Val loss: 0.002466 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.933539\n",
      "Epoch num: 97 \n",
      "Train loss: 0.006912 \n",
      "Val loss: 0.002479 \n",
      "Val acc: 0.945607\n",
      "Val balanced acc: 0.939095\n",
      "Epoch num: 98 \n",
      "Train loss: 0.007468 \n",
      "Val loss: 0.002446 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.936254\n",
      "Epoch num: 99 \n",
      "Train loss: 0.006082 \n",
      "Val loss: 0.002478 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.929117\n",
      "Epoch num: 100 \n",
      "Train loss: 0.006128 \n",
      "Val loss: 0.002466 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.933867\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "data_flag = 'bloodmnist'\n",
    "\n",
    "info = medmnist.INFO[data_flag].copy()\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "for i in range(num_clients):\n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "    root = os.path.join(part_dir, \"client_\" + str(i))\n",
    "    train_dataset = DataClass(root=os.path.join(part_dir, \"client_\" + str(i)), split='train', download=False)\n",
    "    val_dataset = DataClass(root=os.path.join(part_dir, \"client_\" + str(i)), split='val', download=False)\n",
    "    info = medmnist.INFO[data_flag].copy()\n",
    "    train_dataset.info = info\n",
    "    val_dataset.info = info\n",
    "    train_dataset.info[\"n_samples\"][\"train\"] = train_dataset.imgs.shape[0]\n",
    "    val_dataset.info[\"n_samples\"][\"val\"] = val_dataset.imgs.shape[0]\n",
    "    #TODO NEED TO FIX THIS INFO SAMPLE SIZE ISSUE\n",
    "    # train_dataset.info[\"n_samples\"][\"test\"] = 0\n",
    "\n",
    "    aug_list = []\n",
    "    aug_list.append(transforms.RandomCrop(28, padding=4))\n",
    "    aug_list.append(transforms.RandomHorizontalFlip())\n",
    "    preprocess_list = [transforms.ToTensor(), transforms.Normalize(mean=[.5], std=[.5])]\n",
    "\n",
    "    loc_trainer_test = LocalTrainer(ResNet18(in_channels=3, num_classes=8), DataClass, os.path.join(\"blood_models\", \"client_\" + str(i)), n_classes, root=root, epochs=100, lr_adj = [10, 15], aug=aug_list, preprocess=preprocess_list, seed=i)\n",
    "    loc_trainer_test.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hjZqNxfphQ1"
   },
   "source": [
    "3. Federate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBILfUyepi5h"
   },
   "source": [
    "For **demo 2**, we use the global test set as the metric for federation. In future experiements, we will test on the individual validation sets as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4290,
     "status": "ok",
     "timestamp": 1734409491500,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "MHy13xaHc3J_",
    "outputId": "af156b5e-52b2-4736-b426-71fabf6d557e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python_class': 'BloodMNIST', 'description': 'The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3×360×363 pixels are center-cropped into 3×200×200, and then resized into 3×28×28.', 'url': 'https://zenodo.org/records/10519652/files/bloodmnist.npz?download=1', 'MD5': '7053d0359d879ad8a5505303e11de1dc', 'url_64': 'https://zenodo.org/records/10519652/files/bloodmnist_64.npz?download=1', 'MD5_64': '2b94928a2ae4916078ca51e05b6b800b', 'url_128': 'https://zenodo.org/records/10519652/files/bloodmnist_128.npz?download=1', 'MD5_128': 'adace1e0ed228fccda1f39692059dd4c', 'url_224': 'https://zenodo.org/records/10519652/files/bloodmnist_224.npz?download=1', 'MD5_224': 'b718ff6835fcbdb22ba9eacccd7b2601', 'task': 'multi-class', 'label': {'0': 'basophil', '1': 'eosinophil', '2': 'erythroblast', '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)', '4': 'lymphocyte', '5': 'monocyte', '6': 'neutrophil', '7': 'platelet'}, 'n_channels': 3, 'n_samples': {'train': 11959, 'val': 1712, 'test': 3421}, 'license': 'CC BY 4.0'}\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "FedBlood = fedisca.FedISCA(test_data_dir=\"./\", model_weights_dir=\"./blood_models\", exper_dir=\"./blood_exper/federated\", input_size=28, in_channels=3, num_classes=8, batch_size=64, epochs=25, iter_mi=500, lr_steps=[15, 20], log_freq=25, is_medmnist=True, medmnist=\"bloodmnist\")\n",
    "ens_local = FedBlood.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3244189,
     "status": "ok",
     "timestamp": 1734391245534,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "eiUhbY0ttn_6",
    "outputId": "e2841e2f-9004-4679-c9c6-a962f70f42ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7,\n",
      "        0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7,\n",
      "        0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')\n",
      "Dataset BloodMNIST of size 28 (bloodmnist)\n",
      "    Number of datapoints: 3421\n",
      "    Root location: .\n",
      "    Split: test\n",
      "    Task: multi-class\n",
      "    Number of channels: 3\n",
      "    Meaning of labels: {'0': 'basophil', '1': 'eosinophil', '2': 'erythroblast', '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)', '4': 'lymphocyte', '5': 'monocyte', '6': 'neutrophil', '7': 'platelet'}\n",
      "    Number of samples: {'train': 1913, 'val': 478, 'test': 3421}\n",
      "    Description: The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3×360×363 pixels are center-cropped into 3×200×200, and then resized into 3×28×28.\n",
      "    License: CC BY 4.0\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199397.484,\ttarget: 5.303 \tR_feature_loss scaled:\t 199392.125\n",
      "It 25\t Losses: total: 116421.758,\ttarget: 8.698 \tR_feature_loss scaled:\t 116413.016\n",
      "It 50\t Losses: total: 57444.957,\ttarget: 8.067 \tR_feature_loss scaled:\t 57436.852\n",
      "It 75\t Losses: total: 46022.297,\ttarget: 8.741 \tR_feature_loss scaled:\t 46013.520\n",
      "It 100\t Losses: total: 38467.898,\ttarget: 9.215 \tR_feature_loss scaled:\t 38458.648\n",
      "It 125\t Losses: total: 29092.283,\ttarget: 8.791 \tR_feature_loss scaled:\t 29083.459\n",
      "It 150\t Losses: total: 24818.307,\ttarget: 9.756 \tR_feature_loss scaled:\t 24808.520\n",
      "It 175\t Losses: total: 22105.309,\ttarget: 9.741 \tR_feature_loss scaled:\t 22095.539\n",
      "It 200\t Losses: total: 22405.662,\ttarget: 9.742 \tR_feature_loss scaled:\t 22395.891\n",
      "It 225\t Losses: total: 23083.100,\ttarget: 10.166 \tR_feature_loss scaled:\t 23072.906\n",
      "It 250\t Losses: total: 23060.809,\ttarget: 9.881 \tR_feature_loss scaled:\t 23050.900\n",
      "It 275\t Losses: total: 20650.986,\ttarget: 10.397 \tR_feature_loss scaled:\t 20640.564\n",
      "It 300\t Losses: total: 19342.088,\ttarget: 9.980 \tR_feature_loss scaled:\t 19332.082\n",
      "It 325\t Losses: total: 14709.791,\ttarget: 10.415 \tR_feature_loss scaled:\t 14699.352\n",
      "It 350\t Losses: total: 19951.770,\ttarget: 10.277 \tR_feature_loss scaled:\t 19941.469\n",
      "It 375\t Losses: total: 20272.777,\ttarget: 10.087 \tR_feature_loss scaled:\t 20262.666\n",
      "It 400\t Losses: total: 23198.547,\ttarget: 9.992 \tR_feature_loss scaled:\t 23188.531\n",
      "It 425\t Losses: total: 14969.337,\ttarget: 10.125 \tR_feature_loss scaled:\t 14959.189\n",
      "It 450\t Losses: total: 17735.547,\ttarget: 10.624 \tR_feature_loss scaled:\t 17724.900\n",
      "It 475\t Losses: total: 13893.563,\ttarget: 10.416 \tR_feature_loss scaled:\t 13883.125\n",
      "It 500\t Losses: total: 16773.410,\ttarget: 10.501 \tR_feature_loss scaled:\t 16762.887\n",
      "Teacher correct out of 64: 6, loss at 10.481575012207031\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([7, 7, 1, 3, 7, 7, 3, 3, 7, 1, 7, 1, 7, 1, 7, 6, 7, 3, 7, 3, 3, 7, 3, 7,\n",
      "        7, 7, 7, 1, 3], device='cuda:0')\n",
      "Loss: 2.230 | Acc: 48.670% (1665/3421), B. Acc: 0.374%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199229.062,\ttarget: 5.386 \tR_feature_loss scaled:\t 199223.625\n",
      "It 25\t Losses: total: 116402.156,\ttarget: 9.234 \tR_feature_loss scaled:\t 116392.875\n",
      "It 50\t Losses: total: 61449.457,\ttarget: 7.930 \tR_feature_loss scaled:\t 61441.488\n",
      "It 75\t Losses: total: 45118.250,\ttarget: 8.388 \tR_feature_loss scaled:\t 45109.824\n",
      "It 100\t Losses: total: 32895.430,\ttarget: 8.779 \tR_feature_loss scaled:\t 32886.617\n",
      "It 125\t Losses: total: 32147.240,\ttarget: 9.056 \tR_feature_loss scaled:\t 32138.152\n",
      "It 150\t Losses: total: 24392.082,\ttarget: 9.075 \tR_feature_loss scaled:\t 24382.977\n",
      "It 175\t Losses: total: 28437.412,\ttarget: 8.747 \tR_feature_loss scaled:\t 28428.637\n",
      "It 200\t Losses: total: 21201.459,\ttarget: 9.426 \tR_feature_loss scaled:\t 21192.004\n",
      "It 225\t Losses: total: 19058.568,\ttarget: 9.760 \tR_feature_loss scaled:\t 19048.781\n",
      "It 250\t Losses: total: 25804.490,\ttarget: 9.563 \tR_feature_loss scaled:\t 25794.900\n",
      "It 275\t Losses: total: 21633.699,\ttarget: 9.770 \tR_feature_loss scaled:\t 21623.904\n",
      "It 300\t Losses: total: 18094.098,\ttarget: 9.192 \tR_feature_loss scaled:\t 18084.881\n",
      "It 325\t Losses: total: 20326.459,\ttarget: 9.674 \tR_feature_loss scaled:\t 20316.760\n",
      "It 350\t Losses: total: 15263.769,\ttarget: 9.612 \tR_feature_loss scaled:\t 15254.133\n",
      "It 375\t Losses: total: 23093.994,\ttarget: 9.078 \tR_feature_loss scaled:\t 23084.893\n",
      "It 400\t Losses: total: 21292.557,\ttarget: 9.778 \tR_feature_loss scaled:\t 21282.756\n",
      "It 425\t Losses: total: 14925.510,\ttarget: 9.212 \tR_feature_loss scaled:\t 14916.274\n",
      "It 450\t Losses: total: 14730.000,\ttarget: 9.509 \tR_feature_loss scaled:\t 14720.468\n",
      "It 475\t Losses: total: 15243.719,\ttarget: 9.559 \tR_feature_loss scaled:\t 15234.137\n",
      "It 500\t Losses: total: 18007.691,\ttarget: 9.225 \tR_feature_loss scaled:\t 17998.443\n",
      "Teacher correct out of 64: 6, loss at 9.512426376342773\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([3, 1, 6, 3, 1, 3, 3, 7, 1, 3, 6, 1, 1, 6, 3, 1, 6, 7, 1, 7, 7, 1, 7, 6,\n",
      "        7, 7, 6, 3, 3], device='cuda:0')\n",
      "Loss: 1.354 | Acc: 64.016% (2190/3421), B. Acc: 0.490%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199484.906,\ttarget: 5.357 \tR_feature_loss scaled:\t 199479.500\n",
      "It 25\t Losses: total: 119433.336,\ttarget: 9.379 \tR_feature_loss scaled:\t 119423.914\n",
      "It 50\t Losses: total: 54541.500,\ttarget: 7.717 \tR_feature_loss scaled:\t 54533.746\n",
      "It 75\t Losses: total: 42776.453,\ttarget: 8.433 \tR_feature_loss scaled:\t 42767.984\n",
      "It 100\t Losses: total: 33637.344,\ttarget: 8.564 \tR_feature_loss scaled:\t 33628.746\n",
      "It 125\t Losses: total: 28786.135,\ttarget: 9.192 \tR_feature_loss scaled:\t 28776.912\n",
      "It 150\t Losses: total: 33319.020,\ttarget: 9.276 \tR_feature_loss scaled:\t 33309.715\n",
      "It 175\t Losses: total: 20584.336,\ttarget: 9.423 \tR_feature_loss scaled:\t 20574.885\n",
      "It 200\t Losses: total: 21877.260,\ttarget: 9.612 \tR_feature_loss scaled:\t 21867.621\n",
      "It 225\t Losses: total: 19434.223,\ttarget: 9.715 \tR_feature_loss scaled:\t 19424.480\n",
      "It 250\t Losses: total: 19004.479,\ttarget: 9.825 \tR_feature_loss scaled:\t 18994.627\n",
      "It 275\t Losses: total: 21970.748,\ttarget: 9.337 \tR_feature_loss scaled:\t 21961.385\n",
      "It 300\t Losses: total: 18192.758,\ttarget: 10.060 \tR_feature_loss scaled:\t 18182.674\n",
      "It 325\t Losses: total: 18625.986,\ttarget: 9.835 \tR_feature_loss scaled:\t 18616.127\n",
      "It 350\t Losses: total: 16906.605,\ttarget: 10.279 \tR_feature_loss scaled:\t 16896.303\n",
      "It 375\t Losses: total: 19226.818,\ttarget: 10.106 \tR_feature_loss scaled:\t 19216.688\n",
      "It 400\t Losses: total: 15476.637,\ttarget: 9.867 \tR_feature_loss scaled:\t 15466.746\n",
      "It 425\t Losses: total: 13099.477,\ttarget: 10.442 \tR_feature_loss scaled:\t 13089.012\n",
      "It 450\t Losses: total: 16457.615,\ttarget: 9.874 \tR_feature_loss scaled:\t 16447.719\n",
      "It 475\t Losses: total: 15853.135,\ttarget: 10.587 \tR_feature_loss scaled:\t 15842.525\n",
      "It 500\t Losses: total: 17056.473,\ttarget: 9.629 \tR_feature_loss scaled:\t 17046.822\n",
      "Teacher correct out of 64: 7, loss at 9.979214668273926\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([7, 4, 7, 6, 5, 3, 2, 6, 3, 3, 3, 3, 1, 6, 3, 7, 6, 6, 3, 3, 3, 5, 6, 7,\n",
      "        2, 5, 1, 3, 3], device='cuda:0')\n",
      "Loss: 0.805 | Acc: 75.738% (2591/3421), B. Acc: 0.658%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199126.547,\ttarget: 5.384 \tR_feature_loss scaled:\t 199121.109\n",
      "It 25\t Losses: total: 117868.359,\ttarget: 9.477 \tR_feature_loss scaled:\t 117858.836\n",
      "It 50\t Losses: total: 61277.277,\ttarget: 8.534 \tR_feature_loss scaled:\t 61268.703\n",
      "It 75\t Losses: total: 48153.094,\ttarget: 8.447 \tR_feature_loss scaled:\t 48144.609\n",
      "It 100\t Losses: total: 35129.035,\ttarget: 8.594 \tR_feature_loss scaled:\t 35120.406\n",
      "It 125\t Losses: total: 31699.027,\ttarget: 8.980 \tR_feature_loss scaled:\t 31690.016\n",
      "It 150\t Losses: total: 23782.180,\ttarget: 9.436 \tR_feature_loss scaled:\t 23772.713\n",
      "It 175\t Losses: total: 36251.449,\ttarget: 9.575 \tR_feature_loss scaled:\t 36241.844\n",
      "It 200\t Losses: total: 27437.275,\ttarget: 9.840 \tR_feature_loss scaled:\t 27427.406\n",
      "It 225\t Losses: total: 20319.645,\ttarget: 9.716 \tR_feature_loss scaled:\t 20309.900\n",
      "It 250\t Losses: total: 18991.043,\ttarget: 9.530 \tR_feature_loss scaled:\t 18981.486\n",
      "It 275\t Losses: total: 22668.828,\ttarget: 10.367 \tR_feature_loss scaled:\t 22658.436\n",
      "It 300\t Losses: total: 20977.695,\ttarget: 9.892 \tR_feature_loss scaled:\t 20967.777\n",
      "It 325\t Losses: total: 20368.680,\ttarget: 9.912 \tR_feature_loss scaled:\t 20358.742\n",
      "It 350\t Losses: total: 16580.877,\ttarget: 10.396 \tR_feature_loss scaled:\t 16570.457\n",
      "It 375\t Losses: total: 16027.341,\ttarget: 10.435 \tR_feature_loss scaled:\t 16016.881\n",
      "It 400\t Losses: total: 15680.972,\ttarget: 10.320 \tR_feature_loss scaled:\t 15670.627\n",
      "It 425\t Losses: total: 16734.354,\ttarget: 10.250 \tR_feature_loss scaled:\t 16724.080\n",
      "It 450\t Losses: total: 14254.852,\ttarget: 10.659 \tR_feature_loss scaled:\t 14244.169\n",
      "It 475\t Losses: total: 15668.746,\ttarget: 10.638 \tR_feature_loss scaled:\t 15658.085\n",
      "It 500\t Losses: total: 18031.645,\ttarget: 11.057 \tR_feature_loss scaled:\t 18020.564\n",
      "Teacher correct out of 64: 3, loss at 10.3971529006958\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([6, 1, 2, 7, 7, 1, 4, 3, 6, 6, 2, 7, 2, 4, 7, 7, 5, 6, 1, 6, 1, 1, 1, 5,\n",
      "        4, 1, 3, 4, 5], device='cuda:0')\n",
      "Loss: 0.937 | Acc: 75.680% (2589/3421), B. Acc: 0.714%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199038.219,\ttarget: 5.374 \tR_feature_loss scaled:\t 199032.797\n",
      "It 25\t Losses: total: 117232.938,\ttarget: 7.964 \tR_feature_loss scaled:\t 117224.930\n",
      "It 50\t Losses: total: 62588.102,\ttarget: 7.247 \tR_feature_loss scaled:\t 62580.816\n",
      "It 75\t Losses: total: 40499.223,\ttarget: 7.245 \tR_feature_loss scaled:\t 40491.941\n",
      "It 100\t Losses: total: 32788.438,\ttarget: 8.030 \tR_feature_loss scaled:\t 32780.375\n",
      "It 125\t Losses: total: 31355.912,\ttarget: 7.895 \tR_feature_loss scaled:\t 31347.984\n",
      "It 150\t Losses: total: 27415.244,\ttarget: 8.113 \tR_feature_loss scaled:\t 27407.102\n",
      "It 175\t Losses: total: 23370.100,\ttarget: 8.214 \tR_feature_loss scaled:\t 23361.857\n",
      "It 200\t Losses: total: 21808.529,\ttarget: 8.199 \tR_feature_loss scaled:\t 21800.303\n",
      "It 225\t Losses: total: 19537.068,\ttarget: 8.337 \tR_feature_loss scaled:\t 19528.703\n",
      "It 250\t Losses: total: 21647.783,\ttarget: 8.216 \tR_feature_loss scaled:\t 21639.541\n",
      "It 275\t Losses: total: 20270.307,\ttarget: 8.818 \tR_feature_loss scaled:\t 20261.463\n",
      "It 300\t Losses: total: 19895.693,\ttarget: 8.658 \tR_feature_loss scaled:\t 19887.010\n",
      "It 325\t Losses: total: 18898.873,\ttarget: 8.411 \tR_feature_loss scaled:\t 18890.438\n",
      "It 350\t Losses: total: 17128.223,\ttarget: 8.631 \tR_feature_loss scaled:\t 17119.566\n",
      "It 375\t Losses: total: 13774.072,\ttarget: 8.704 \tR_feature_loss scaled:\t 13765.344\n",
      "It 400\t Losses: total: 14592.281,\ttarget: 8.831 \tR_feature_loss scaled:\t 14583.427\n",
      "It 425\t Losses: total: 16200.016,\ttarget: 8.928 \tR_feature_loss scaled:\t 16191.064\n",
      "It 450\t Losses: total: 14348.423,\ttarget: 8.948 \tR_feature_loss scaled:\t 14339.451\n",
      "It 475\t Losses: total: 14620.764,\ttarget: 8.769 \tR_feature_loss scaled:\t 14611.972\n",
      "It 500\t Losses: total: 14866.907,\ttarget: 8.902 \tR_feature_loss scaled:\t 14857.982\n",
      "Teacher correct out of 64: 6, loss at 8.85246467590332\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([3, 5, 5, 5, 6, 2, 5, 2, 6, 1, 6, 1, 1, 1, 4, 1, 1, 7, 2, 7, 1, 4, 1, 0,\n",
      "        6, 6, 6, 5, 7], device='cuda:0')\n",
      "Loss: 0.587 | Acc: 81.789% (2798/3421), B. Acc: 0.781%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199009.875,\ttarget: 5.233 \tR_feature_loss scaled:\t 199004.594\n",
      "It 25\t Losses: total: 115554.234,\ttarget: 7.354 \tR_feature_loss scaled:\t 115546.836\n",
      "It 50\t Losses: total: 59783.742,\ttarget: 7.493 \tR_feature_loss scaled:\t 59776.211\n",
      "It 75\t Losses: total: 37817.680,\ttarget: 8.334 \tR_feature_loss scaled:\t 37809.309\n",
      "It 100\t Losses: total: 31546.691,\ttarget: 8.452 \tR_feature_loss scaled:\t 31538.205\n",
      "It 125\t Losses: total: 27489.797,\ttarget: 8.826 \tR_feature_loss scaled:\t 27480.939\n",
      "It 150\t Losses: total: 24139.438,\ttarget: 9.242 \tR_feature_loss scaled:\t 24130.166\n",
      "It 175\t Losses: total: 24133.402,\ttarget: 9.348 \tR_feature_loss scaled:\t 24124.025\n",
      "It 200\t Losses: total: 20986.863,\ttarget: 9.631 \tR_feature_loss scaled:\t 20977.205\n",
      "It 225\t Losses: total: 21182.979,\ttarget: 9.575 \tR_feature_loss scaled:\t 21173.377\n",
      "It 250\t Losses: total: 17051.285,\ttarget: 9.867 \tR_feature_loss scaled:\t 17041.391\n",
      "It 275\t Losses: total: 17355.340,\ttarget: 10.083 \tR_feature_loss scaled:\t 17345.230\n",
      "It 300\t Losses: total: 20229.318,\ttarget: 9.589 \tR_feature_loss scaled:\t 20219.703\n",
      "It 325\t Losses: total: 19212.434,\ttarget: 9.984 \tR_feature_loss scaled:\t 19202.424\n",
      "It 350\t Losses: total: 17373.410,\ttarget: 10.067 \tR_feature_loss scaled:\t 17363.318\n",
      "It 375\t Losses: total: 16384.268,\ttarget: 10.165 \tR_feature_loss scaled:\t 16374.078\n",
      "It 400\t Losses: total: 14447.096,\ttarget: 10.201 \tR_feature_loss scaled:\t 14436.871\n",
      "It 425\t Losses: total: 15991.403,\ttarget: 10.595 \tR_feature_loss scaled:\t 15980.785\n",
      "It 450\t Losses: total: 15171.246,\ttarget: 10.262 \tR_feature_loss scaled:\t 15160.961\n",
      "It 475\t Losses: total: 14305.016,\ttarget: 10.119 \tR_feature_loss scaled:\t 14294.874\n",
      "It 500\t Losses: total: 15026.392,\ttarget: 10.199 \tR_feature_loss scaled:\t 15016.170\n",
      "Teacher correct out of 64: 1, loss at 10.492757797241211\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([2, 1, 6, 1, 5, 1, 2, 7, 7, 1, 5, 1, 1, 6, 3, 1, 6, 5, 4, 0, 0, 1, 0, 1,\n",
      "        7, 0, 3, 1, 3], device='cuda:0')\n",
      "Loss: 0.529 | Acc: 83.017% (2840/3421), B. Acc: 0.776%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199465.172,\ttarget: 5.274 \tR_feature_loss scaled:\t 199459.844\n",
      "It 25\t Losses: total: 116708.484,\ttarget: 8.404 \tR_feature_loss scaled:\t 116700.039\n",
      "It 50\t Losses: total: 55258.734,\ttarget: 7.465 \tR_feature_loss scaled:\t 55251.230\n",
      "It 75\t Losses: total: 46671.246,\ttarget: 7.350 \tR_feature_loss scaled:\t 46663.859\n",
      "It 100\t Losses: total: 33207.395,\ttarget: 8.144 \tR_feature_loss scaled:\t 33199.215\n",
      "It 125\t Losses: total: 32378.625,\ttarget: 8.264 \tR_feature_loss scaled:\t 32370.328\n",
      "It 150\t Losses: total: 26016.166,\ttarget: 8.607 \tR_feature_loss scaled:\t 26007.529\n",
      "It 175\t Losses: total: 24500.527,\ttarget: 8.758 \tR_feature_loss scaled:\t 24491.740\n",
      "It 200\t Losses: total: 23409.625,\ttarget: 8.746 \tR_feature_loss scaled:\t 23400.852\n",
      "It 225\t Losses: total: 18711.010,\ttarget: 8.700 \tR_feature_loss scaled:\t 18702.283\n",
      "It 250\t Losses: total: 20410.547,\ttarget: 8.258 \tR_feature_loss scaled:\t 20402.262\n",
      "It 275\t Losses: total: 18023.389,\ttarget: 8.678 \tR_feature_loss scaled:\t 18014.686\n",
      "It 300\t Losses: total: 15240.954,\ttarget: 8.631 \tR_feature_loss scaled:\t 15232.299\n",
      "It 325\t Losses: total: 16484.184,\ttarget: 9.028 \tR_feature_loss scaled:\t 16475.131\n",
      "It 350\t Losses: total: 23055.434,\ttarget: 8.318 \tR_feature_loss scaled:\t 23047.092\n",
      "It 375\t Losses: total: 19000.486,\ttarget: 8.711 \tR_feature_loss scaled:\t 18991.752\n",
      "It 400\t Losses: total: 20488.176,\ttarget: 8.707 \tR_feature_loss scaled:\t 20479.445\n",
      "It 425\t Losses: total: 15544.979,\ttarget: 8.607 \tR_feature_loss scaled:\t 15536.350\n",
      "It 450\t Losses: total: 14602.097,\ttarget: 8.579 \tR_feature_loss scaled:\t 14593.494\n",
      "It 475\t Losses: total: 15278.989,\ttarget: 9.454 \tR_feature_loss scaled:\t 15269.513\n",
      "It 500\t Losses: total: 13167.243,\ttarget: 9.165 \tR_feature_loss scaled:\t 13158.056\n",
      "Teacher correct out of 64: 10, loss at 9.166930198669434\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([7, 7, 3, 3, 3, 5, 2, 3, 5, 6, 3, 1, 3, 1, 3, 7, 7, 3, 3, 1, 4, 6, 6, 3,\n",
      "        6, 1, 3, 3, 3], device='cuda:0')\n",
      "Loss: 0.515 | Acc: 83.338% (2851/3421), B. Acc: 0.771%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 198998.062,\ttarget: 5.370 \tR_feature_loss scaled:\t 198992.641\n",
      "It 25\t Losses: total: 120562.555,\ttarget: 8.536 \tR_feature_loss scaled:\t 120553.977\n",
      "It 50\t Losses: total: 59152.512,\ttarget: 6.801 \tR_feature_loss scaled:\t 59145.672\n",
      "It 75\t Losses: total: 46909.074,\ttarget: 6.905 \tR_feature_loss scaled:\t 46902.133\n",
      "It 100\t Losses: total: 35300.949,\ttarget: 8.054 \tR_feature_loss scaled:\t 35292.863\n",
      "It 125\t Losses: total: 38101.617,\ttarget: 8.039 \tR_feature_loss scaled:\t 38093.547\n",
      "It 150\t Losses: total: 28284.541,\ttarget: 8.132 \tR_feature_loss scaled:\t 28276.379\n",
      "It 175\t Losses: total: 23446.555,\ttarget: 8.772 \tR_feature_loss scaled:\t 23437.754\n",
      "It 200\t Losses: total: 28204.477,\ttarget: 9.143 \tR_feature_loss scaled:\t 28195.305\n",
      "It 225\t Losses: total: 23746.102,\ttarget: 9.180 \tR_feature_loss scaled:\t 23736.895\n",
      "It 250\t Losses: total: 22660.570,\ttarget: 9.028 \tR_feature_loss scaled:\t 22651.516\n",
      "It 275\t Losses: total: 19023.172,\ttarget: 8.967 \tR_feature_loss scaled:\t 19014.180\n",
      "It 300\t Losses: total: 18874.682,\ttarget: 8.478 \tR_feature_loss scaled:\t 18866.178\n",
      "It 325\t Losses: total: 20430.445,\ttarget: 8.933 \tR_feature_loss scaled:\t 20421.488\n",
      "It 350\t Losses: total: 18954.324,\ttarget: 8.900 \tR_feature_loss scaled:\t 18945.400\n",
      "It 375\t Losses: total: 19410.365,\ttarget: 9.300 \tR_feature_loss scaled:\t 19401.041\n",
      "It 400\t Losses: total: 20460.775,\ttarget: 9.380 \tR_feature_loss scaled:\t 20451.371\n",
      "It 425\t Losses: total: 15123.587,\ttarget: 9.539 \tR_feature_loss scaled:\t 15114.024\n",
      "It 450\t Losses: total: 15426.001,\ttarget: 8.865 \tR_feature_loss scaled:\t 15417.113\n",
      "It 475\t Losses: total: 19776.072,\ttarget: 9.083 \tR_feature_loss scaled:\t 19766.967\n",
      "It 500\t Losses: total: 14208.234,\ttarget: 8.578 \tR_feature_loss scaled:\t 14199.634\n",
      "Teacher correct out of 64: 14, loss at 8.62411117553711\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([7, 3, 3, 3, 1, 5, 1, 4, 5, 0, 3, 3, 1, 2, 2, 5, 4, 3, 3, 3, 6, 2, 1, 3,\n",
      "        1, 3, 1, 3, 3], device='cuda:0')\n",
      "Loss: 0.506 | Acc: 82.724% (2830/3421), B. Acc: 0.755%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199449.953,\ttarget: 5.200 \tR_feature_loss scaled:\t 199444.703\n",
      "It 25\t Losses: total: 116757.492,\ttarget: 8.296 \tR_feature_loss scaled:\t 116749.156\n",
      "It 50\t Losses: total: 55274.441,\ttarget: 7.480 \tR_feature_loss scaled:\t 55266.922\n",
      "It 75\t Losses: total: 41352.871,\ttarget: 7.404 \tR_feature_loss scaled:\t 41345.430\n",
      "It 100\t Losses: total: 32974.703,\ttarget: 7.615 \tR_feature_loss scaled:\t 32967.055\n",
      "It 125\t Losses: total: 31320.012,\ttarget: 8.042 \tR_feature_loss scaled:\t 31311.938\n",
      "It 150\t Losses: total: 27294.535,\ttarget: 8.130 \tR_feature_loss scaled:\t 27286.375\n",
      "It 175\t Losses: total: 24634.469,\ttarget: 8.225 \tR_feature_loss scaled:\t 24626.215\n",
      "It 200\t Losses: total: 28595.482,\ttarget: 8.325 \tR_feature_loss scaled:\t 28587.129\n",
      "It 225\t Losses: total: 25609.441,\ttarget: 7.974 \tR_feature_loss scaled:\t 25601.441\n",
      "It 250\t Losses: total: 18032.863,\ttarget: 8.614 \tR_feature_loss scaled:\t 18024.223\n",
      "It 275\t Losses: total: 23727.057,\ttarget: 8.504 \tR_feature_loss scaled:\t 23718.527\n",
      "It 300\t Losses: total: 16881.893,\ttarget: 8.351 \tR_feature_loss scaled:\t 16873.516\n",
      "It 325\t Losses: total: 26091.438,\ttarget: 8.141 \tR_feature_loss scaled:\t 26083.271\n",
      "It 350\t Losses: total: 17951.881,\ttarget: 8.328 \tR_feature_loss scaled:\t 17943.529\n",
      "It 375\t Losses: total: 18096.082,\ttarget: 9.006 \tR_feature_loss scaled:\t 18087.053\n",
      "It 400\t Losses: total: 15570.421,\ttarget: 8.891 \tR_feature_loss scaled:\t 15561.506\n",
      "It 425\t Losses: total: 12656.301,\ttarget: 8.879 \tR_feature_loss scaled:\t 12647.398\n",
      "It 450\t Losses: total: 16052.215,\ttarget: 8.684 \tR_feature_loss scaled:\t 16043.508\n",
      "It 475\t Losses: total: 13898.336,\ttarget: 8.854 \tR_feature_loss scaled:\t 13889.459\n",
      "It 500\t Losses: total: 15215.886,\ttarget: 8.564 \tR_feature_loss scaled:\t 15207.300\n",
      "Teacher correct out of 64: 9, loss at 8.494855880737305\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([1, 2, 0, 6, 1, 6, 0, 7, 1, 4, 6, 3, 5, 7, 6, 3, 3, 7, 3, 6, 3, 3, 6, 6,\n",
      "        1, 3, 0, 1, 2], device='cuda:0')\n",
      "Loss: 0.598 | Acc: 80.210% (2744/3421), B. Acc: 0.737%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199228.375,\ttarget: 5.351 \tR_feature_loss scaled:\t 199222.969\n",
      "It 25\t Losses: total: 117349.266,\ttarget: 8.436 \tR_feature_loss scaled:\t 117340.789\n",
      "It 50\t Losses: total: 58172.926,\ttarget: 7.344 \tR_feature_loss scaled:\t 58165.543\n",
      "It 75\t Losses: total: 44257.867,\ttarget: 8.383 \tR_feature_loss scaled:\t 44249.449\n",
      "It 100\t Losses: total: 34177.996,\ttarget: 8.465 \tR_feature_loss scaled:\t 34169.496\n",
      "It 125\t Losses: total: 34391.879,\ttarget: 8.819 \tR_feature_loss scaled:\t 34383.027\n",
      "It 150\t Losses: total: 24067.924,\ttarget: 8.601 \tR_feature_loss scaled:\t 24059.293\n",
      "It 175\t Losses: total: 24214.105,\ttarget: 8.791 \tR_feature_loss scaled:\t 24205.285\n",
      "It 200\t Losses: total: 23236.197,\ttarget: 8.795 \tR_feature_loss scaled:\t 23227.375\n",
      "It 225\t Losses: total: 25355.516,\ttarget: 9.767 \tR_feature_loss scaled:\t 25345.721\n",
      "It 250\t Losses: total: 17050.340,\ttarget: 8.821 \tR_feature_loss scaled:\t 17041.492\n",
      "It 275\t Losses: total: 21162.639,\ttarget: 9.624 \tR_feature_loss scaled:\t 21152.988\n",
      "It 300\t Losses: total: 25380.145,\ttarget: 8.936 \tR_feature_loss scaled:\t 25371.184\n",
      "It 325\t Losses: total: 26225.871,\ttarget: 8.929 \tR_feature_loss scaled:\t 26216.918\n",
      "It 350\t Losses: total: 19177.043,\ttarget: 9.327 \tR_feature_loss scaled:\t 19167.691\n",
      "It 375\t Losses: total: 14982.887,\ttarget: 9.297 \tR_feature_loss scaled:\t 14973.565\n",
      "It 400\t Losses: total: 21137.857,\ttarget: 9.242 \tR_feature_loss scaled:\t 21128.592\n",
      "It 425\t Losses: total: 20153.967,\ttarget: 8.737 \tR_feature_loss scaled:\t 20145.207\n",
      "It 450\t Losses: total: 16700.438,\ttarget: 9.196 \tR_feature_loss scaled:\t 16691.219\n",
      "It 475\t Losses: total: 17779.074,\ttarget: 8.551 \tR_feature_loss scaled:\t 17770.500\n",
      "It 500\t Losses: total: 14036.381,\ttarget: 9.200 \tR_feature_loss scaled:\t 14027.158\n",
      "Teacher correct out of 64: 12, loss at 9.079695701599121\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([6, 1, 3, 3, 5, 3, 1, 1, 1, 6, 1, 6, 5, 6, 1, 1, 4, 3, 0, 3, 5, 3, 7, 3,\n",
      "        6, 7, 4, 2, 2], device='cuda:0')\n",
      "Loss: 0.416 | Acc: 85.472% (2924/3421), B. Acc: 0.789%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199344.719,\ttarget: 5.262 \tR_feature_loss scaled:\t 199339.406\n",
      "It 25\t Losses: total: 118003.844,\ttarget: 8.722 \tR_feature_loss scaled:\t 117995.078\n",
      "It 50\t Losses: total: 54750.887,\ttarget: 7.645 \tR_feature_loss scaled:\t 54743.203\n",
      "It 75\t Losses: total: 39207.102,\ttarget: 8.184 \tR_feature_loss scaled:\t 39198.883\n",
      "It 100\t Losses: total: 34271.691,\ttarget: 8.374 \tR_feature_loss scaled:\t 34263.285\n",
      "It 125\t Losses: total: 36945.242,\ttarget: 8.617 \tR_feature_loss scaled:\t 36936.594\n",
      "It 150\t Losses: total: 25039.854,\ttarget: 8.706 \tR_feature_loss scaled:\t 25031.117\n",
      "It 175\t Losses: total: 26126.621,\ttarget: 9.267 \tR_feature_loss scaled:\t 26117.324\n",
      "It 200\t Losses: total: 21744.465,\ttarget: 9.154 \tR_feature_loss scaled:\t 21735.283\n",
      "It 225\t Losses: total: 22598.703,\ttarget: 9.056 \tR_feature_loss scaled:\t 22589.621\n",
      "It 250\t Losses: total: 24959.270,\ttarget: 9.211 \tR_feature_loss scaled:\t 24950.031\n",
      "It 275\t Losses: total: 18780.279,\ttarget: 9.432 \tR_feature_loss scaled:\t 18770.822\n",
      "It 300\t Losses: total: 16635.189,\ttarget: 9.309 \tR_feature_loss scaled:\t 16625.855\n",
      "It 325\t Losses: total: 22260.645,\ttarget: 9.067 \tR_feature_loss scaled:\t 22251.553\n",
      "It 350\t Losses: total: 15401.242,\ttarget: 9.614 \tR_feature_loss scaled:\t 15391.604\n",
      "It 375\t Losses: total: 16981.426,\ttarget: 9.640 \tR_feature_loss scaled:\t 16971.762\n",
      "It 400\t Losses: total: 21235.098,\ttarget: 9.918 \tR_feature_loss scaled:\t 21225.156\n",
      "It 425\t Losses: total: 16270.165,\ttarget: 9.946 \tR_feature_loss scaled:\t 16260.195\n",
      "It 450\t Losses: total: 19946.406,\ttarget: 9.336 \tR_feature_loss scaled:\t 19937.047\n",
      "It 475\t Losses: total: 12590.405,\ttarget: 9.619 \tR_feature_loss scaled:\t 12580.764\n",
      "It 500\t Losses: total: 15612.231,\ttarget: 9.930 \tR_feature_loss scaled:\t 15602.279\n",
      "Teacher correct out of 64: 8, loss at 9.604229927062988\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([3, 5, 7, 1, 1, 3, 6, 4, 7, 3, 7, 1, 1, 1, 6, 6, 6, 6, 3, 1, 2, 3, 3, 3,\n",
      "        3, 3, 6, 0, 5], device='cuda:0')\n",
      "Loss: 0.388 | Acc: 86.846% (2971/3421), B. Acc: 0.824%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199227.688,\ttarget: 5.201 \tR_feature_loss scaled:\t 199222.438\n",
      "It 25\t Losses: total: 117739.828,\ttarget: 7.867 \tR_feature_loss scaled:\t 117731.922\n",
      "It 50\t Losses: total: 62022.867,\ttarget: 7.640 \tR_feature_loss scaled:\t 62015.188\n",
      "It 75\t Losses: total: 38415.445,\ttarget: 7.483 \tR_feature_loss scaled:\t 38407.926\n",
      "It 100\t Losses: total: 35487.191,\ttarget: 8.039 \tR_feature_loss scaled:\t 35479.117\n",
      "It 125\t Losses: total: 29270.059,\ttarget: 7.895 \tR_feature_loss scaled:\t 29262.133\n",
      "It 150\t Losses: total: 23092.576,\ttarget: 8.188 \tR_feature_loss scaled:\t 23084.357\n",
      "It 175\t Losses: total: 23613.291,\ttarget: 8.380 \tR_feature_loss scaled:\t 23604.883\n",
      "It 200\t Losses: total: 23087.779,\ttarget: 8.557 \tR_feature_loss scaled:\t 23079.195\n",
      "It 225\t Losses: total: 23130.322,\ttarget: 8.724 \tR_feature_loss scaled:\t 23121.572\n",
      "It 250\t Losses: total: 20293.645,\ttarget: 8.803 \tR_feature_loss scaled:\t 20284.814\n",
      "It 275\t Losses: total: 18808.658,\ttarget: 8.460 \tR_feature_loss scaled:\t 18800.172\n",
      "It 300\t Losses: total: 17585.105,\ttarget: 8.419 \tR_feature_loss scaled:\t 17576.662\n",
      "It 325\t Losses: total: 19254.943,\ttarget: 8.468 \tR_feature_loss scaled:\t 19246.451\n",
      "It 350\t Losses: total: 17390.299,\ttarget: 8.509 \tR_feature_loss scaled:\t 17381.766\n",
      "It 375\t Losses: total: 16226.489,\ttarget: 8.497 \tR_feature_loss scaled:\t 16217.969\n",
      "It 400\t Losses: total: 13857.884,\ttarget: 8.782 \tR_feature_loss scaled:\t 13849.078\n",
      "It 425\t Losses: total: 16047.069,\ttarget: 8.617 \tR_feature_loss scaled:\t 16038.430\n",
      "It 450\t Losses: total: 23743.164,\ttarget: 8.905 \tR_feature_loss scaled:\t 23734.236\n",
      "It 475\t Losses: total: 17571.469,\ttarget: 8.633 \tR_feature_loss scaled:\t 17562.812\n",
      "It 500\t Losses: total: 13349.836,\ttarget: 8.608 \tR_feature_loss scaled:\t 13341.205\n",
      "Teacher correct out of 64: 11, loss at 8.404871940612793\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([1, 3, 7, 3, 7, 6, 3, 6, 1, 4, 7, 2, 3, 7, 1, 3, 6, 2, 6, 7, 3, 1, 0, 1,\n",
      "        5, 5, 1, 6, 7], device='cuda:0')\n",
      "Loss: 0.332 | Acc: 88.834% (3039/3421), B. Acc: 0.851%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199554.500,\ttarget: 5.355 \tR_feature_loss scaled:\t 199549.094\n",
      "It 25\t Losses: total: 121579.914,\ttarget: 9.158 \tR_feature_loss scaled:\t 121570.711\n",
      "It 50\t Losses: total: 57527.578,\ttarget: 7.154 \tR_feature_loss scaled:\t 57520.387\n",
      "It 75\t Losses: total: 42160.031,\ttarget: 7.549 \tR_feature_loss scaled:\t 42152.445\n",
      "It 100\t Losses: total: 36172.383,\ttarget: 7.906 \tR_feature_loss scaled:\t 36164.441\n",
      "It 125\t Losses: total: 27833.947,\ttarget: 7.872 \tR_feature_loss scaled:\t 27826.043\n",
      "It 150\t Losses: total: 31373.115,\ttarget: 7.900 \tR_feature_loss scaled:\t 31365.186\n",
      "It 175\t Losses: total: 30903.424,\ttarget: 8.473 \tR_feature_loss scaled:\t 30894.922\n",
      "It 200\t Losses: total: 22035.133,\ttarget: 8.110 \tR_feature_loss scaled:\t 22026.994\n",
      "It 225\t Losses: total: 20239.717,\ttarget: 8.065 \tR_feature_loss scaled:\t 20231.625\n",
      "It 250\t Losses: total: 19781.100,\ttarget: 8.276 \tR_feature_loss scaled:\t 19772.797\n",
      "It 275\t Losses: total: 25016.230,\ttarget: 8.653 \tR_feature_loss scaled:\t 25007.551\n",
      "It 300\t Losses: total: 28936.396,\ttarget: 8.430 \tR_feature_loss scaled:\t 28927.941\n",
      "It 325\t Losses: total: 18821.441,\ttarget: 8.699 \tR_feature_loss scaled:\t 18812.717\n",
      "It 350\t Losses: total: 17774.957,\ttarget: 8.967 \tR_feature_loss scaled:\t 17765.965\n",
      "It 375\t Losses: total: 17521.777,\ttarget: 8.970 \tR_feature_loss scaled:\t 17512.783\n",
      "It 400\t Losses: total: 17214.656,\ttarget: 8.935 \tR_feature_loss scaled:\t 17205.697\n",
      "It 425\t Losses: total: 18933.414,\ttarget: 9.389 \tR_feature_loss scaled:\t 18924.002\n",
      "It 450\t Losses: total: 24837.141,\ttarget: 8.931 \tR_feature_loss scaled:\t 24828.186\n",
      "It 475\t Losses: total: 19370.092,\ttarget: 9.480 \tR_feature_loss scaled:\t 19360.588\n",
      "It 500\t Losses: total: 18773.145,\ttarget: 9.194 \tR_feature_loss scaled:\t 18763.928\n",
      "Teacher correct out of 64: 7, loss at 9.328315734863281\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([1, 6, 6, 6, 5, 4, 5, 4, 3, 7, 7, 5, 4, 4, 6, 7, 6, 4, 1, 3, 2, 3, 2, 4,\n",
      "        0, 6, 5, 4, 5], device='cuda:0')\n",
      "Loss: 0.349 | Acc: 89.009% (3045/3421), B. Acc: 0.855%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 198858.844,\ttarget: 5.257 \tR_feature_loss scaled:\t 198853.531\n",
      "It 25\t Losses: total: 120555.742,\ttarget: 7.937 \tR_feature_loss scaled:\t 120547.766\n",
      "It 50\t Losses: total: 56527.418,\ttarget: 6.847 \tR_feature_loss scaled:\t 56520.531\n",
      "It 75\t Losses: total: 40224.434,\ttarget: 7.307 \tR_feature_loss scaled:\t 40217.090\n",
      "It 100\t Losses: total: 40597.930,\ttarget: 7.388 \tR_feature_loss scaled:\t 40590.508\n",
      "It 125\t Losses: total: 26559.141,\ttarget: 8.037 \tR_feature_loss scaled:\t 26551.072\n",
      "It 150\t Losses: total: 37989.117,\ttarget: 7.897 \tR_feature_loss scaled:\t 37981.191\n",
      "It 175\t Losses: total: 27651.572,\ttarget: 8.339 \tR_feature_loss scaled:\t 27643.205\n",
      "It 200\t Losses: total: 20488.176,\ttarget: 8.558 \tR_feature_loss scaled:\t 20479.590\n",
      "It 225\t Losses: total: 19884.354,\ttarget: 8.742 \tR_feature_loss scaled:\t 19875.584\n",
      "It 250\t Losses: total: 27013.178,\ttarget: 8.526 \tR_feature_loss scaled:\t 27004.625\n",
      "It 275\t Losses: total: 16858.213,\ttarget: 8.336 \tR_feature_loss scaled:\t 16849.852\n",
      "It 300\t Losses: total: 18713.322,\ttarget: 8.720 \tR_feature_loss scaled:\t 18704.578\n",
      "It 325\t Losses: total: 18787.104,\ttarget: 8.799 \tR_feature_loss scaled:\t 18778.279\n",
      "It 350\t Losses: total: 22484.357,\ttarget: 9.236 \tR_feature_loss scaled:\t 22475.098\n",
      "It 375\t Losses: total: 15880.608,\ttarget: 8.968 \tR_feature_loss scaled:\t 15871.616\n",
      "It 400\t Losses: total: 15182.225,\ttarget: 8.579 \tR_feature_loss scaled:\t 15173.622\n",
      "It 425\t Losses: total: 16233.899,\ttarget: 8.759 \tR_feature_loss scaled:\t 16225.117\n",
      "It 450\t Losses: total: 16056.997,\ttarget: 8.551 \tR_feature_loss scaled:\t 16048.423\n",
      "It 475\t Losses: total: 13329.153,\ttarget: 8.875 \tR_feature_loss scaled:\t 13320.255\n",
      "It 500\t Losses: total: 16597.008,\ttarget: 8.881 \tR_feature_loss scaled:\t 16588.104\n",
      "Teacher correct out of 64: 8, loss at 9.122889518737793\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([7, 3, 3, 1, 7, 1, 3, 5, 6, 6, 3, 1, 1, 3, 6, 7, 7, 7, 3, 6, 3, 0, 3, 6,\n",
      "        6, 3, 7, 6, 7], device='cuda:0')\n",
      "Loss: 0.338 | Acc: 88.191% (3017/3421), B. Acc: 0.836%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199090.562,\ttarget: 5.298 \tR_feature_loss scaled:\t 199085.219\n",
      "It 25\t Losses: total: 119374.156,\ttarget: 8.476 \tR_feature_loss scaled:\t 119365.641\n",
      "It 50\t Losses: total: 56559.672,\ttarget: 7.148 \tR_feature_loss scaled:\t 56552.484\n",
      "It 75\t Losses: total: 37907.668,\ttarget: 6.980 \tR_feature_loss scaled:\t 37900.652\n",
      "It 100\t Losses: total: 34398.609,\ttarget: 7.522 \tR_feature_loss scaled:\t 34391.055\n",
      "It 125\t Losses: total: 27919.141,\ttarget: 7.224 \tR_feature_loss scaled:\t 27911.885\n",
      "It 150\t Losses: total: 31625.848,\ttarget: 6.895 \tR_feature_loss scaled:\t 31618.922\n",
      "It 175\t Losses: total: 25870.912,\ttarget: 7.774 \tR_feature_loss scaled:\t 25863.109\n",
      "It 200\t Losses: total: 21718.387,\ttarget: 7.656 \tR_feature_loss scaled:\t 21710.703\n",
      "It 225\t Losses: total: 23777.092,\ttarget: 7.479 \tR_feature_loss scaled:\t 23769.586\n",
      "It 250\t Losses: total: 26206.180,\ttarget: 7.450 \tR_feature_loss scaled:\t 26198.703\n",
      "It 275\t Losses: total: 17295.564,\ttarget: 7.573 \tR_feature_loss scaled:\t 17287.967\n",
      "It 300\t Losses: total: 18287.631,\ttarget: 7.858 \tR_feature_loss scaled:\t 18279.748\n",
      "It 325\t Losses: total: 17361.416,\ttarget: 7.725 \tR_feature_loss scaled:\t 17353.666\n",
      "It 350\t Losses: total: 19378.043,\ttarget: 7.925 \tR_feature_loss scaled:\t 19370.094\n",
      "It 375\t Losses: total: 21378.312,\ttarget: 7.594 \tR_feature_loss scaled:\t 21370.695\n",
      "It 400\t Losses: total: 15239.213,\ttarget: 7.812 \tR_feature_loss scaled:\t 15231.377\n",
      "It 425\t Losses: total: 18656.639,\ttarget: 8.085 \tR_feature_loss scaled:\t 18648.531\n",
      "It 450\t Losses: total: 14176.273,\ttarget: 7.676 \tR_feature_loss scaled:\t 14168.574\n",
      "It 475\t Losses: total: 16007.543,\ttarget: 7.616 \tR_feature_loss scaled:\t 15999.904\n",
      "It 500\t Losses: total: 16675.357,\ttarget: 7.802 \tR_feature_loss scaled:\t 16667.533\n",
      "Teacher correct out of 64: 15, loss at 7.877984046936035\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([0, 6, 7, 3, 3, 5, 1, 4, 5, 7, 1, 1, 7, 3, 7, 3, 6, 2, 3, 1, 0, 6, 5, 5,\n",
      "        1, 3, 5, 7, 3], device='cuda:0')\n",
      "Loss: 0.380 | Acc: 88.220% (3018/3421), B. Acc: 0.845%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199119.500,\ttarget: 5.380 \tR_feature_loss scaled:\t 199114.062\n",
      "It 25\t Losses: total: 116003.305,\ttarget: 8.038 \tR_feature_loss scaled:\t 115995.227\n",
      "It 50\t Losses: total: 62543.711,\ttarget: 6.665 \tR_feature_loss scaled:\t 62537.008\n",
      "It 75\t Losses: total: 40155.453,\ttarget: 7.602 \tR_feature_loss scaled:\t 40147.816\n",
      "It 100\t Losses: total: 30832.818,\ttarget: 7.991 \tR_feature_loss scaled:\t 30824.793\n",
      "It 125\t Losses: total: 29551.959,\ttarget: 8.219 \tR_feature_loss scaled:\t 29543.709\n",
      "It 150\t Losses: total: 27258.404,\ttarget: 8.059 \tR_feature_loss scaled:\t 27250.314\n",
      "It 175\t Losses: total: 24825.682,\ttarget: 8.028 \tR_feature_loss scaled:\t 24817.625\n",
      "It 200\t Losses: total: 20439.523,\ttarget: 8.035 \tR_feature_loss scaled:\t 20431.461\n",
      "It 225\t Losses: total: 21352.172,\ttarget: 8.071 \tR_feature_loss scaled:\t 21344.074\n",
      "It 250\t Losses: total: 20418.273,\ttarget: 8.078 \tR_feature_loss scaled:\t 20410.170\n",
      "It 275\t Losses: total: 17543.480,\ttarget: 8.482 \tR_feature_loss scaled:\t 17534.973\n",
      "It 300\t Losses: total: 17143.859,\ttarget: 8.534 \tR_feature_loss scaled:\t 17135.301\n",
      "It 325\t Losses: total: 21938.025,\ttarget: 7.814 \tR_feature_loss scaled:\t 21930.186\n",
      "It 350\t Losses: total: 23078.443,\ttarget: 8.320 \tR_feature_loss scaled:\t 23070.100\n",
      "It 375\t Losses: total: 21630.588,\ttarget: 9.077 \tR_feature_loss scaled:\t 21621.486\n",
      "It 400\t Losses: total: 18053.611,\ttarget: 8.268 \tR_feature_loss scaled:\t 18045.320\n",
      "It 425\t Losses: total: 17722.799,\ttarget: 8.469 \tR_feature_loss scaled:\t 17714.307\n",
      "It 450\t Losses: total: 15137.523,\ttarget: 8.373 \tR_feature_loss scaled:\t 15129.127\n",
      "It 475\t Losses: total: 14964.088,\ttarget: 8.407 \tR_feature_loss scaled:\t 14955.658\n",
      "It 500\t Losses: total: 13672.134,\ttarget: 8.362 \tR_feature_loss scaled:\t 13663.749\n",
      "Teacher correct out of 64: 14, loss at 8.362041473388672\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([3, 5, 6, 7, 5, 7, 6, 1, 7, 3, 3, 7, 1, 7, 7, 1, 7, 7, 7, 6, 6, 2, 5, 1,\n",
      "        1, 1, 7, 1, 3], device='cuda:0')\n",
      "Loss: 0.337 | Acc: 88.921% (3042/3421), B. Acc: 0.862%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 198859.953,\ttarget: 5.246 \tR_feature_loss scaled:\t 198854.656\n",
      "It 25\t Losses: total: 116999.180,\ttarget: 8.559 \tR_feature_loss scaled:\t 116990.578\n",
      "It 50\t Losses: total: 54799.695,\ttarget: 7.397 \tR_feature_loss scaled:\t 54792.258\n",
      "It 75\t Losses: total: 43146.406,\ttarget: 8.032 \tR_feature_loss scaled:\t 43138.336\n",
      "It 100\t Losses: total: 34496.031,\ttarget: 8.728 \tR_feature_loss scaled:\t 34487.270\n",
      "It 125\t Losses: total: 27923.107,\ttarget: 9.286 \tR_feature_loss scaled:\t 27913.791\n",
      "It 150\t Losses: total: 23555.396,\ttarget: 9.446 \tR_feature_loss scaled:\t 23545.922\n",
      "It 175\t Losses: total: 20887.768,\ttarget: 9.526 \tR_feature_loss scaled:\t 20878.213\n",
      "It 200\t Losses: total: 33934.457,\ttarget: 9.439 \tR_feature_loss scaled:\t 33924.992\n",
      "It 225\t Losses: total: 22526.012,\ttarget: 9.392 \tR_feature_loss scaled:\t 22516.594\n",
      "It 250\t Losses: total: 23440.635,\ttarget: 9.386 \tR_feature_loss scaled:\t 23431.223\n",
      "It 275\t Losses: total: 19033.314,\ttarget: 9.920 \tR_feature_loss scaled:\t 19023.367\n",
      "It 300\t Losses: total: 17507.396,\ttarget: 9.924 \tR_feature_loss scaled:\t 17497.447\n",
      "It 325\t Losses: total: 14741.048,\ttarget: 9.905 \tR_feature_loss scaled:\t 14731.118\n",
      "It 350\t Losses: total: 15023.272,\ttarget: 9.601 \tR_feature_loss scaled:\t 15013.647\n",
      "It 375\t Losses: total: 15138.552,\ttarget: 9.754 \tR_feature_loss scaled:\t 15128.774\n",
      "It 400\t Losses: total: 15506.466,\ttarget: 9.823 \tR_feature_loss scaled:\t 15496.619\n",
      "It 425\t Losses: total: 15074.036,\ttarget: 9.327 \tR_feature_loss scaled:\t 15064.687\n",
      "It 450\t Losses: total: 14317.685,\ttarget: 10.169 \tR_feature_loss scaled:\t 14307.492\n",
      "It 475\t Losses: total: 17292.406,\ttarget: 9.652 \tR_feature_loss scaled:\t 17282.732\n",
      "It 500\t Losses: total: 13470.951,\ttarget: 9.941 \tR_feature_loss scaled:\t 13460.987\n",
      "Teacher correct out of 64: 7, loss at 10.099687576293945\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([5, 7, 1, 1, 1, 3, 3, 5, 4, 1, 6, 1, 1, 6, 5, 5, 6, 1, 3, 4, 1, 2, 1, 7,\n",
      "        2, 5, 2, 3, 5], device='cuda:0')\n",
      "Loss: 0.472 | Acc: 85.911% (2939/3421), B. Acc: 0.807%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199344.875,\ttarget: 5.310 \tR_feature_loss scaled:\t 199339.516\n",
      "It 25\t Losses: total: 123261.898,\ttarget: 8.803 \tR_feature_loss scaled:\t 123253.055\n",
      "It 50\t Losses: total: 54749.867,\ttarget: 7.140 \tR_feature_loss scaled:\t 54742.688\n",
      "It 75\t Losses: total: 41489.734,\ttarget: 7.591 \tR_feature_loss scaled:\t 41482.105\n",
      "It 100\t Losses: total: 37304.766,\ttarget: 7.522 \tR_feature_loss scaled:\t 37297.211\n",
      "It 125\t Losses: total: 31272.717,\ttarget: 8.006 \tR_feature_loss scaled:\t 31264.680\n",
      "It 150\t Losses: total: 23894.750,\ttarget: 8.555 \tR_feature_loss scaled:\t 23886.164\n",
      "It 175\t Losses: total: 22348.664,\ttarget: 8.554 \tR_feature_loss scaled:\t 22340.080\n",
      "It 200\t Losses: total: 22658.336,\ttarget: 8.833 \tR_feature_loss scaled:\t 22649.475\n",
      "It 225\t Losses: total: 21347.627,\ttarget: 9.112 \tR_feature_loss scaled:\t 21338.488\n",
      "It 250\t Losses: total: 17862.779,\ttarget: 9.175 \tR_feature_loss scaled:\t 17853.578\n",
      "It 275\t Losses: total: 20120.758,\ttarget: 9.243 \tR_feature_loss scaled:\t 20111.488\n",
      "It 300\t Losses: total: 15744.137,\ttarget: 9.045 \tR_feature_loss scaled:\t 15735.066\n",
      "It 325\t Losses: total: 27003.645,\ttarget: 8.892 \tR_feature_loss scaled:\t 26994.729\n",
      "It 350\t Losses: total: 29633.686,\ttarget: 9.589 \tR_feature_loss scaled:\t 29624.072\n",
      "It 375\t Losses: total: 16024.915,\ttarget: 8.768 \tR_feature_loss scaled:\t 16016.123\n",
      "It 400\t Losses: total: 17011.842,\ttarget: 9.335 \tR_feature_loss scaled:\t 17002.482\n",
      "It 425\t Losses: total: 15113.993,\ttarget: 9.418 \tR_feature_loss scaled:\t 15104.552\n",
      "It 450\t Losses: total: 15094.663,\ttarget: 9.236 \tR_feature_loss scaled:\t 15085.404\n",
      "It 475\t Losses: total: 14507.966,\ttarget: 9.156 \tR_feature_loss scaled:\t 14498.787\n",
      "It 500\t Losses: total: 16495.018,\ttarget: 9.411 \tR_feature_loss scaled:\t 16485.584\n",
      "Teacher correct out of 64: 10, loss at 9.311556816101074\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([7, 3, 1, 6, 5, 1, 3, 7, 5, 7, 2, 1, 7, 6, 5, 7, 3, 6, 7, 1, 7, 1, 7, 6,\n",
      "        1, 4, 4, 3, 3], device='cuda:0')\n",
      "Loss: 0.364 | Acc: 88.308% (3021/3421), B. Acc: 0.856%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199209.109,\ttarget: 5.271 \tR_feature_loss scaled:\t 199203.781\n",
      "It 25\t Losses: total: 115729.414,\ttarget: 8.393 \tR_feature_loss scaled:\t 115720.977\n",
      "It 50\t Losses: total: 57875.539,\ttarget: 8.295 \tR_feature_loss scaled:\t 57867.207\n",
      "It 75\t Losses: total: 37772.008,\ttarget: 8.608 \tR_feature_loss scaled:\t 37763.363\n",
      "It 100\t Losses: total: 30621.135,\ttarget: 9.071 \tR_feature_loss scaled:\t 30612.031\n",
      "It 125\t Losses: total: 28952.914,\ttarget: 9.437 \tR_feature_loss scaled:\t 28943.445\n",
      "It 150\t Losses: total: 25166.109,\ttarget: 9.586 \tR_feature_loss scaled:\t 25156.494\n",
      "It 175\t Losses: total: 23118.637,\ttarget: 9.446 \tR_feature_loss scaled:\t 23109.162\n",
      "It 200\t Losses: total: 17933.566,\ttarget: 9.174 \tR_feature_loss scaled:\t 17924.365\n",
      "It 225\t Losses: total: 21832.840,\ttarget: 9.282 \tR_feature_loss scaled:\t 21823.531\n",
      "It 250\t Losses: total: 19624.254,\ttarget: 9.601 \tR_feature_loss scaled:\t 19614.627\n",
      "It 275\t Losses: total: 16563.031,\ttarget: 9.770 \tR_feature_loss scaled:\t 16553.236\n",
      "It 300\t Losses: total: 20886.252,\ttarget: 9.408 \tR_feature_loss scaled:\t 20876.818\n",
      "It 325\t Losses: total: 15162.283,\ttarget: 9.460 \tR_feature_loss scaled:\t 15152.799\n",
      "It 350\t Losses: total: 21659.510,\ttarget: 9.900 \tR_feature_loss scaled:\t 21649.586\n",
      "It 375\t Losses: total: 17807.791,\ttarget: 10.181 \tR_feature_loss scaled:\t 17797.586\n",
      "It 400\t Losses: total: 15528.114,\ttarget: 9.603 \tR_feature_loss scaled:\t 15518.488\n",
      "It 425\t Losses: total: 13929.479,\ttarget: 9.613 \tR_feature_loss scaled:\t 13919.844\n",
      "It 450\t Losses: total: 16741.205,\ttarget: 9.369 \tR_feature_loss scaled:\t 16731.814\n",
      "It 475\t Losses: total: 12676.591,\ttarget: 9.867 \tR_feature_loss scaled:\t 12666.701\n",
      "It 500\t Losses: total: 13204.910,\ttarget: 9.648 \tR_feature_loss scaled:\t 13195.240\n",
      "Teacher correct out of 64: 9, loss at 9.927800178527832\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([3, 2, 6, 6, 6, 7, 4, 3, 7, 6, 7, 0, 2, 7, 6, 3, 6, 5, 6, 6, 7, 1, 6, 0,\n",
      "        1, 7, 6, 7, 7], device='cuda:0')\n",
      "Loss: 0.297 | Acc: 90.237% (3087/3421), B. Acc: 0.876%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199408.812,\ttarget: 5.323 \tR_feature_loss scaled:\t 199403.438\n",
      "It 25\t Losses: total: 119902.836,\ttarget: 8.625 \tR_feature_loss scaled:\t 119894.172\n",
      "It 50\t Losses: total: 57567.000,\ttarget: 7.127 \tR_feature_loss scaled:\t 57559.836\n",
      "It 75\t Losses: total: 39772.105,\ttarget: 7.964 \tR_feature_loss scaled:\t 39764.105\n",
      "It 100\t Losses: total: 34793.781,\ttarget: 7.870 \tR_feature_loss scaled:\t 34785.879\n",
      "It 125\t Losses: total: 26156.777,\ttarget: 8.515 \tR_feature_loss scaled:\t 26148.230\n",
      "It 150\t Losses: total: 25014.574,\ttarget: 8.683 \tR_feature_loss scaled:\t 25005.861\n",
      "It 175\t Losses: total: 23315.234,\ttarget: 8.522 \tR_feature_loss scaled:\t 23306.684\n",
      "It 200\t Losses: total: 22233.656,\ttarget: 8.411 \tR_feature_loss scaled:\t 22225.217\n",
      "It 225\t Losses: total: 21886.377,\ttarget: 8.887 \tR_feature_loss scaled:\t 21877.463\n",
      "It 250\t Losses: total: 17738.715,\ttarget: 9.220 \tR_feature_loss scaled:\t 17729.469\n",
      "It 275\t Losses: total: 20050.115,\ttarget: 9.043 \tR_feature_loss scaled:\t 20041.047\n",
      "It 300\t Losses: total: 24541.117,\ttarget: 8.862 \tR_feature_loss scaled:\t 24532.230\n",
      "It 325\t Losses: total: 16791.355,\ttarget: 9.353 \tR_feature_loss scaled:\t 16781.979\n",
      "It 350\t Losses: total: 18097.156,\ttarget: 9.151 \tR_feature_loss scaled:\t 18087.980\n",
      "It 375\t Losses: total: 18766.348,\ttarget: 9.283 \tR_feature_loss scaled:\t 18757.041\n",
      "It 400\t Losses: total: 13677.520,\ttarget: 9.450 \tR_feature_loss scaled:\t 13668.046\n",
      "It 425\t Losses: total: 21489.410,\ttarget: 8.641 \tR_feature_loss scaled:\t 21480.746\n",
      "It 450\t Losses: total: 14873.839,\ttarget: 9.563 \tR_feature_loss scaled:\t 14864.253\n",
      "It 475\t Losses: total: 22116.248,\ttarget: 9.234 \tR_feature_loss scaled:\t 22106.992\n",
      "It 500\t Losses: total: 15879.786,\ttarget: 9.799 \tR_feature_loss scaled:\t 15869.965\n",
      "Teacher correct out of 64: 5, loss at 9.85781192779541\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([6, 7, 7, 1, 5, 7, 3, 4, 0, 1, 5, 2, 6, 2, 0, 6, 6, 3, 3, 3, 4, 1, 4, 4,\n",
      "        3, 7, 1, 7, 7], device='cuda:0')\n",
      "Loss: 0.328 | Acc: 89.184% (3051/3421), B. Acc: 0.863%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199235.766,\ttarget: 5.364 \tR_feature_loss scaled:\t 199230.344\n",
      "It 25\t Losses: total: 117127.867,\ttarget: 8.779 \tR_feature_loss scaled:\t 117119.047\n",
      "It 50\t Losses: total: 56629.074,\ttarget: 7.530 \tR_feature_loss scaled:\t 56621.504\n",
      "It 75\t Losses: total: 39878.098,\ttarget: 7.903 \tR_feature_loss scaled:\t 39870.160\n",
      "It 100\t Losses: total: 32224.922,\ttarget: 8.797 \tR_feature_loss scaled:\t 32216.092\n",
      "It 125\t Losses: total: 31173.174,\ttarget: 8.862 \tR_feature_loss scaled:\t 31164.279\n",
      "It 150\t Losses: total: 27425.902,\ttarget: 9.527 \tR_feature_loss scaled:\t 27416.346\n",
      "It 175\t Losses: total: 23583.373,\ttarget: 9.629 \tR_feature_loss scaled:\t 23573.715\n",
      "It 200\t Losses: total: 21462.004,\ttarget: 9.552 \tR_feature_loss scaled:\t 21452.424\n",
      "It 225\t Losses: total: 19652.369,\ttarget: 10.274 \tR_feature_loss scaled:\t 19642.068\n",
      "It 250\t Losses: total: 18058.682,\ttarget: 9.717 \tR_feature_loss scaled:\t 18048.939\n",
      "It 275\t Losses: total: 19901.072,\ttarget: 9.265 \tR_feature_loss scaled:\t 19891.781\n",
      "It 300\t Losses: total: 16216.062,\ttarget: 10.079 \tR_feature_loss scaled:\t 16205.957\n",
      "It 325\t Losses: total: 19079.143,\ttarget: 10.625 \tR_feature_loss scaled:\t 19068.494\n",
      "It 350\t Losses: total: 13919.771,\ttarget: 10.198 \tR_feature_loss scaled:\t 13909.550\n",
      "It 375\t Losses: total: 18276.404,\ttarget: 9.651 \tR_feature_loss scaled:\t 18266.729\n",
      "It 400\t Losses: total: 14996.322,\ttarget: 9.923 \tR_feature_loss scaled:\t 14986.376\n",
      "It 425\t Losses: total: 16818.818,\ttarget: 10.128 \tR_feature_loss scaled:\t 16808.668\n",
      "It 450\t Losses: total: 14879.755,\ttarget: 10.582 \tR_feature_loss scaled:\t 14869.150\n",
      "It 475\t Losses: total: 11423.806,\ttarget: 10.307 \tR_feature_loss scaled:\t 11413.477\n",
      "It 500\t Losses: total: 16309.889,\ttarget: 10.421 \tR_feature_loss scaled:\t 16299.445\n",
      "Teacher correct out of 64: 3, loss at 9.86223316192627\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([1, 7, 7, 6, 2, 3, 6, 1, 2, 6, 6, 7, 6, 3, 7, 6, 7, 6, 1, 6, 1, 7, 1, 6,\n",
      "        2, 6, 1, 3, 5], device='cuda:0')\n",
      "Loss: 0.302 | Acc: 90.237% (3087/3421), B. Acc: 0.878%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199269.547,\ttarget: 5.425 \tR_feature_loss scaled:\t 199264.062\n",
      "It 25\t Losses: total: 116511.023,\ttarget: 9.036 \tR_feature_loss scaled:\t 116501.945\n",
      "It 50\t Losses: total: 58033.598,\ttarget: 7.886 \tR_feature_loss scaled:\t 58025.672\n",
      "It 75\t Losses: total: 40859.590,\ttarget: 8.209 \tR_feature_loss scaled:\t 40851.344\n",
      "It 100\t Losses: total: 41447.777,\ttarget: 8.830 \tR_feature_loss scaled:\t 41438.914\n",
      "It 125\t Losses: total: 34508.406,\ttarget: 8.106 \tR_feature_loss scaled:\t 34500.266\n",
      "It 150\t Losses: total: 29421.338,\ttarget: 8.261 \tR_feature_loss scaled:\t 29413.047\n",
      "It 175\t Losses: total: 24810.932,\ttarget: 8.361 \tR_feature_loss scaled:\t 24802.541\n",
      "It 200\t Losses: total: 25016.418,\ttarget: 8.462 \tR_feature_loss scaled:\t 25007.928\n",
      "It 225\t Losses: total: 21520.689,\ttarget: 8.249 \tR_feature_loss scaled:\t 21512.412\n",
      "It 250\t Losses: total: 27457.689,\ttarget: 8.652 \tR_feature_loss scaled:\t 27449.012\n",
      "It 275\t Losses: total: 17348.201,\ttarget: 8.391 \tR_feature_loss scaled:\t 17339.783\n",
      "It 300\t Losses: total: 17852.967,\ttarget: 8.155 \tR_feature_loss scaled:\t 17844.787\n",
      "It 325\t Losses: total: 15045.813,\ttarget: 8.531 \tR_feature_loss scaled:\t 15037.258\n",
      "It 350\t Losses: total: 23324.361,\ttarget: 8.747 \tR_feature_loss scaled:\t 23315.590\n",
      "It 375\t Losses: total: 18881.838,\ttarget: 8.268 \tR_feature_loss scaled:\t 18873.547\n",
      "It 400\t Losses: total: 15469.817,\ttarget: 7.969 \tR_feature_loss scaled:\t 15461.825\n",
      "It 425\t Losses: total: 13827.043,\ttarget: 8.141 \tR_feature_loss scaled:\t 13818.879\n",
      "It 450\t Losses: total: 13709.582,\ttarget: 8.308 \tR_feature_loss scaled:\t 13701.251\n",
      "It 475\t Losses: total: 15073.505,\ttarget: 8.443 \tR_feature_loss scaled:\t 15065.040\n",
      "It 500\t Losses: total: 19125.109,\ttarget: 8.301 \tR_feature_loss scaled:\t 19116.787\n",
      "Teacher correct out of 64: 8, loss at 8.344536781311035\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([1, 2, 2, 4, 1, 6, 7, 7, 7, 1, 3, 1, 2, 1, 5, 4, 7, 6, 3, 5, 1, 4, 1, 4,\n",
      "        7, 4, 3, 7, 7], device='cuda:0')\n",
      "Loss: 0.297 | Acc: 90.324% (3090/3421), B. Acc: 0.876%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199069.375,\ttarget: 5.350 \tR_feature_loss scaled:\t 199063.969\n",
      "It 25\t Losses: total: 120202.383,\ttarget: 9.233 \tR_feature_loss scaled:\t 120193.109\n",
      "It 50\t Losses: total: 57782.621,\ttarget: 7.422 \tR_feature_loss scaled:\t 57775.160\n",
      "It 75\t Losses: total: 35655.434,\ttarget: 7.585 \tR_feature_loss scaled:\t 35647.812\n",
      "It 100\t Losses: total: 39427.754,\ttarget: 7.974 \tR_feature_loss scaled:\t 39419.746\n",
      "It 125\t Losses: total: 30011.633,\ttarget: 7.816 \tR_feature_loss scaled:\t 30003.785\n",
      "It 150\t Losses: total: 25630.695,\ttarget: 8.091 \tR_feature_loss scaled:\t 25622.574\n",
      "It 175\t Losses: total: 23654.570,\ttarget: 8.431 \tR_feature_loss scaled:\t 23646.109\n",
      "It 200\t Losses: total: 23840.695,\ttarget: 8.463 \tR_feature_loss scaled:\t 23832.205\n",
      "It 225\t Losses: total: 19782.221,\ttarget: 9.018 \tR_feature_loss scaled:\t 19773.176\n",
      "It 250\t Losses: total: 20870.611,\ttarget: 8.710 \tR_feature_loss scaled:\t 20861.875\n",
      "It 275\t Losses: total: 22328.645,\ttarget: 8.615 \tR_feature_loss scaled:\t 22320.004\n",
      "It 300\t Losses: total: 16531.254,\ttarget: 9.000 \tR_feature_loss scaled:\t 16522.229\n",
      "It 325\t Losses: total: 18231.088,\ttarget: 9.030 \tR_feature_loss scaled:\t 18222.033\n",
      "It 350\t Losses: total: 17461.559,\ttarget: 9.111 \tR_feature_loss scaled:\t 17452.424\n",
      "It 375\t Losses: total: 17853.314,\ttarget: 8.906 \tR_feature_loss scaled:\t 17844.385\n",
      "It 400\t Losses: total: 13275.026,\ttarget: 9.120 \tR_feature_loss scaled:\t 13265.883\n",
      "It 425\t Losses: total: 22753.889,\ttarget: 8.459 \tR_feature_loss scaled:\t 22745.406\n",
      "It 450\t Losses: total: 16798.908,\ttarget: 9.539 \tR_feature_loss scaled:\t 16789.348\n",
      "It 475\t Losses: total: 18590.891,\ttarget: 8.908 \tR_feature_loss scaled:\t 18581.961\n",
      "It 500\t Losses: total: 15659.673,\ttarget: 9.249 \tR_feature_loss scaled:\t 15650.401\n",
      "Teacher correct out of 64: 11, loss at 9.22986888885498\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([1, 1, 2, 1, 0, 3, 1, 6, 1, 7, 3, 2, 1, 6, 3, 3, 5, 6, 3, 7, 3, 6, 7, 4,\n",
      "        1, 6, 7, 1, 1], device='cuda:0')\n",
      "Loss: 0.299 | Acc: 89.886% (3075/3421), B. Acc: 0.869%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199193.891,\ttarget: 5.365 \tR_feature_loss scaled:\t 199188.469\n",
      "It 25\t Losses: total: 112575.688,\ttarget: 8.661 \tR_feature_loss scaled:\t 112566.984\n",
      "It 50\t Losses: total: 58385.297,\ttarget: 6.991 \tR_feature_loss scaled:\t 58378.266\n",
      "It 75\t Losses: total: 38970.965,\ttarget: 7.844 \tR_feature_loss scaled:\t 38963.086\n",
      "It 100\t Losses: total: 29684.840,\ttarget: 8.242 \tR_feature_loss scaled:\t 29676.564\n",
      "It 125\t Losses: total: 38462.844,\ttarget: 8.069 \tR_feature_loss scaled:\t 38454.742\n",
      "It 150\t Losses: total: 23487.568,\ttarget: 8.401 \tR_feature_loss scaled:\t 23479.137\n",
      "It 175\t Losses: total: 38019.633,\ttarget: 8.667 \tR_feature_loss scaled:\t 38010.938\n",
      "It 200\t Losses: total: 24760.900,\ttarget: 8.761 \tR_feature_loss scaled:\t 24752.111\n",
      "It 225\t Losses: total: 22350.004,\ttarget: 8.431 \tR_feature_loss scaled:\t 22341.547\n",
      "It 250\t Losses: total: 18724.945,\ttarget: 8.753 \tR_feature_loss scaled:\t 18716.166\n",
      "It 275\t Losses: total: 22591.279,\ttarget: 8.432 \tR_feature_loss scaled:\t 22582.820\n",
      "It 300\t Losses: total: 15881.292,\ttarget: 8.403 \tR_feature_loss scaled:\t 15872.863\n",
      "It 325\t Losses: total: 17828.422,\ttarget: 8.692 \tR_feature_loss scaled:\t 17819.705\n",
      "It 350\t Losses: total: 17693.551,\ttarget: 8.435 \tR_feature_loss scaled:\t 17685.092\n",
      "It 375\t Losses: total: 14228.155,\ttarget: 9.002 \tR_feature_loss scaled:\t 14219.129\n",
      "It 400\t Losses: total: 15896.747,\ttarget: 8.592 \tR_feature_loss scaled:\t 15888.131\n",
      "It 425\t Losses: total: 14665.611,\ttarget: 8.643 \tR_feature_loss scaled:\t 14656.944\n",
      "It 450\t Losses: total: 14738.695,\ttarget: 8.409 \tR_feature_loss scaled:\t 14730.264\n",
      "It 475\t Losses: total: 13236.005,\ttarget: 8.890 \tR_feature_loss scaled:\t 13227.092\n",
      "It 500\t Losses: total: 18878.963,\ttarget: 8.682 \tR_feature_loss scaled:\t 18870.260\n",
      "Teacher correct out of 64: 10, loss at 8.774913787841797\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([3, 6, 3, 2, 3, 3, 6, 6, 5, 4, 1, 2, 6, 3, 6, 6, 1, 7, 1, 3, 6, 7, 7, 0,\n",
      "        5, 4, 3, 3, 3], device='cuda:0')\n",
      "Loss: 0.302 | Acc: 90.061% (3081/3421), B. Acc: 0.871%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 199377.500,\ttarget: 5.289 \tR_feature_loss scaled:\t 199372.156\n",
      "It 25\t Losses: total: 121187.422,\ttarget: 8.716 \tR_feature_loss scaled:\t 121178.664\n",
      "It 50\t Losses: total: 54825.031,\ttarget: 7.118 \tR_feature_loss scaled:\t 54817.875\n",
      "It 75\t Losses: total: 47089.914,\ttarget: 8.232 \tR_feature_loss scaled:\t 47081.645\n",
      "It 100\t Losses: total: 38323.863,\ttarget: 7.501 \tR_feature_loss scaled:\t 38316.328\n",
      "It 125\t Losses: total: 30375.932,\ttarget: 7.648 \tR_feature_loss scaled:\t 30368.252\n",
      "It 150\t Losses: total: 23457.828,\ttarget: 8.453 \tR_feature_loss scaled:\t 23449.346\n",
      "It 175\t Losses: total: 23033.072,\ttarget: 8.742 \tR_feature_loss scaled:\t 23024.301\n",
      "It 200\t Losses: total: 22656.289,\ttarget: 8.625 \tR_feature_loss scaled:\t 22647.637\n",
      "It 225\t Losses: total: 19760.555,\ttarget: 8.644 \tR_feature_loss scaled:\t 19751.883\n",
      "It 250\t Losses: total: 20596.887,\ttarget: 8.423 \tR_feature_loss scaled:\t 20588.438\n",
      "It 275\t Losses: total: 22427.371,\ttarget: 8.988 \tR_feature_loss scaled:\t 22418.357\n",
      "It 300\t Losses: total: 18882.324,\ttarget: 8.935 \tR_feature_loss scaled:\t 18873.363\n",
      "It 325\t Losses: total: 17637.117,\ttarget: 8.866 \tR_feature_loss scaled:\t 17628.227\n",
      "It 350\t Losses: total: 18354.432,\ttarget: 8.942 \tR_feature_loss scaled:\t 18345.465\n",
      "It 375\t Losses: total: 15328.376,\ttarget: 8.976 \tR_feature_loss scaled:\t 15319.376\n",
      "It 400\t Losses: total: 17307.551,\ttarget: 8.956 \tR_feature_loss scaled:\t 17298.570\n",
      "It 425\t Losses: total: 18543.842,\ttarget: 9.352 \tR_feature_loss scaled:\t 18534.467\n",
      "It 450\t Losses: total: 22541.436,\ttarget: 9.228 \tR_feature_loss scaled:\t 22532.184\n",
      "It 475\t Losses: total: 16067.769,\ttarget: 9.246 \tR_feature_loss scaled:\t 16058.500\n",
      "It 500\t Losses: total: 15059.345,\ttarget: 9.523 \tR_feature_loss scaled:\t 15049.800\n",
      "Teacher correct out of 64: 12, loss at 9.19758415222168\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([64])\n",
      "Test classifier\n",
      "tensor([7, 0, 7, 2, 3, 1, 5, 7, 3, 3, 7, 6, 1, 2, 2, 7, 0, 3, 3, 3, 6, 6, 3, 5,\n",
      "        3, 6, 4, 5, 2], device='cuda:0')\n",
      "Loss: 0.327 | Acc: 89.360% (3057/3421), B. Acc: 0.857%\n",
      "Test ensemble...\n",
      "tensor([2, 1, 0, 2, 7, 1, 3, 2, 3, 6, 6, 6, 7, 6, 5, 6, 3, 1, 1, 6, 7, 0, 1, 5,\n",
      "        0, 1, 1, 0, 3], device='cuda:0')\n",
      "Loss: 0.134 | Acc: 96.083% (3287/3421), B. Acc: 0.957%\n",
      "Test classifier\n",
      "tensor([7, 4, 3, 7, 3, 3, 1, 0, 7, 5, 1, 6, 7, 3, 6, 6, 1, 6, 7, 7, 2, 6, 6, 2,\n",
      "        7, 5, 1, 7, 2], device='cuda:0')\n",
      "Loss: 0.327 | Acc: 89.360% (3057/3421), B. Acc: 0.857%\n"
     ]
    }
   ],
   "source": [
    "fed_model, noise_adapt = FedBlood.model_inversion(ens_local, large_jtr=5, small_jtr=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1734409642988,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "Z_UU5Lrk3eGH",
    "outputId": "9988972e-e2b0-45f9-d00d-ca72b379f301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/.shortcut-targets-by-id/1D1SxO5JoM4fHICMtrKkdp70SChR3NCos/CS4701_FL\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2055,
     "status": "ok",
     "timestamp": 1734409990569,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "9uDwRkQ865IY",
    "outputId": "99577e3b-1819-4504-cc60-6c1f1141bfc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-eac6e1f1ecab>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fed_model = torch.load(\"./blood_exper/federated/glb_model_last.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 5, 3, 5, 3, 3, 2, 5, 2, 3, 7, 6, 5, 6, 1, 6, 3, 3, 6, 1, 6, 2, 1, 1,\n",
      "        3, 1, 0, 6, 6, 4, 2, 1, 6, 6, 1, 2, 4, 1, 7, 4, 2, 7, 7, 1, 1, 6, 5, 3,\n",
      "        6, 1, 6, 3, 3, 7, 1, 7, 3, 7, 4, 6, 6, 6, 6, 1, 3, 4, 7, 0, 1, 7, 3, 1,\n",
      "        2, 6, 7, 1, 1, 1, 7, 3, 2, 3, 3, 6, 3, 3, 5, 4, 6, 3, 3, 1, 7],\n",
      "       device='cuda:0')\n",
      "Loss: 0.325 | Acc: 89.360% (3057/3421), B. Acc: 0.857%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8.774762317538261, 0.8935983630517392, 0.8570388553931055)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "#fed_model = ResNet18(in_channels=3, num_classes=8)\n",
    "#fed_model.to('cuda')\n",
    "fed_model = torch.load(\"./blood_exper/federated/glb_model_last.pth\")\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "test_data = DataClass(split='test', root=\"./\", transform=test_transforms, download=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "FedBlood.test_model(fed_model, test_loader, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1982,
     "status": "ok",
     "timestamp": 1734410197228,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "kNmlJBJb277l",
    "outputId": "ad2280cd-bdd9-4960-f7d5-71f867d5309e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-65a36cce6aaa>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  client_0_weights = torch.load(\"./blood_models/client_4/best.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 5, 4, 5, 3, 3, 2, 5, 2, 3, 7, 6, 5, 3, 1, 6, 0, 3, 6, 1, 6, 2, 1, 1,\n",
      "        3, 1, 0, 6, 6, 4, 2, 1, 6, 6, 1, 2, 4, 1, 7, 4, 2, 7, 7, 1, 1, 6, 5, 3,\n",
      "        6, 1, 6, 3, 0, 7, 1, 7, 3, 7, 4, 6, 6, 6, 6, 1, 3, 4, 7, 0, 1, 7, 3, 1,\n",
      "        2, 6, 7, 1, 1, 1, 7, 3, 2, 3, 5, 6, 3, 3, 5, 4, 6, 3, 3, 1, 7],\n",
      "       device='cuda:0')\n",
      "Loss: 0.227 | Acc: 93.803% (3209/3421), B. Acc: 0.930%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.1330351755023, 0.9380298158433207, 0.9301921356667815)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_0_weights = torch.load(\"./blood_models/client_4/best.pth\")\n",
    "client_0_model = ResNet18(in_channels=3, num_classes=8)\n",
    "client_0_model.load_state_dict(client_0_weights)\n",
    "client_0_model.to('cuda')\n",
    "FedBlood.test_model(client_0_model, test_loader, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OK4nmkT0rdD"
   },
   "source": [
    "## Testing on non-iid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4895,
     "status": "ok",
     "timestamp": 1734443723943,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "gCRND8fm05XF",
    "outputId": "46d549dc-4f27-4877-fa11-0fd6997b9b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./bloodmnist.npz\n",
      "Using downloaded and verified file: ./bloodmnist.npz\n",
      "Using downloaded and verified file: ./bloodmnist.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset BloodMNIST of size 28 (bloodmnist)\n",
       "    Number of datapoints: 11959\n",
       "    Root location: ./\n",
       "    Split: train\n",
       "    Task: multi-class\n",
       "    Number of channels: 3\n",
       "    Meaning of labels: {'0': 'basophil', '1': 'eosinophil', '2': 'erythroblast', '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)', '4': 'lymphocyte', '5': 'monocyte', '6': 'neutrophil', '7': 'platelet'}\n",
       "    Number of samples: {'train': 11959, 'val': 1712, 'test': 3421}\n",
       "    Description: The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3×360×363 pixels are center-cropped into 3×200×200, and then resized into 3×28×28.\n",
       "    License: CC BY 4.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_flag = 'bloodmnist'\n",
    "download = True\n",
    "\n",
    "info = medmnist.INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "glb_train_dataset = DataClass(root=\"./\", split='train', download=download)\n",
    "glb_val_dataset = DataClass(root=\"./\", split='val', download=download)\n",
    "glb_test_dataset = DataClass(root=\"./\", split='test', download=download)\n",
    "info[\"n_samples\"][\"train\"] = glb_train_dataset.imgs.shape[0]\n",
    "info[\"n_samples\"][\"val\"] = glb_val_dataset.imgs.shape[0]\n",
    "info[\"n_samples\"][\"test\"] = glb_test_dataset.imgs.shape[0]\n",
    "glb_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 885,
     "status": "ok",
     "timestamp": 1734443757950,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "l_Q23rjG05XG"
   },
   "outputs": [],
   "source": [
    "num_clients = 5\n",
    "num_classes = 8\n",
    "part_dir, list_name, split_ids = utils.fl_partition(glb_train_dataset, data_flag, num_clients, num_classes, iid=False, beta=1, val_split = 0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1734406368178,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "Im4XKQmGqtyY",
    "outputId": "ca289e10-a3ab-4e1f-b9ba-66241ad85cec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2539"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1734406431862,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "FQNJeRDc1M1c",
    "outputId": "7d90c742-16b9-4033-f580-ad67ed10278b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5), sharey=True)\n",
    "\n",
    "for list_ind, ax in enumerate(axes):\n",
    "    f_name = list_name[list_ind]\n",
    "    data = np.load(f_name)\n",
    "    labels = data['train_labels'].flatten().tolist()\n",
    "    label_counts = Counter(labels)\n",
    "    categories = list(label_counts.keys())\n",
    "    counts = list(label_counts.values())\n",
    "\n",
    "    # Plot on the current subplot\n",
    "    ax.bar(categories, counts, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f\"Dataset {list_ind + 1}\", fontsize=14)\n",
    "    ax.set_xlabel('Labels', fontsize=12)\n",
    "    ax.set_xticks(ticks=categories)\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "fig.supylabel('Count', fontsize=14)\n",
    "fig.suptitle('Label distribution for iid partition', fontsize=16)\n",
    "fig.subplots_adjust(left=0.05, right=0.95, top=0.85, bottom=0.1, wspace=0.3)\n",
    "#plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 685391,
     "status": "ok",
     "timestamp": 1734393255957,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "MPgWSAv_05XG",
    "outputId": "aa3462f0-26ee-4b03-b44d-90ee30a5c64d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch num: 1 \n",
      "Train loss: 0.186484 \n",
      "Val loss: 0.014164 \n",
      "Val acc: 0.493097\n",
      "Val balanced acc: 0.307373\n",
      "Epoch num: 2 \n",
      "Train loss: 0.126273 \n",
      "Val loss: 0.015852 \n",
      "Val acc: 0.558185\n",
      "Val balanced acc: 0.330379\n",
      "Epoch num: 3 \n",
      "Train loss: 0.093666 \n",
      "Val loss: 0.014422 \n",
      "Val acc: 0.532544\n",
      "Val balanced acc: 0.302713\n",
      "Epoch num: 4 \n",
      "Train loss: 0.078253 \n",
      "Val loss: 0.008027 \n",
      "Val acc: 0.656805\n",
      "Val balanced acc: 0.650062\n",
      "Epoch num: 5 \n",
      "Train loss: 0.072732 \n",
      "Val loss: 0.008086 \n",
      "Val acc: 0.690335\n",
      "Val balanced acc: 0.595264\n",
      "Epoch num: 6 \n",
      "Train loss: 0.067362 \n",
      "Val loss: 0.004627 \n",
      "Val acc: 0.790927\n",
      "Val balanced acc: 0.792062\n",
      "Epoch num: 7 \n",
      "Train loss: 0.060546 \n",
      "Val loss: 0.004303 \n",
      "Val acc: 0.783037\n",
      "Val balanced acc: 0.789860\n",
      "Epoch num: 8 \n",
      "Train loss: 0.061448 \n",
      "Val loss: 0.019181 \n",
      "Val acc: 0.469428\n",
      "Val balanced acc: 0.439182\n",
      "Epoch num: 9 \n",
      "Train loss: 0.058768 \n",
      "Val loss: 0.011673 \n",
      "Val acc: 0.593688\n",
      "Val balanced acc: 0.569991\n",
      "Epoch num: 10 \n",
      "Train loss: 0.052053 \n",
      "Val loss: 0.004240 \n",
      "Val acc: 0.796844\n",
      "Val balanced acc: 0.740377\n",
      "Epoch num: 11 \n",
      "Train loss: 0.048329 \n",
      "Val loss: 0.009664 \n",
      "Val acc: 0.648915\n",
      "Val balanced acc: 0.663694\n",
      "Epoch num: 12 \n",
      "Train loss: 0.054201 \n",
      "Val loss: 0.008116 \n",
      "Val acc: 0.680473\n",
      "Val balanced acc: 0.681219\n",
      "Epoch num: 13 \n",
      "Train loss: 0.050334 \n",
      "Val loss: 0.004587 \n",
      "Val acc: 0.816568\n",
      "Val balanced acc: 0.748304\n",
      "Epoch num: 14 \n",
      "Train loss: 0.047186 \n",
      "Val loss: 0.005643 \n",
      "Val acc: 0.777120\n",
      "Val balanced acc: 0.729156\n",
      "Epoch num: 15 \n",
      "Train loss: 0.046134 \n",
      "Val loss: 0.012671 \n",
      "Val acc: 0.627219\n",
      "Val balanced acc: 0.491704\n",
      "Epoch num: 16 \n",
      "Train loss: 0.042984 \n",
      "Val loss: 0.003605 \n",
      "Val acc: 0.856016\n",
      "Val balanced acc: 0.827739\n",
      "Epoch num: 17 \n",
      "Train loss: 0.039836 \n",
      "Val loss: 0.006301 \n",
      "Val acc: 0.767258\n",
      "Val balanced acc: 0.686283\n",
      "Epoch num: 18 \n",
      "Train loss: 0.043398 \n",
      "Val loss: 0.006115 \n",
      "Val acc: 0.794872\n",
      "Val balanced acc: 0.714978\n",
      "Epoch num: 19 \n",
      "Train loss: 0.043038 \n",
      "Val loss: 0.005159 \n",
      "Val acc: 0.804734\n",
      "Val balanced acc: 0.740460\n",
      "Epoch num: 20 \n",
      "Train loss: 0.041594 \n",
      "Val loss: 0.005355 \n",
      "Val acc: 0.785010\n",
      "Val balanced acc: 0.691620\n",
      "Epoch num: 21 \n",
      "Train loss: 0.038679 \n",
      "Val loss: 0.003926 \n",
      "Val acc: 0.830375\n",
      "Val balanced acc: 0.804507\n",
      "Epoch num: 22 \n",
      "Train loss: 0.036064 \n",
      "Val loss: 0.004392 \n",
      "Val acc: 0.824458\n",
      "Val balanced acc: 0.865334\n",
      "Epoch num: 23 \n",
      "Train loss: 0.030681 \n",
      "Val loss: 0.003437 \n",
      "Val acc: 0.871795\n",
      "Val balanced acc: 0.851959\n",
      "Epoch num: 24 \n",
      "Train loss: 0.029452 \n",
      "Val loss: 0.020958 \n",
      "Val acc: 0.483235\n",
      "Val balanced acc: 0.572441\n",
      "Epoch num: 25 \n",
      "Train loss: 0.044470 \n",
      "Val loss: 0.008393 \n",
      "Val acc: 0.723866\n",
      "Val balanced acc: 0.607614\n",
      "Epoch num: 26 \n",
      "Train loss: 0.033948 \n",
      "Val loss: 0.004438 \n",
      "Val acc: 0.844181\n",
      "Val balanced acc: 0.799223\n",
      "Epoch num: 27 \n",
      "Train loss: 0.029837 \n",
      "Val loss: 0.004472 \n",
      "Val acc: 0.818540\n",
      "Val balanced acc: 0.798406\n",
      "Epoch num: 28 \n",
      "Train loss: 0.027563 \n",
      "Val loss: 0.002795 \n",
      "Val acc: 0.881657\n",
      "Val balanced acc: 0.856871\n",
      "Epoch num: 29 \n",
      "Train loss: 0.025075 \n",
      "Val loss: 0.003827 \n",
      "Val acc: 0.877712\n",
      "Val balanced acc: 0.857819\n",
      "Epoch num: 30 \n",
      "Train loss: 0.026716 \n",
      "Val loss: 0.012823 \n",
      "Val acc: 0.694280\n",
      "Val balanced acc: 0.685535\n",
      "Epoch num: 31 \n",
      "Train loss: 0.033823 \n",
      "Val loss: 0.005435 \n",
      "Val acc: 0.810651\n",
      "Val balanced acc: 0.742025\n",
      "Epoch num: 32 \n",
      "Train loss: 0.031292 \n",
      "Val loss: 0.004032 \n",
      "Val acc: 0.846154\n",
      "Val balanced acc: 0.838857\n",
      "Epoch num: 33 \n",
      "Train loss: 0.028938 \n",
      "Val loss: 0.006585 \n",
      "Val acc: 0.779093\n",
      "Val balanced acc: 0.754112\n",
      "Epoch num: 34 \n",
      "Train loss: 0.027172 \n",
      "Val loss: 0.004768 \n",
      "Val acc: 0.828402\n",
      "Val balanced acc: 0.792763\n",
      "Epoch num: 35 \n",
      "Train loss: 0.024119 \n",
      "Val loss: 0.005612 \n",
      "Val acc: 0.820513\n",
      "Val balanced acc: 0.746940\n",
      "Epoch num: 36 \n",
      "Train loss: 0.023491 \n",
      "Val loss: 0.004111 \n",
      "Val acc: 0.844181\n",
      "Val balanced acc: 0.826508\n",
      "Epoch num: 37 \n",
      "Train loss: 0.019174 \n",
      "Val loss: 0.005945 \n",
      "Val acc: 0.796844\n",
      "Val balanced acc: 0.728714\n",
      "Epoch num: 38 \n",
      "Train loss: 0.020705 \n",
      "Val loss: 0.007674 \n",
      "Val acc: 0.751479\n",
      "Val balanced acc: 0.737294\n",
      "Epoch num: 39 \n",
      "Train loss: 0.022534 \n",
      "Val loss: 0.006731 \n",
      "Val acc: 0.775148\n",
      "Val balanced acc: 0.671713\n",
      "Epoch num: 40 \n",
      "Train loss: 0.015844 \n",
      "Val loss: 0.002639 \n",
      "Val acc: 0.899408\n",
      "Val balanced acc: 0.896208\n",
      "Epoch num: 41 \n",
      "Train loss: 0.012480 \n",
      "Val loss: 0.002208 \n",
      "Val acc: 0.923077\n",
      "Val balanced acc: 0.926582\n",
      "Epoch num: 42 \n",
      "Train loss: 0.012795 \n",
      "Val loss: 0.002078 \n",
      "Val acc: 0.917160\n",
      "Val balanced acc: 0.918471\n",
      "Epoch num: 43 \n",
      "Train loss: 0.010583 \n",
      "Val loss: 0.001992 \n",
      "Val acc: 0.927022\n",
      "Val balanced acc: 0.920063\n",
      "Epoch num: 44 \n",
      "Train loss: 0.010622 \n",
      "Val loss: 0.002063 \n",
      "Val acc: 0.913215\n",
      "Val balanced acc: 0.906531\n",
      "Epoch num: 45 \n",
      "Train loss: 0.011070 \n",
      "Val loss: 0.001985 \n",
      "Val acc: 0.927022\n",
      "Val balanced acc: 0.918829\n",
      "Epoch num: 46 \n",
      "Train loss: 0.010098 \n",
      "Val loss: 0.002065 \n",
      "Val acc: 0.921105\n",
      "Val balanced acc: 0.919592\n",
      "Epoch num: 47 \n",
      "Train loss: 0.008772 \n",
      "Val loss: 0.002080 \n",
      "Val acc: 0.928994\n",
      "Val balanced acc: 0.919518\n",
      "Epoch num: 48 \n",
      "Train loss: 0.008670 \n",
      "Val loss: 0.002054 \n",
      "Val acc: 0.923077\n",
      "Val balanced acc: 0.923988\n",
      "Epoch num: 49 \n",
      "Train loss: 0.009221 \n",
      "Val loss: 0.002126 \n",
      "Val acc: 0.928994\n",
      "Val balanced acc: 0.927934\n",
      "Epoch num: 50 \n",
      "Train loss: 0.009060 \n",
      "Val loss: 0.002166 \n",
      "Val acc: 0.925049\n",
      "Val balanced acc: 0.921415\n",
      "All done!\n",
      "Epoch num: 1 \n",
      "Train loss: 0.128797 \n",
      "Val loss: 0.018446 \n",
      "Val acc: 0.669456\n",
      "Val balanced acc: 0.272283\n",
      "Epoch num: 2 \n",
      "Train loss: 0.065383 \n",
      "Val loss: 0.008035 \n",
      "Val acc: 0.709205\n",
      "Val balanced acc: 0.443117\n",
      "Epoch num: 3 \n",
      "Train loss: 0.052668 \n",
      "Val loss: 0.006400 \n",
      "Val acc: 0.767782\n",
      "Val balanced acc: 0.506256\n",
      "Epoch num: 4 \n",
      "Train loss: 0.047344 \n",
      "Val loss: 0.004377 \n",
      "Val acc: 0.807531\n",
      "Val balanced acc: 0.548888\n",
      "Epoch num: 5 \n",
      "Train loss: 0.046787 \n",
      "Val loss: 0.004797 \n",
      "Val acc: 0.801255\n",
      "Val balanced acc: 0.589863\n",
      "Epoch num: 6 \n",
      "Train loss: 0.043215 \n",
      "Val loss: 0.003665 \n",
      "Val acc: 0.822176\n",
      "Val balanced acc: 0.601879\n",
      "Epoch num: 7 \n",
      "Train loss: 0.041376 \n",
      "Val loss: 0.002932 \n",
      "Val acc: 0.882845\n",
      "Val balanced acc: 0.705015\n",
      "Epoch num: 8 \n",
      "Train loss: 0.037990 \n",
      "Val loss: 0.003586 \n",
      "Val acc: 0.859833\n",
      "Val balanced acc: 0.701265\n",
      "Epoch num: 9 \n",
      "Train loss: 0.034969 \n",
      "Val loss: 0.002484 \n",
      "Val acc: 0.893305\n",
      "Val balanced acc: 0.784193\n",
      "Epoch num: 10 \n",
      "Train loss: 0.033171 \n",
      "Val loss: 0.005775 \n",
      "Val acc: 0.799163\n",
      "Val balanced acc: 0.573545\n",
      "Epoch num: 11 \n",
      "Train loss: 0.032224 \n",
      "Val loss: 0.006707 \n",
      "Val acc: 0.755230\n",
      "Val balanced acc: 0.476055\n",
      "Epoch num: 12 \n",
      "Train loss: 0.030106 \n",
      "Val loss: 0.003968 \n",
      "Val acc: 0.847280\n",
      "Val balanced acc: 0.681204\n",
      "Epoch num: 13 \n",
      "Train loss: 0.026661 \n",
      "Val loss: 0.005867 \n",
      "Val acc: 0.824268\n",
      "Val balanced acc: 0.641433\n",
      "Epoch num: 14 \n",
      "Train loss: 0.028138 \n",
      "Val loss: 0.003031 \n",
      "Val acc: 0.874477\n",
      "Val balanced acc: 0.676111\n",
      "Epoch num: 15 \n",
      "Train loss: 0.027328 \n",
      "Val loss: 0.002442 \n",
      "Val acc: 0.891213\n",
      "Val balanced acc: 0.718317\n",
      "Epoch num: 16 \n",
      "Train loss: 0.029342 \n",
      "Val loss: 0.002545 \n",
      "Val acc: 0.903766\n",
      "Val balanced acc: 0.801336\n",
      "Epoch num: 17 \n",
      "Train loss: 0.024739 \n",
      "Val loss: 0.002785 \n",
      "Val acc: 0.878661\n",
      "Val balanced acc: 0.725977\n",
      "Epoch num: 18 \n",
      "Train loss: 0.026105 \n",
      "Val loss: 0.004628 \n",
      "Val acc: 0.822176\n",
      "Val balanced acc: 0.636723\n",
      "Epoch num: 19 \n",
      "Train loss: 0.024498 \n",
      "Val loss: 0.004248 \n",
      "Val acc: 0.864017\n",
      "Val balanced acc: 0.683502\n",
      "Epoch num: 20 \n",
      "Train loss: 0.021546 \n",
      "Val loss: 0.001955 \n",
      "Val acc: 0.903766\n",
      "Val balanced acc: 0.806299\n",
      "Epoch num: 21 \n",
      "Train loss: 0.018090 \n",
      "Val loss: 0.002305 \n",
      "Val acc: 0.903766\n",
      "Val balanced acc: 0.830674\n",
      "Epoch num: 22 \n",
      "Train loss: 0.019862 \n",
      "Val loss: 0.002019 \n",
      "Val acc: 0.916318\n",
      "Val balanced acc: 0.844208\n",
      "Epoch num: 23 \n",
      "Train loss: 0.022712 \n",
      "Val loss: 0.002794 \n",
      "Val acc: 0.893305\n",
      "Val balanced acc: 0.687904\n",
      "Epoch num: 24 \n",
      "Train loss: 0.019075 \n",
      "Val loss: 0.003138 \n",
      "Val acc: 0.876569\n",
      "Val balanced acc: 0.796249\n",
      "Epoch num: 25 \n",
      "Train loss: 0.020276 \n",
      "Val loss: 0.007622 \n",
      "Val acc: 0.751046\n",
      "Val balanced acc: 0.612916\n",
      "Epoch num: 26 \n",
      "Train loss: 0.018948 \n",
      "Val loss: 0.004609 \n",
      "Val acc: 0.830544\n",
      "Val balanced acc: 0.612089\n",
      "Epoch num: 27 \n",
      "Train loss: 0.016804 \n",
      "Val loss: 0.002179 \n",
      "Val acc: 0.924686\n",
      "Val balanced acc: 0.844136\n",
      "Epoch num: 28 \n",
      "Train loss: 0.016535 \n",
      "Val loss: 0.007470 \n",
      "Val acc: 0.799163\n",
      "Val balanced acc: 0.613117\n",
      "Epoch num: 29 \n",
      "Train loss: 0.016724 \n",
      "Val loss: 0.002203 \n",
      "Val acc: 0.920502\n",
      "Val balanced acc: 0.859570\n",
      "Epoch num: 30 \n",
      "Train loss: 0.018266 \n",
      "Val loss: 0.009230 \n",
      "Val acc: 0.744770\n",
      "Val balanced acc: 0.563370\n",
      "Epoch num: 31 \n",
      "Train loss: 0.016602 \n",
      "Val loss: 0.004499 \n",
      "Val acc: 0.822176\n",
      "Val balanced acc: 0.675044\n",
      "Epoch num: 32 \n",
      "Train loss: 0.016097 \n",
      "Val loss: 0.001778 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.861026\n",
      "Epoch num: 33 \n",
      "Train loss: 0.011502 \n",
      "Val loss: 0.001626 \n",
      "Val acc: 0.930962\n",
      "Val balanced acc: 0.860094\n",
      "Epoch num: 34 \n",
      "Train loss: 0.011635 \n",
      "Val loss: 0.001602 \n",
      "Val acc: 0.935146\n",
      "Val balanced acc: 0.866771\n",
      "Epoch num: 35 \n",
      "Train loss: 0.009224 \n",
      "Val loss: 0.001500 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.857982\n",
      "Epoch num: 36 \n",
      "Train loss: 0.009663 \n",
      "Val loss: 0.001567 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.881744\n",
      "Epoch num: 37 \n",
      "Train loss: 0.009548 \n",
      "Val loss: 0.001572 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.859193\n",
      "Epoch num: 38 \n",
      "Train loss: 0.008266 \n",
      "Val loss: 0.001341 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.880808\n",
      "Epoch num: 39 \n",
      "Train loss: 0.007947 \n",
      "Val loss: 0.001304 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.880808\n",
      "Epoch num: 40 \n",
      "Train loss: 0.007637 \n",
      "Val loss: 0.001527 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.874411\n",
      "Epoch num: 41 \n",
      "Train loss: 0.007473 \n",
      "Val loss: 0.001530 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.870342\n",
      "Epoch num: 42 \n",
      "Train loss: 0.010013 \n",
      "Val loss: 0.001449 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.856554\n",
      "Epoch num: 43 \n",
      "Train loss: 0.008346 \n",
      "Val loss: 0.001412 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.880560\n",
      "Epoch num: 44 \n",
      "Train loss: 0.007361 \n",
      "Val loss: 0.001466 \n",
      "Val acc: 0.945607\n",
      "Val balanced acc: 0.882982\n",
      "Epoch num: 45 \n",
      "Train loss: 0.007704 \n",
      "Val loss: 0.001539 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.876771\n",
      "Epoch num: 46 \n",
      "Train loss: 0.006923 \n",
      "Val loss: 0.001425 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.881522\n",
      "Epoch num: 47 \n",
      "Train loss: 0.007356 \n",
      "Val loss: 0.001428 \n",
      "Val acc: 0.943515\n",
      "Val balanced acc: 0.880560\n",
      "Epoch num: 48 \n",
      "Train loss: 0.008572 \n",
      "Val loss: 0.001417 \n",
      "Val acc: 0.941423\n",
      "Val balanced acc: 0.877237\n",
      "Epoch num: 49 \n",
      "Train loss: 0.006649 \n",
      "Val loss: 0.001516 \n",
      "Val acc: 0.937238\n",
      "Val balanced acc: 0.871088\n",
      "Epoch num: 50 \n",
      "Train loss: 0.006002 \n",
      "Val loss: 0.001572 \n",
      "Val acc: 0.939331\n",
      "Val balanced acc: 0.858696\n",
      "All done!\n",
      "Epoch num: 1 \n",
      "Train loss: 0.123209 \n",
      "Val loss: 0.018720 \n",
      "Val acc: 0.645902\n",
      "Val balanced acc: 0.251374\n",
      "Epoch num: 2 \n",
      "Train loss: 0.077166 \n",
      "Val loss: 0.016266 \n",
      "Val acc: 0.667213\n",
      "Val balanced acc: 0.277289\n",
      "Epoch num: 3 \n",
      "Train loss: 0.063750 \n",
      "Val loss: 0.007455 \n",
      "Val acc: 0.739344\n",
      "Val balanced acc: 0.366013\n",
      "Epoch num: 4 \n",
      "Train loss: 0.057097 \n",
      "Val loss: 0.009199 \n",
      "Val acc: 0.627869\n",
      "Val balanced acc: 0.447639\n",
      "Epoch num: 5 \n",
      "Train loss: 0.056706 \n",
      "Val loss: 0.016641 \n",
      "Val acc: 0.675410\n",
      "Val balanced acc: 0.291942\n",
      "Epoch num: 6 \n",
      "Train loss: 0.054737 \n",
      "Val loss: 0.002978 \n",
      "Val acc: 0.880328\n",
      "Val balanced acc: 0.645303\n",
      "Epoch num: 7 \n",
      "Train loss: 0.047658 \n",
      "Val loss: 0.015047 \n",
      "Val acc: 0.740984\n",
      "Val balanced acc: 0.472150\n",
      "Epoch num: 8 \n",
      "Train loss: 0.047570 \n",
      "Val loss: 0.005795 \n",
      "Val acc: 0.780328\n",
      "Val balanced acc: 0.497899\n",
      "Epoch num: 9 \n",
      "Train loss: 0.042300 \n",
      "Val loss: 0.002454 \n",
      "Val acc: 0.895082\n",
      "Val balanced acc: 0.697462\n",
      "Epoch num: 10 \n",
      "Train loss: 0.039601 \n",
      "Val loss: 0.002518 \n",
      "Val acc: 0.886885\n",
      "Val balanced acc: 0.635602\n",
      "Epoch num: 11 \n",
      "Train loss: 0.039194 \n",
      "Val loss: 0.002875 \n",
      "Val acc: 0.873770\n",
      "Val balanced acc: 0.618430\n",
      "Epoch num: 12 \n",
      "Train loss: 0.036388 \n",
      "Val loss: 0.003937 \n",
      "Val acc: 0.865574\n",
      "Val balanced acc: 0.629384\n",
      "Epoch num: 13 \n",
      "Train loss: 0.036579 \n",
      "Val loss: 0.002232 \n",
      "Val acc: 0.906557\n",
      "Val balanced acc: 0.658126\n",
      "Epoch num: 14 \n",
      "Train loss: 0.035117 \n",
      "Val loss: 0.006816 \n",
      "Val acc: 0.793443\n",
      "Val balanced acc: 0.586249\n",
      "Epoch num: 15 \n",
      "Train loss: 0.037019 \n",
      "Val loss: 0.005235 \n",
      "Val acc: 0.814754\n",
      "Val balanced acc: 0.524099\n",
      "Epoch num: 16 \n",
      "Train loss: 0.065683 \n",
      "Val loss: 0.015798 \n",
      "Val acc: 0.688525\n",
      "Val balanced acc: 0.307426\n",
      "Epoch num: 17 \n",
      "Train loss: 0.051127 \n",
      "Val loss: 0.004308 \n",
      "Val acc: 0.827869\n",
      "Val balanced acc: 0.497903\n",
      "Epoch num: 18 \n",
      "Train loss: 0.048404 \n",
      "Val loss: 0.003379 \n",
      "Val acc: 0.872131\n",
      "Val balanced acc: 0.600933\n",
      "Epoch num: 19 \n",
      "Train loss: 0.042638 \n",
      "Val loss: 0.002617 \n",
      "Val acc: 0.890164\n",
      "Val balanced acc: 0.661494\n",
      "Epoch num: 20 \n",
      "Train loss: 0.049556 \n",
      "Val loss: 0.002665 \n",
      "Val acc: 0.883607\n",
      "Val balanced acc: 0.684518\n",
      "Epoch num: 21 \n",
      "Train loss: 0.043430 \n",
      "Val loss: 0.005098 \n",
      "Val acc: 0.813115\n",
      "Val balanced acc: 0.520756\n",
      "Epoch num: 22 \n",
      "Train loss: 0.034731 \n",
      "Val loss: 0.002539 \n",
      "Val acc: 0.890164\n",
      "Val balanced acc: 0.644871\n",
      "Epoch num: 23 \n",
      "Train loss: 0.034336 \n",
      "Val loss: 0.003045 \n",
      "Val acc: 0.863934\n",
      "Val balanced acc: 0.555832\n",
      "Epoch num: 24 \n",
      "Train loss: 0.033326 \n",
      "Val loss: 0.002373 \n",
      "Val acc: 0.906557\n",
      "Val balanced acc: 0.653643\n",
      "Epoch num: 25 \n",
      "Train loss: 0.030154 \n",
      "Val loss: 0.001964 \n",
      "Val acc: 0.914754\n",
      "Val balanced acc: 0.698528\n",
      "Epoch num: 26 \n",
      "Train loss: 0.033580 \n",
      "Val loss: 0.001878 \n",
      "Val acc: 0.913115\n",
      "Val balanced acc: 0.665429\n",
      "Epoch num: 27 \n",
      "Train loss: 0.029578 \n",
      "Val loss: 0.001795 \n",
      "Val acc: 0.918033\n",
      "Val balanced acc: 0.708186\n",
      "Epoch num: 28 \n",
      "Train loss: 0.026794 \n",
      "Val loss: 0.001894 \n",
      "Val acc: 0.909836\n",
      "Val balanced acc: 0.695050\n",
      "Epoch num: 29 \n",
      "Train loss: 0.026709 \n",
      "Val loss: 0.001842 \n",
      "Val acc: 0.913115\n",
      "Val balanced acc: 0.698781\n",
      "Epoch num: 30 \n",
      "Train loss: 0.024609 \n",
      "Val loss: 0.001897 \n",
      "Val acc: 0.919672\n",
      "Val balanced acc: 0.703975\n",
      "Epoch num: 31 \n",
      "Train loss: 0.024521 \n",
      "Val loss: 0.001844 \n",
      "Val acc: 0.921311\n",
      "Val balanced acc: 0.706633\n",
      "Epoch num: 32 \n",
      "Train loss: 0.023295 \n",
      "Val loss: 0.001840 \n",
      "Val acc: 0.919672\n",
      "Val balanced acc: 0.707902\n",
      "Epoch num: 33 \n",
      "Train loss: 0.022817 \n",
      "Val loss: 0.001824 \n",
      "Val acc: 0.924590\n",
      "Val balanced acc: 0.685813\n",
      "Epoch num: 34 \n",
      "Train loss: 0.026977 \n",
      "Val loss: 0.001834 \n",
      "Val acc: 0.924590\n",
      "Val balanced acc: 0.692574\n",
      "Epoch num: 35 \n",
      "Train loss: 0.024365 \n",
      "Val loss: 0.001938 \n",
      "Val acc: 0.918033\n",
      "Val balanced acc: 0.701658\n",
      "Epoch num: 36 \n",
      "Train loss: 0.022988 \n",
      "Val loss: 0.001896 \n",
      "Val acc: 0.919672\n",
      "Val balanced acc: 0.704275\n",
      "Epoch num: 37 \n",
      "Train loss: 0.024664 \n",
      "Val loss: 0.001850 \n",
      "Val acc: 0.924590\n",
      "Val balanced acc: 0.686456\n",
      "Epoch num: 38 \n",
      "Train loss: 0.023070 \n",
      "Val loss: 0.001786 \n",
      "Val acc: 0.924590\n",
      "Val balanced acc: 0.685622\n",
      "Epoch num: 39 \n",
      "Train loss: 0.024189 \n",
      "Val loss: 0.001825 \n",
      "Val acc: 0.921311\n",
      "Val balanced acc: 0.679440\n",
      "Epoch num: 40 \n",
      "Train loss: 0.023305 \n",
      "Val loss: 0.001820 \n",
      "Val acc: 0.926230\n",
      "Val balanced acc: 0.687037\n",
      "Epoch num: 41 \n",
      "Train loss: 0.020847 \n",
      "Val loss: 0.001835 \n",
      "Val acc: 0.924590\n",
      "Val balanced acc: 0.716796\n",
      "Epoch num: 42 \n",
      "Train loss: 0.021583 \n",
      "Val loss: 0.001734 \n",
      "Val acc: 0.931148\n",
      "Val balanced acc: 0.710887\n",
      "Epoch num: 43 \n",
      "Train loss: 0.024509 \n",
      "Val loss: 0.001767 \n",
      "Val acc: 0.924590\n",
      "Val balanced acc: 0.714495\n",
      "Epoch num: 44 \n",
      "Train loss: 0.020422 \n",
      "Val loss: 0.001660 \n",
      "Val acc: 0.931148\n",
      "Val balanced acc: 0.707261\n",
      "Epoch num: 45 \n",
      "Train loss: 0.020925 \n",
      "Val loss: 0.001726 \n",
      "Val acc: 0.927869\n",
      "Val balanced acc: 0.710249\n",
      "Epoch num: 46 \n",
      "Train loss: 0.021906 \n",
      "Val loss: 0.001858 \n",
      "Val acc: 0.918033\n",
      "Val balanced acc: 0.702792\n",
      "Epoch num: 47 \n",
      "Train loss: 0.021538 \n",
      "Val loss: 0.001884 \n",
      "Val acc: 0.924590\n",
      "Val balanced acc: 0.707010\n",
      "Epoch num: 48 \n",
      "Train loss: 0.019995 \n",
      "Val loss: 0.001699 \n",
      "Val acc: 0.929508\n",
      "Val balanced acc: 0.712156\n",
      "Epoch num: 49 \n",
      "Train loss: 0.023852 \n",
      "Val loss: 0.001785 \n",
      "Val acc: 0.926230\n",
      "Val balanced acc: 0.688171\n",
      "Epoch num: 50 \n",
      "Train loss: 0.020286 \n",
      "Val loss: 0.001905 \n",
      "Val acc: 0.921311\n",
      "Val balanced acc: 0.707450\n",
      "All done!\n",
      "Epoch num: 1 \n",
      "Train loss: 0.117366 \n",
      "Val loss: 0.051180 \n",
      "Val acc: 0.326861\n",
      "Val balanced acc: 0.273469\n",
      "Epoch num: 2 \n",
      "Train loss: 0.056298 \n",
      "Val loss: 0.054698 \n",
      "Val acc: 0.323625\n",
      "Val balanced acc: 0.269388\n",
      "Epoch num: 3 \n",
      "Train loss: 0.046074 \n",
      "Val loss: 0.007795 \n",
      "Val acc: 0.744337\n",
      "Val balanced acc: 0.408488\n",
      "Epoch num: 4 \n",
      "Train loss: 0.032835 \n",
      "Val loss: 0.038247 \n",
      "Val acc: 0.411003\n",
      "Val balanced acc: 0.319249\n",
      "Epoch num: 5 \n",
      "Train loss: 0.024998 \n",
      "Val loss: 0.009202 \n",
      "Val acc: 0.679612\n",
      "Val balanced acc: 0.446798\n",
      "Epoch num: 6 \n",
      "Train loss: 0.021079 \n",
      "Val loss: 0.017421 \n",
      "Val acc: 0.621359\n",
      "Val balanced acc: 0.309784\n",
      "Epoch num: 7 \n",
      "Train loss: 0.017548 \n",
      "Val loss: 0.006239 \n",
      "Val acc: 0.818770\n",
      "Val balanced acc: 0.535794\n",
      "Epoch num: 8 \n",
      "Train loss: 0.015866 \n",
      "Val loss: 0.003870 \n",
      "Val acc: 0.883495\n",
      "Val balanced acc: 0.533808\n",
      "Epoch num: 9 \n",
      "Train loss: 0.015274 \n",
      "Val loss: 0.004462 \n",
      "Val acc: 0.880259\n",
      "Val balanced acc: 0.531295\n",
      "Epoch num: 10 \n",
      "Train loss: 0.015801 \n",
      "Val loss: 0.004148 \n",
      "Val acc: 0.902913\n",
      "Val balanced acc: 0.584879\n",
      "Epoch num: 11 \n",
      "Train loss: 0.016907 \n",
      "Val loss: 0.004941 \n",
      "Val acc: 0.867314\n",
      "Val balanced acc: 0.511967\n",
      "Epoch num: 12 \n",
      "Train loss: 0.015465 \n",
      "Val loss: 0.003974 \n",
      "Val acc: 0.873786\n",
      "Val balanced acc: 0.657096\n",
      "Epoch num: 13 \n",
      "Train loss: 0.014561 \n",
      "Val loss: 0.001981 \n",
      "Val acc: 0.935275\n",
      "Val balanced acc: 0.636629\n",
      "Epoch num: 14 \n",
      "Train loss: 0.014166 \n",
      "Val loss: 0.004522 \n",
      "Val acc: 0.873786\n",
      "Val balanced acc: 0.533310\n",
      "Epoch num: 15 \n",
      "Train loss: 0.013885 \n",
      "Val loss: 0.006252 \n",
      "Val acc: 0.825243\n",
      "Val balanced acc: 0.474509\n",
      "Epoch num: 16 \n",
      "Train loss: 0.012217 \n",
      "Val loss: 0.003355 \n",
      "Val acc: 0.909385\n",
      "Val balanced acc: 0.565281\n",
      "Epoch num: 17 \n",
      "Train loss: 0.011177 \n",
      "Val loss: 0.002630 \n",
      "Val acc: 0.938511\n",
      "Val balanced acc: 0.607303\n",
      "Epoch num: 18 \n",
      "Train loss: 0.009304 \n",
      "Val loss: 0.001776 \n",
      "Val acc: 0.954693\n",
      "Val balanced acc: 0.630165\n",
      "Epoch num: 19 \n",
      "Train loss: 0.011357 \n",
      "Val loss: 0.009433 \n",
      "Val acc: 0.776699\n",
      "Val balanced acc: 0.603646\n",
      "Epoch num: 20 \n",
      "Train loss: 0.011596 \n",
      "Val loss: 0.004771 \n",
      "Val acc: 0.844660\n",
      "Val balanced acc: 0.603828\n",
      "Epoch num: 21 \n",
      "Train loss: 0.010948 \n",
      "Val loss: 0.002786 \n",
      "Val acc: 0.932039\n",
      "Val balanced acc: 0.601037\n",
      "Epoch num: 22 \n",
      "Train loss: 0.010602 \n",
      "Val loss: 0.001917 \n",
      "Val acc: 0.951456\n",
      "Val balanced acc: 0.686772\n",
      "Epoch num: 23 \n",
      "Train loss: 0.010678 \n",
      "Val loss: 0.002619 \n",
      "Val acc: 0.935275\n",
      "Val balanced acc: 0.603344\n",
      "Epoch num: 24 \n",
      "Train loss: 0.010858 \n",
      "Val loss: 0.001788 \n",
      "Val acc: 0.944984\n",
      "Val balanced acc: 0.626872\n",
      "Epoch num: 25 \n",
      "Train loss: 0.009556 \n",
      "Val loss: 0.001615 \n",
      "Val acc: 0.954693\n",
      "Val balanced acc: 0.667351\n",
      "Epoch num: 26 \n",
      "Train loss: 0.010694 \n",
      "Val loss: 0.005045 \n",
      "Val acc: 0.854369\n",
      "Val balanced acc: 0.636309\n",
      "Epoch num: 27 \n",
      "Train loss: 0.009136 \n",
      "Val loss: 0.001907 \n",
      "Val acc: 0.948220\n",
      "Val balanced acc: 0.612852\n",
      "Epoch num: 28 \n",
      "Train loss: 0.009301 \n",
      "Val loss: 0.001288 \n",
      "Val acc: 0.957929\n",
      "Val balanced acc: 0.672065\n",
      "Epoch num: 29 \n",
      "Train loss: 0.007486 \n",
      "Val loss: 0.001374 \n",
      "Val acc: 0.957929\n",
      "Val balanced acc: 0.632746\n",
      "Epoch num: 30 \n",
      "Train loss: 0.006871 \n",
      "Val loss: 0.001788 \n",
      "Val acc: 0.954693\n",
      "Val balanced acc: 0.681666\n",
      "Epoch num: 31 \n",
      "Train loss: 0.007160 \n",
      "Val loss: 0.001291 \n",
      "Val acc: 0.951456\n",
      "Val balanced acc: 0.681097\n",
      "Epoch num: 32 \n",
      "Train loss: 0.008374 \n",
      "Val loss: 0.002208 \n",
      "Val acc: 0.948220\n",
      "Val balanced acc: 0.591870\n",
      "Epoch num: 33 \n",
      "Train loss: 0.008703 \n",
      "Val loss: 0.003281 \n",
      "Val acc: 0.935275\n",
      "Val balanced acc: 0.550577\n",
      "Epoch num: 34 \n",
      "Train loss: 0.008768 \n",
      "Val loss: 0.010800 \n",
      "Val acc: 0.760518\n",
      "Val balanced acc: 0.626770\n",
      "Epoch num: 35 \n",
      "Train loss: 0.009652 \n",
      "Val loss: 0.002365 \n",
      "Val acc: 0.932039\n",
      "Val balanced acc: 0.612793\n",
      "Epoch num: 36 \n",
      "Train loss: 0.010376 \n",
      "Val loss: 0.003245 \n",
      "Val acc: 0.922330\n",
      "Val balanced acc: 0.734462\n",
      "Epoch num: 37 \n",
      "Train loss: 0.008270 \n",
      "Val loss: 0.002265 \n",
      "Val acc: 0.941748\n",
      "Val balanced acc: 0.662223\n",
      "Epoch num: 38 \n",
      "Train loss: 0.007736 \n",
      "Val loss: 0.002047 \n",
      "Val acc: 0.944984\n",
      "Val balanced acc: 0.690715\n",
      "Epoch num: 39 \n",
      "Train loss: 0.006691 \n",
      "Val loss: 0.001760 \n",
      "Val acc: 0.948220\n",
      "Val balanced acc: 0.682965\n",
      "Epoch num: 40 \n",
      "Train loss: 0.005590 \n",
      "Val loss: 0.001356 \n",
      "Val acc: 0.957929\n",
      "Val balanced acc: 0.690045\n",
      "Epoch num: 41 \n",
      "Train loss: 0.004471 \n",
      "Val loss: 0.001429 \n",
      "Val acc: 0.964401\n",
      "Val balanced acc: 0.699709\n",
      "Epoch num: 42 \n",
      "Train loss: 0.004299 \n",
      "Val loss: 0.001464 \n",
      "Val acc: 0.961165\n",
      "Val balanced acc: 0.697639\n",
      "Epoch num: 43 \n",
      "Train loss: 0.004242 \n",
      "Val loss: 0.001393 \n",
      "Val acc: 0.964401\n",
      "Val balanced acc: 0.698566\n",
      "Epoch num: 44 \n",
      "Train loss: 0.003998 \n",
      "Val loss: 0.001377 \n",
      "Val acc: 0.967638\n",
      "Val balanced acc: 0.702648\n",
      "Epoch num: 45 \n",
      "Train loss: 0.003660 \n",
      "Val loss: 0.001414 \n",
      "Val acc: 0.964401\n",
      "Val balanced acc: 0.698566\n",
      "Epoch num: 46 \n",
      "Train loss: 0.004461 \n",
      "Val loss: 0.001381 \n",
      "Val acc: 0.964401\n",
      "Val balanced acc: 0.698566\n",
      "Epoch num: 47 \n",
      "Train loss: 0.004106 \n",
      "Val loss: 0.001542 \n",
      "Val acc: 0.964401\n",
      "Val balanced acc: 0.699076\n",
      "Epoch num: 48 \n",
      "Train loss: 0.004348 \n",
      "Val loss: 0.001502 \n",
      "Val acc: 0.964401\n",
      "Val balanced acc: 0.699076\n",
      "Epoch num: 49 \n",
      "Train loss: 0.003714 \n",
      "Val loss: 0.001420 \n",
      "Val acc: 0.967638\n",
      "Val balanced acc: 0.702648\n",
      "Epoch num: 50 \n",
      "Train loss: 0.003982 \n",
      "Val loss: 0.001406 \n",
      "Val acc: 0.967638\n",
      "Val balanced acc: 0.702648\n",
      "All done!\n",
      "Epoch num: 1 \n",
      "Train loss: 0.142207 \n",
      "Val loss: 0.072268 \n",
      "Val acc: 0.086598\n",
      "Val balanced acc: 0.242857\n",
      "Epoch num: 2 \n",
      "Train loss: 0.063377 \n",
      "Val loss: 0.068411 \n",
      "Val acc: 0.179381\n",
      "Val balanced acc: 0.232246\n",
      "Epoch num: 3 \n",
      "Train loss: 0.063817 \n",
      "Val loss: 0.011212 \n",
      "Val acc: 0.569072\n",
      "Val balanced acc: 0.560745\n",
      "Epoch num: 4 \n",
      "Train loss: 0.055298 \n",
      "Val loss: 0.017810 \n",
      "Val acc: 0.556701\n",
      "Val balanced acc: 0.502230\n",
      "Epoch num: 5 \n",
      "Train loss: 0.048067 \n",
      "Val loss: 0.003998 \n",
      "Val acc: 0.820619\n",
      "Val balanced acc: 0.623991\n",
      "Epoch num: 6 \n",
      "Train loss: 0.050551 \n",
      "Val loss: 0.005989 \n",
      "Val acc: 0.746392\n",
      "Val balanced acc: 0.587521\n",
      "Epoch num: 7 \n",
      "Train loss: 0.048920 \n",
      "Val loss: 0.004134 \n",
      "Val acc: 0.812371\n",
      "Val balanced acc: 0.641211\n",
      "Epoch num: 8 \n",
      "Train loss: 0.045967 \n",
      "Val loss: 0.016885 \n",
      "Val acc: 0.602062\n",
      "Val balanced acc: 0.386410\n",
      "Epoch num: 9 \n",
      "Train loss: 0.048835 \n",
      "Val loss: 0.005449 \n",
      "Val acc: 0.789691\n",
      "Val balanced acc: 0.671911\n",
      "Epoch num: 10 \n",
      "Train loss: 0.045328 \n",
      "Val loss: 0.005469 \n",
      "Val acc: 0.785567\n",
      "Val balanced acc: 0.538720\n",
      "Epoch num: 11 \n",
      "Train loss: 0.039394 \n",
      "Val loss: 0.008164 \n",
      "Val acc: 0.742268\n",
      "Val balanced acc: 0.616266\n",
      "Epoch num: 12 \n",
      "Train loss: 0.037935 \n",
      "Val loss: 0.007212 \n",
      "Val acc: 0.723711\n",
      "Val balanced acc: 0.537149\n",
      "Epoch num: 13 \n",
      "Train loss: 0.041022 \n",
      "Val loss: 0.005034 \n",
      "Val acc: 0.785567\n",
      "Val balanced acc: 0.634312\n",
      "Epoch num: 14 \n",
      "Train loss: 0.038438 \n",
      "Val loss: 0.004919 \n",
      "Val acc: 0.797938\n",
      "Val balanced acc: 0.535988\n",
      "Epoch num: 15 \n",
      "Train loss: 0.042914 \n",
      "Val loss: 0.003584 \n",
      "Val acc: 0.868041\n",
      "Val balanced acc: 0.707671\n",
      "Epoch num: 16 \n",
      "Train loss: 0.042643 \n",
      "Val loss: 0.005674 \n",
      "Val acc: 0.779381\n",
      "Val balanced acc: 0.608278\n",
      "Epoch num: 17 \n",
      "Train loss: 0.037507 \n",
      "Val loss: 0.002908 \n",
      "Val acc: 0.872165\n",
      "Val balanced acc: 0.683082\n",
      "Epoch num: 18 \n",
      "Train loss: 0.034581 \n",
      "Val loss: 0.003413 \n",
      "Val acc: 0.851546\n",
      "Val balanced acc: 0.708988\n",
      "Epoch num: 19 \n",
      "Train loss: 0.037586 \n",
      "Val loss: 0.002861 \n",
      "Val acc: 0.870103\n",
      "Val balanced acc: 0.739541\n",
      "Epoch num: 20 \n",
      "Train loss: 0.037645 \n",
      "Val loss: 0.002957 \n",
      "Val acc: 0.870103\n",
      "Val balanced acc: 0.756031\n",
      "Epoch num: 21 \n",
      "Train loss: 0.034306 \n",
      "Val loss: 0.002982 \n",
      "Val acc: 0.886598\n",
      "Val balanced acc: 0.723545\n",
      "Epoch num: 22 \n",
      "Train loss: 0.032096 \n",
      "Val loss: 0.006741 \n",
      "Val acc: 0.762887\n",
      "Val balanced acc: 0.608908\n",
      "Epoch num: 23 \n",
      "Train loss: 0.038228 \n",
      "Val loss: 0.017014 \n",
      "Val acc: 0.597938\n",
      "Val balanced acc: 0.517646\n",
      "Epoch num: 24 \n",
      "Train loss: 0.036489 \n",
      "Val loss: 0.002319 \n",
      "Val acc: 0.903093\n",
      "Val balanced acc: 0.721955\n",
      "Epoch num: 25 \n",
      "Train loss: 0.034640 \n",
      "Val loss: 0.058628 \n",
      "Val acc: 0.505155\n",
      "Val balanced acc: 0.479676\n",
      "Epoch num: 26 \n",
      "Train loss: 0.034562 \n",
      "Val loss: 0.006442 \n",
      "Val acc: 0.769072\n",
      "Val balanced acc: 0.692690\n",
      "Epoch num: 27 \n",
      "Train loss: 0.034306 \n",
      "Val loss: 0.005083 \n",
      "Val acc: 0.814433\n",
      "Val balanced acc: 0.695196\n",
      "Epoch num: 28 \n",
      "Train loss: 0.032414 \n",
      "Val loss: 0.005647 \n",
      "Val acc: 0.779381\n",
      "Val balanced acc: 0.640278\n",
      "Epoch num: 29 \n",
      "Train loss: 0.034550 \n",
      "Val loss: 0.004186 \n",
      "Val acc: 0.824742\n",
      "Val balanced acc: 0.726211\n",
      "Epoch num: 30 \n",
      "Train loss: 0.034886 \n",
      "Val loss: 0.006333 \n",
      "Val acc: 0.797938\n",
      "Val balanced acc: 0.650062\n",
      "Epoch num: 31 \n",
      "Train loss: 0.029936 \n",
      "Val loss: 0.005322 \n",
      "Val acc: 0.818557\n",
      "Val balanced acc: 0.661682\n",
      "Epoch num: 32 \n",
      "Train loss: 0.026849 \n",
      "Val loss: 0.005363 \n",
      "Val acc: 0.800000\n",
      "Val balanced acc: 0.581124\n",
      "Epoch num: 33 \n",
      "Train loss: 0.029972 \n",
      "Val loss: 0.002486 \n",
      "Val acc: 0.890722\n",
      "Val balanced acc: 0.749426\n",
      "Epoch num: 34 \n",
      "Train loss: 0.029543 \n",
      "Val loss: 0.004385 \n",
      "Val acc: 0.859794\n",
      "Val balanced acc: 0.676202\n",
      "Epoch num: 35 \n",
      "Train loss: 0.028359 \n",
      "Val loss: 0.007291 \n",
      "Val acc: 0.760825\n",
      "Val balanced acc: 0.672736\n",
      "Epoch num: 36 \n",
      "Train loss: 0.025911 \n",
      "Val loss: 0.002049 \n",
      "Val acc: 0.913402\n",
      "Val balanced acc: 0.801857\n",
      "Epoch num: 37 \n",
      "Train loss: 0.019877 \n",
      "Val loss: 0.002066 \n",
      "Val acc: 0.907217\n",
      "Val balanced acc: 0.789658\n",
      "Epoch num: 38 \n",
      "Train loss: 0.020948 \n",
      "Val loss: 0.001771 \n",
      "Val acc: 0.915464\n",
      "Val balanced acc: 0.795178\n",
      "Epoch num: 39 \n",
      "Train loss: 0.020407 \n",
      "Val loss: 0.001690 \n",
      "Val acc: 0.915464\n",
      "Val balanced acc: 0.805976\n",
      "Epoch num: 40 \n",
      "Train loss: 0.019601 \n",
      "Val loss: 0.001751 \n",
      "Val acc: 0.913402\n",
      "Val balanced acc: 0.807332\n",
      "Epoch num: 41 \n",
      "Train loss: 0.017581 \n",
      "Val loss: 0.002354 \n",
      "Val acc: 0.903093\n",
      "Val balanced acc: 0.765988\n",
      "Epoch num: 42 \n",
      "Train loss: 0.017617 \n",
      "Val loss: 0.001761 \n",
      "Val acc: 0.921650\n",
      "Val balanced acc: 0.808207\n",
      "Epoch num: 43 \n",
      "Train loss: 0.020017 \n",
      "Val loss: 0.001795 \n",
      "Val acc: 0.919588\n",
      "Val balanced acc: 0.796303\n",
      "Epoch num: 44 \n",
      "Train loss: 0.017677 \n",
      "Val loss: 0.002541 \n",
      "Val acc: 0.896907\n",
      "Val balanced acc: 0.763433\n",
      "Epoch num: 45 \n",
      "Train loss: 0.020536 \n",
      "Val loss: 0.001668 \n",
      "Val acc: 0.927835\n",
      "Val balanced acc: 0.819121\n",
      "Epoch num: 46 \n",
      "Train loss: 0.019021 \n",
      "Val loss: 0.001804 \n",
      "Val acc: 0.911340\n",
      "Val balanced acc: 0.797938\n",
      "Epoch num: 47 \n",
      "Train loss: 0.018588 \n",
      "Val loss: 0.001747 \n",
      "Val acc: 0.921650\n",
      "Val balanced acc: 0.819005\n",
      "Epoch num: 48 \n",
      "Train loss: 0.018472 \n",
      "Val loss: 0.002063 \n",
      "Val acc: 0.907217\n",
      "Val balanced acc: 0.771125\n",
      "Epoch num: 49 \n",
      "Train loss: 0.019393 \n",
      "Val loss: 0.002242 \n",
      "Val acc: 0.905155\n",
      "Val balanced acc: 0.772852\n",
      "Epoch num: 50 \n",
      "Train loss: 0.020462 \n",
      "Val loss: 0.002009 \n",
      "Val acc: 0.911340\n",
      "Val balanced acc: 0.798140\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "data_flag = 'bloodmnist'\n",
    "\n",
    "info = medmnist.INFO[data_flag].copy()\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "for i in range(num_clients):\n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "    root = os.path.join(part_dir, \"client_\" + str(i))\n",
    "    train_dataset = DataClass(root=os.path.join(part_dir, \"client_\" + str(i)), split='train', download=False)\n",
    "    val_dataset = DataClass(root=os.path.join(part_dir, \"client_\" + str(i)), split='val', download=False)\n",
    "    info = medmnist.INFO[data_flag].copy()\n",
    "    train_dataset.info = info\n",
    "    val_dataset.info = info\n",
    "    train_dataset.info[\"n_samples\"][\"train\"] = train_dataset.imgs.shape[0]\n",
    "    val_dataset.info[\"n_samples\"][\"val\"] = val_dataset.imgs.shape[0]\n",
    "    #TODO NEED TO FIX THIS INFO SAMPLE SIZE ISSUE\n",
    "    # train_dataset.info[\"n_samples\"][\"test\"] = 0\n",
    "\n",
    "    aug_list = []\n",
    "    aug_list.append(transforms.RandomCrop(28, padding=4))\n",
    "    aug_list.append(transforms.RandomHorizontalFlip())\n",
    "    preprocess_list = [transforms.ToTensor(), transforms.Normalize(mean=[.5], std=[.5])]\n",
    "\n",
    "    loc_trainer_test = LocalTrainer(ResNet18(in_channels=3, num_classes=8), DataClass, os.path.join(\"blood_models_dirich\", \"client_\" + str(i)), n_classes, root=root, epochs=50, lr_adj = [25, 40], aug=aug_list, preprocess=preprocess_list, seed=i)\n",
    "    loc_trainer_test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18153,
     "status": "ok",
     "timestamp": 1734443789374,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "XiNpAqXB05XG",
    "outputId": "986c0441-9317-4242-ccb8-2adad68fddfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python_class': 'BloodMNIST', 'description': 'The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3×360×363 pixels are center-cropped into 3×200×200, and then resized into 3×28×28.', 'url': 'https://zenodo.org/records/10519652/files/bloodmnist.npz?download=1', 'MD5': '7053d0359d879ad8a5505303e11de1dc', 'url_64': 'https://zenodo.org/records/10519652/files/bloodmnist_64.npz?download=1', 'MD5_64': '2b94928a2ae4916078ca51e05b6b800b', 'url_128': 'https://zenodo.org/records/10519652/files/bloodmnist_128.npz?download=1', 'MD5_128': 'adace1e0ed228fccda1f39692059dd4c', 'url_224': 'https://zenodo.org/records/10519652/files/bloodmnist_224.npz?download=1', 'MD5_224': 'b718ff6835fcbdb22ba9eacccd7b2601', 'task': 'multi-class', 'label': {'0': 'basophil', '1': 'eosinophil', '2': 'erythroblast', '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)', '4': 'lymphocyte', '5': 'monocyte', '6': 'neutrophil', '7': 'platelet'}, 'n_channels': 3, 'n_samples': {'train': 11959, 'val': 1712, 'test': 3421}, 'license': 'CC BY 4.0'}\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "FedBlood = fedisca.FedISCA(test_data_dir=\"./\", model_weights_dir=\"./blood_models_dirich\", exper_dir=\"./blood_exper_dirich/federated\", input_size=28, in_channels=3, num_classes=8, batch_size=128, epochs=25, iter_mi=500, lr_steps=[16, 20], log_freq=100, is_medmnist=True, medmnist=\"bloodmnist\")\n",
    "ens_local = FedBlood.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7091593,
     "status": "ok",
     "timestamp": 1734426696182,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "z4IRhjzl05XG",
    "outputId": "af68ec2d-5478-447d-844c-0bd6c0b23ae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7,\n",
      "        0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7,\n",
      "        0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7,\n",
      "        0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7,\n",
      "        0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7,\n",
      "        0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')\n",
      "Dataset BloodMNIST of size 28 (bloodmnist)\n",
      "    Number of datapoints: 3421\n",
      "    Root location: .\n",
      "    Split: test\n",
      "    Task: multi-class\n",
      "    Number of channels: 3\n",
      "    Meaning of labels: {'0': 'basophil', '1': 'eosinophil', '2': 'erythroblast', '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)', '4': 'lymphocyte', '5': 'monocyte', '6': 'neutrophil', '7': 'platelet'}\n",
      "    Number of samples: {'train': 11959, 'val': 1712, 'test': 3421}\n",
      "    Description: The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3×360×363 pixels are center-cropped into 3×200×200, and then resized into 3×28×28.\n",
      "    License: CC BY 4.0\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238288.531,\ttarget: 3.460 \tR_feature_loss scaled:\t 238285.000\n",
      "It 100\t Losses: total: 35718.418,\ttarget: 5.462 \tR_feature_loss scaled:\t 35712.918\n",
      "It 200\t Losses: total: 22838.713,\ttarget: 5.822 \tR_feature_loss scaled:\t 22832.859\n",
      "It 300\t Losses: total: 15410.664,\ttarget: 5.822 \tR_feature_loss scaled:\t 15404.813\n",
      "It 400\t Losses: total: 12803.430,\ttarget: 5.874 \tR_feature_loss scaled:\t 12797.529\n",
      "It 500\t Losses: total: 15808.823,\ttarget: 5.987 \tR_feature_loss scaled:\t 15802.811\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([1, 6, 1, 6, 2, 3, 3, 3, 3, 2, 3, 3, 2, 6, 3, 3, 1, 1, 5, 5, 1, 2, 2, 3,\n",
      "        4, 1, 1, 3, 1, 6, 3, 3, 3, 3, 6, 1, 3, 2, 5, 3, 2, 4, 2, 2, 2, 2, 5, 2,\n",
      "        2, 6, 1, 3, 1, 2, 2, 1, 3, 3, 3, 5, 5, 3, 5, 3, 5, 2, 1, 3, 3, 5, 1, 2,\n",
      "        3, 1, 3, 2, 4, 3, 3, 3, 2, 2, 1, 2, 2, 3, 3, 3, 3, 1, 3, 3, 5],\n",
      "       device='cuda:0')\n",
      "Loss: 1.996 | Acc: 51.973% (1778/3421), B. Acc: 0.503%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238489.125,\ttarget: 3.501 \tR_feature_loss scaled:\t 238485.547\n",
      "It 100\t Losses: total: 38717.262,\ttarget: 5.120 \tR_feature_loss scaled:\t 38712.102\n",
      "It 200\t Losses: total: 23222.920,\ttarget: 5.663 \tR_feature_loss scaled:\t 23217.225\n",
      "It 300\t Losses: total: 34149.215,\ttarget: 5.988 \tR_feature_loss scaled:\t 34143.199\n",
      "It 400\t Losses: total: 15411.487,\ttarget: 5.824 \tR_feature_loss scaled:\t 15405.637\n",
      "It 500\t Losses: total: 19242.961,\ttarget: 5.817 \tR_feature_loss scaled:\t 19237.117\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([4, 5, 2, 4, 4, 5, 3, 4, 2, 2, 2, 5, 0, 4, 5, 1, 6, 5, 3, 1, 2, 3, 2, 6,\n",
      "        4, 5, 2, 5, 5, 6, 3, 2, 5, 1, 1, 4, 5, 1, 6, 1, 4, 2, 3, 1, 2, 4, 2, 2,\n",
      "        1, 2, 4, 1, 4, 5, 2, 4, 6, 1, 2, 3, 2, 5, 2, 2, 3, 4, 2, 4, 5, 3, 2, 1,\n",
      "        1, 5, 0, 3, 2, 3, 3, 3, 1, 1, 6, 5, 2, 4, 2, 2, 2, 3, 6, 1, 3],\n",
      "       device='cuda:0')\n",
      "Loss: 2.276 | Acc: 53.610% (1834/3421), B. Acc: 0.553%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238142.422,\ttarget: 3.428 \tR_feature_loss scaled:\t 238138.922\n",
      "It 100\t Losses: total: 32136.600,\ttarget: 5.268 \tR_feature_loss scaled:\t 32131.293\n",
      "It 200\t Losses: total: 18389.879,\ttarget: 5.508 \tR_feature_loss scaled:\t 18384.340\n",
      "It 300\t Losses: total: 16315.762,\ttarget: 5.700 \tR_feature_loss scaled:\t 16310.034\n",
      "It 400\t Losses: total: 13830.139,\ttarget: 5.468 \tR_feature_loss scaled:\t 13824.644\n",
      "It 500\t Losses: total: 17849.945,\ttarget: 5.573 \tR_feature_loss scaled:\t 17844.348\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([2, 1, 2, 4, 0, 1, 1, 3, 3, 3, 3, 6, 3, 1, 0, 1, 6, 2, 5, 2, 2, 5, 2, 2,\n",
      "        4, 6, 2, 3, 3, 3, 2, 4, 5, 5, 2, 4, 6, 5, 2, 2, 3, 1, 5, 6, 2, 4, 1, 2,\n",
      "        4, 3, 1, 6, 2, 6, 1, 2, 2, 3, 4, 1, 1, 5, 5, 2, 1, 2, 2, 1, 2, 3, 4, 5,\n",
      "        1, 5, 2, 1, 5, 4, 5, 6, 2, 2, 1, 3, 1, 3, 3, 2, 6, 2, 3, 1, 5],\n",
      "       device='cuda:0')\n",
      "Loss: 1.866 | Acc: 62.029% (2122/3421), B. Acc: 0.620%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 237943.422,\ttarget: 3.442 \tR_feature_loss scaled:\t 237939.906\n",
      "It 100\t Losses: total: 33049.641,\ttarget: 5.176 \tR_feature_loss scaled:\t 33044.426\n",
      "It 200\t Losses: total: 23280.521,\ttarget: 5.695 \tR_feature_loss scaled:\t 23274.795\n",
      "It 300\t Losses: total: 26979.082,\ttarget: 5.866 \tR_feature_loss scaled:\t 26973.188\n",
      "It 400\t Losses: total: 15398.048,\ttarget: 5.952 \tR_feature_loss scaled:\t 15392.069\n",
      "It 500\t Losses: total: 22050.746,\ttarget: 6.436 \tR_feature_loss scaled:\t 22044.285\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([6, 1, 5, 4, 5, 3, 3, 5, 5, 4, 1, 6, 1, 6, 1, 2, 5, 6, 4, 4, 5, 3, 3, 1,\n",
      "        5, 2, 4, 4, 2, 6, 3, 5, 5, 2, 1, 3, 3, 1, 3, 5, 1, 2, 2, 3, 3, 2, 5, 5,\n",
      "        2, 6, 6, 5, 2, 1, 1, 4, 3, 1, 6, 1, 2, 6, 3, 3, 3, 4, 6, 2, 4, 3, 4, 1,\n",
      "        2, 5, 5, 5, 5, 6, 2, 5, 6, 3, 2, 1, 3, 2, 4, 2, 2, 5, 4, 6, 3],\n",
      "       device='cuda:0')\n",
      "Loss: 1.918 | Acc: 61.766% (2113/3421), B. Acc: 0.615%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238286.469,\ttarget: 3.453 \tR_feature_loss scaled:\t 238282.938\n",
      "It 100\t Losses: total: 35751.641,\ttarget: 5.323 \tR_feature_loss scaled:\t 35746.277\n",
      "It 200\t Losses: total: 22605.979,\ttarget: 6.152 \tR_feature_loss scaled:\t 22599.795\n",
      "It 300\t Losses: total: 14432.554,\ttarget: 6.180 \tR_feature_loss scaled:\t 14426.346\n",
      "It 400\t Losses: total: 19721.928,\ttarget: 6.424 \tR_feature_loss scaled:\t 19715.477\n",
      "It 500\t Losses: total: 14038.438,\ttarget: 6.242 \tR_feature_loss scaled:\t 14032.170\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([3, 1, 2, 1, 2, 3, 6, 6, 4, 5, 5, 3, 2, 2, 3, 2, 2, 3, 4, 5, 6, 1, 6, 3,\n",
      "        5, 2, 3, 2, 3, 5, 6, 2, 4, 3, 2, 2, 6, 3, 0, 2, 4, 5, 6, 3, 2, 5, 3, 2,\n",
      "        6, 5, 5, 4, 3, 6, 2, 5, 4, 4, 4, 2, 1, 1, 2, 4, 3, 2, 3, 4, 2, 2, 1, 2,\n",
      "        1, 6, 3, 2, 2, 1, 4, 1, 3, 6, 5, 6, 3, 5, 1, 1, 3, 1, 3, 2, 3],\n",
      "       device='cuda:0')\n",
      "Loss: 1.784 | Acc: 65.858% (2253/3421), B. Acc: 0.647%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238340.500,\ttarget: 3.462 \tR_feature_loss scaled:\t 238336.969\n",
      "It 100\t Losses: total: 38522.746,\ttarget: 5.622 \tR_feature_loss scaled:\t 38517.086\n",
      "It 200\t Losses: total: 18926.023,\ttarget: 5.918 \tR_feature_loss scaled:\t 18920.074\n",
      "It 300\t Losses: total: 16878.104,\ttarget: 5.993 \tR_feature_loss scaled:\t 16872.082\n",
      "It 400\t Losses: total: 15947.302,\ttarget: 6.054 \tR_feature_loss scaled:\t 15941.221\n",
      "It 500\t Losses: total: 13712.252,\ttarget: 6.047 \tR_feature_loss scaled:\t 13706.180\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([3, 6, 6, 1, 4, 3, 2, 4, 1, 6, 3, 6, 5, 1, 4, 2, 4, 5, 3, 2, 3, 2, 1, 5,\n",
      "        3, 4, 6, 2, 2, 4, 2, 4, 4, 3, 2, 6, 2, 6, 1, 2, 5, 2, 3, 3, 3, 2, 5, 2,\n",
      "        4, 2, 5, 1, 1, 1, 6, 1, 5, 3, 1, 2, 1, 3, 3, 3, 5, 1, 6, 5, 4, 2, 1, 6,\n",
      "        2, 5, 2, 6, 5, 2, 4, 4, 2, 3, 2, 4, 4, 2, 1, 6, 4, 2, 3, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 1.851 | Acc: 66.852% (2287/3421), B. Acc: 0.670%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238223.797,\ttarget: 3.437 \tR_feature_loss scaled:\t 238220.281\n",
      "It 100\t Losses: total: 35302.797,\ttarget: 4.952 \tR_feature_loss scaled:\t 35297.805\n",
      "It 200\t Losses: total: 18843.371,\ttarget: 5.712 \tR_feature_loss scaled:\t 18837.629\n",
      "It 300\t Losses: total: 19401.100,\ttarget: 5.619 \tR_feature_loss scaled:\t 19395.451\n",
      "It 400\t Losses: total: 15314.061,\ttarget: 5.878 \tR_feature_loss scaled:\t 15308.156\n",
      "It 500\t Losses: total: 16660.781,\ttarget: 5.689 \tR_feature_loss scaled:\t 16655.066\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([6, 1, 1, 4, 2, 3, 2, 1, 1, 4, 5, 3, 2, 6, 0, 4, 4, 4, 3, 2, 6, 2, 1, 2,\n",
      "        5, 6, 1, 1, 3, 6, 6, 6, 1, 3, 2, 0, 6, 2, 5, 5, 5, 5, 6, 6, 3, 5, 2, 4,\n",
      "        3, 4, 6, 2, 3, 2, 5, 1, 2, 6, 1, 6, 5, 6, 1, 0, 6, 0, 6, 2, 6, 2, 3, 2,\n",
      "        2, 1, 2, 3, 5, 1, 1, 4, 5, 2, 0, 1, 3, 5, 6, 2, 1, 4, 6, 1, 6],\n",
      "       device='cuda:0')\n",
      "Loss: 1.566 | Acc: 69.015% (2361/3421), B. Acc: 0.696%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238196.484,\ttarget: 3.436 \tR_feature_loss scaled:\t 238192.969\n",
      "It 100\t Losses: total: 47289.000,\ttarget: 5.330 \tR_feature_loss scaled:\t 47283.629\n",
      "It 200\t Losses: total: 21777.920,\ttarget: 5.502 \tR_feature_loss scaled:\t 21772.387\n",
      "It 300\t Losses: total: 16603.678,\ttarget: 5.489 \tR_feature_loss scaled:\t 16598.160\n",
      "It 400\t Losses: total: 16120.758,\ttarget: 5.498 \tR_feature_loss scaled:\t 16115.233\n",
      "It 500\t Losses: total: 13038.271,\ttarget: 5.506 \tR_feature_loss scaled:\t 13032.739\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([6, 2, 6, 0, 2, 2, 5, 6, 4, 6, 1, 6, 6, 4, 5, 0, 4, 5, 6, 4, 4, 3, 6, 3,\n",
      "        6, 3, 1, 4, 4, 6, 1, 1, 6, 0, 1, 3, 5, 5, 2, 1, 6, 3, 6, 3, 5, 2, 1, 3,\n",
      "        4, 1, 1, 4, 4, 1, 5, 4, 1, 6, 1, 6, 2, 1, 2, 3, 2, 1, 4, 2, 4, 2, 3, 3,\n",
      "        1, 6, 6, 2, 4, 1, 2, 4, 3, 6, 6, 6, 3, 6, 2, 6, 3, 3, 2, 3, 4],\n",
      "       device='cuda:0')\n",
      "Loss: 1.650 | Acc: 69.073% (2363/3421), B. Acc: 0.685%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238339.812,\ttarget: 3.490 \tR_feature_loss scaled:\t 238336.250\n",
      "It 100\t Losses: total: 32519.168,\ttarget: 5.754 \tR_feature_loss scaled:\t 32513.375\n",
      "It 200\t Losses: total: 18230.713,\ttarget: 5.908 \tR_feature_loss scaled:\t 18224.773\n",
      "It 300\t Losses: total: 14497.467,\ttarget: 6.078 \tR_feature_loss scaled:\t 14491.361\n",
      "It 400\t Losses: total: 12813.467,\ttarget: 6.086 \tR_feature_loss scaled:\t 12807.354\n",
      "It 500\t Losses: total: 18113.961,\ttarget: 6.039 \tR_feature_loss scaled:\t 18107.896\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([1, 3, 3, 5, 3, 2, 4, 1, 2, 5, 6, 5, 5, 1, 1, 5, 2, 2, 2, 3, 0, 3, 2, 1,\n",
      "        1, 1, 6, 1, 6, 3, 3, 3, 0, 3, 1, 3, 6, 1, 3, 4, 2, 3, 1, 2, 2, 5, 2, 2,\n",
      "        2, 2, 2, 6, 2, 2, 5, 0, 6, 3, 1, 2, 6, 6, 1, 1, 6, 2, 4, 2, 4, 6, 3, 1,\n",
      "        2, 3, 3, 1, 0, 2, 0, 1, 0, 3, 1, 3, 3, 1, 0, 2, 6, 2, 4, 5, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 1.655 | Acc: 70.740% (2420/3421), B. Acc: 0.707%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238300.328,\ttarget: 3.463 \tR_feature_loss scaled:\t 238296.797\n",
      "It 100\t Losses: total: 36285.797,\ttarget: 5.501 \tR_feature_loss scaled:\t 36280.258\n",
      "It 200\t Losses: total: 21577.541,\ttarget: 5.642 \tR_feature_loss scaled:\t 21571.867\n",
      "It 300\t Losses: total: 22972.838,\ttarget: 5.562 \tR_feature_loss scaled:\t 22967.246\n",
      "It 400\t Losses: total: 14585.611,\ttarget: 6.009 \tR_feature_loss scaled:\t 14579.576\n",
      "It 500\t Losses: total: 12997.696,\ttarget: 5.860 \tR_feature_loss scaled:\t 12991.811\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([2, 6, 4, 2, 5, 1, 2, 1, 6, 3, 5, 5, 1, 6, 1, 2, 3, 2, 3, 2, 5, 4, 1, 1,\n",
      "        5, 3, 2, 3, 2, 3, 2, 3, 2, 4, 5, 6, 2, 3, 4, 3, 2, 3, 2, 5, 6, 4, 4, 1,\n",
      "        4, 0, 6, 1, 4, 4, 3, 2, 6, 6, 1, 4, 6, 2, 4, 1, 6, 6, 5, 3, 6, 1, 2, 2,\n",
      "        3, 1, 1, 1, 3, 2, 1, 2, 2, 3, 1, 6, 4, 6, 4, 2, 1, 3, 6, 2, 0],\n",
      "       device='cuda:0')\n",
      "Loss: 1.681 | Acc: 68.576% (2346/3421), B. Acc: 0.684%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238324.000,\ttarget: 3.487 \tR_feature_loss scaled:\t 238320.438\n",
      "It 100\t Losses: total: 41933.168,\ttarget: 5.081 \tR_feature_loss scaled:\t 41928.047\n",
      "It 200\t Losses: total: 15774.818,\ttarget: 5.876 \tR_feature_loss scaled:\t 15768.911\n",
      "It 300\t Losses: total: 20099.178,\ttarget: 6.005 \tR_feature_loss scaled:\t 20093.145\n",
      "It 400\t Losses: total: 14000.247,\ttarget: 6.029 \tR_feature_loss scaled:\t 13994.191\n",
      "It 500\t Losses: total: 11486.552,\ttarget: 5.887 \tR_feature_loss scaled:\t 11480.640\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([5, 6, 6, 1, 1, 6, 4, 4, 2, 1, 3, 6, 2, 2, 2, 5, 1, 5, 6, 6, 0, 1, 2, 6,\n",
      "        6, 2, 5, 2, 1, 6, 6, 1, 0, 1, 5, 6, 0, 5, 5, 2, 1, 6, 1, 4, 3, 4, 2, 1,\n",
      "        3, 1, 6, 5, 2, 5, 5, 1, 6, 6, 2, 1, 1, 6, 3, 1, 5, 6, 5, 1, 2, 2, 3, 6,\n",
      "        2, 1, 1, 6, 1, 3, 2, 2, 2, 1, 5, 1, 4, 6, 2, 6, 2, 4, 4, 4, 5],\n",
      "       device='cuda:0')\n",
      "Loss: 1.650 | Acc: 68.956% (2359/3421), B. Acc: 0.705%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238544.031,\ttarget: 3.458 \tR_feature_loss scaled:\t 238540.500\n",
      "It 100\t Losses: total: 28981.094,\ttarget: 5.500 \tR_feature_loss scaled:\t 28975.555\n",
      "It 200\t Losses: total: 18854.408,\ttarget: 5.660 \tR_feature_loss scaled:\t 18848.719\n",
      "It 300\t Losses: total: 16343.973,\ttarget: 5.822 \tR_feature_loss scaled:\t 16338.123\n",
      "It 400\t Losses: total: 17506.002,\ttarget: 5.878 \tR_feature_loss scaled:\t 17500.098\n",
      "It 500\t Losses: total: 14499.524,\ttarget: 5.822 \tR_feature_loss scaled:\t 14493.678\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([1, 5, 2, 0, 1, 5, 3, 1, 6, 1, 3, 3, 2, 1, 5, 2, 1, 6, 5, 3, 3, 1, 1, 2,\n",
      "        2, 3, 5, 2, 6, 4, 3, 5, 6, 3, 4, 1, 3, 2, 0, 6, 6, 3, 1, 6, 4, 5, 5, 3,\n",
      "        4, 1, 3, 0, 1, 3, 2, 1, 2, 1, 3, 1, 2, 1, 6, 2, 2, 6, 6, 3, 4, 1, 3, 3,\n",
      "        5, 2, 2, 1, 1, 3, 3, 1, 3, 0, 6, 1, 2, 5, 2, 2, 2, 2, 6, 5, 0],\n",
      "       device='cuda:0')\n",
      "Loss: 1.424 | Acc: 73.955% (2530/3421), B. Acc: 0.725%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238331.781,\ttarget: 3.492 \tR_feature_loss scaled:\t 238328.219\n",
      "It 100\t Losses: total: 32958.797,\ttarget: 5.415 \tR_feature_loss scaled:\t 32953.344\n",
      "It 200\t Losses: total: 19186.076,\ttarget: 6.030 \tR_feature_loss scaled:\t 19180.016\n",
      "It 300\t Losses: total: 15660.506,\ttarget: 5.753 \tR_feature_loss scaled:\t 15654.725\n",
      "It 400\t Losses: total: 12432.233,\ttarget: 5.897 \tR_feature_loss scaled:\t 12426.310\n",
      "It 500\t Losses: total: 14439.541,\ttarget: 6.086 \tR_feature_loss scaled:\t 14433.430\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([3, 3, 1, 1, 6, 5, 2, 2, 3, 2, 6, 2, 2, 5, 1, 1, 4, 2, 2, 4, 2, 1, 0, 5,\n",
      "        1, 4, 2, 5, 3, 1, 5, 5, 3, 6, 6, 3, 2, 1, 6, 3, 4, 1, 1, 1, 3, 5, 2, 3,\n",
      "        6, 2, 2, 1, 6, 3, 6, 2, 4, 1, 5, 3, 6, 6, 3, 3, 2, 2, 2, 6, 1, 6, 2, 3,\n",
      "        6, 3, 6, 2, 0, 3, 6, 6, 3, 1, 0, 6, 6, 0, 2, 2, 2, 3, 2, 4, 3],\n",
      "       device='cuda:0')\n",
      "Loss: 1.523 | Acc: 73.955% (2530/3421), B. Acc: 0.735%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238172.688,\ttarget: 3.487 \tR_feature_loss scaled:\t 238169.125\n",
      "It 100\t Losses: total: 34984.984,\ttarget: 5.600 \tR_feature_loss scaled:\t 34979.348\n",
      "It 200\t Losses: total: 23098.834,\ttarget: 6.268 \tR_feature_loss scaled:\t 23092.535\n",
      "It 300\t Losses: total: 17450.234,\ttarget: 6.401 \tR_feature_loss scaled:\t 17443.805\n",
      "It 400\t Losses: total: 16164.863,\ttarget: 6.141 \tR_feature_loss scaled:\t 16158.695\n",
      "It 500\t Losses: total: 14007.986,\ttarget: 6.620 \tR_feature_loss scaled:\t 14001.340\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([2, 3, 4, 2, 1, 3, 2, 3, 0, 6, 1, 1, 2, 6, 3, 6, 1, 1, 4, 6, 1, 1, 0, 1,\n",
      "        2, 2, 2, 2, 2, 4, 0, 5, 6, 1, 1, 6, 5, 6, 3, 2, 6, 6, 3, 3, 1, 4, 2, 1,\n",
      "        2, 2, 4, 4, 3, 6, 3, 5, 5, 3, 6, 2, 6, 0, 2, 3, 2, 1, 1, 2, 2, 2, 2, 1,\n",
      "        6, 5, 4, 5, 1, 1, 1, 5, 1, 4, 5, 5, 1, 6, 5, 1, 1, 6, 1, 2, 0],\n",
      "       device='cuda:0')\n",
      "Loss: 1.496 | Acc: 73.283% (2507/3421), B. Acc: 0.746%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238279.500,\ttarget: 3.485 \tR_feature_loss scaled:\t 238275.938\n",
      "It 100\t Losses: total: 48144.598,\ttarget: 5.222 \tR_feature_loss scaled:\t 48139.336\n",
      "It 200\t Losses: total: 19657.057,\ttarget: 5.682 \tR_feature_loss scaled:\t 19651.344\n",
      "It 300\t Losses: total: 15522.249,\ttarget: 5.762 \tR_feature_loss scaled:\t 15516.459\n",
      "It 400\t Losses: total: 15573.094,\ttarget: 6.036 \tR_feature_loss scaled:\t 15567.031\n",
      "It 500\t Losses: total: 14314.937,\ttarget: 6.004 \tR_feature_loss scaled:\t 14308.906\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([6, 4, 2, 4, 5, 2, 4, 1, 0, 5, 3, 6, 2, 4, 2, 2, 0, 0, 1, 6, 2, 2, 1, 2,\n",
      "        2, 1, 2, 5, 2, 1, 6, 2, 3, 1, 6, 4, 4, 6, 0, 6, 5, 0, 1, 4, 0, 4, 0, 2,\n",
      "        5, 6, 6, 2, 1, 2, 4, 1, 1, 2, 6, 2, 3, 3, 1, 2, 5, 5, 4, 2, 2, 3, 2, 3,\n",
      "        2, 2, 2, 1, 5, 1, 6, 2, 5, 1, 1, 3, 2, 1, 6, 6, 2, 2, 2, 5, 6],\n",
      "       device='cuda:0')\n",
      "Loss: 1.599 | Acc: 70.886% (2425/3421), B. Acc: 0.727%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238097.281,\ttarget: 3.490 \tR_feature_loss scaled:\t 238093.719\n",
      "It 100\t Losses: total: 42932.008,\ttarget: 5.676 \tR_feature_loss scaled:\t 42926.293\n",
      "It 200\t Losses: total: 25177.633,\ttarget: 6.320 \tR_feature_loss scaled:\t 25171.281\n",
      "It 300\t Losses: total: 16691.379,\ttarget: 5.995 \tR_feature_loss scaled:\t 16685.355\n",
      "It 400\t Losses: total: 15033.171,\ttarget: 6.222 \tR_feature_loss scaled:\t 15026.922\n",
      "It 500\t Losses: total: 17559.693,\ttarget: 6.359 \tR_feature_loss scaled:\t 17553.309\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([3, 2, 2, 2, 2, 1, 1, 3, 0, 1, 0, 2, 4, 5, 2, 3, 2, 1, 2, 4, 0, 3, 6, 4,\n",
      "        3, 3, 2, 2, 3, 3, 6, 6, 6, 3, 1, 2, 6, 2, 2, 5, 1, 1, 1, 6, 5, 1, 6, 6,\n",
      "        6, 5, 6, 6, 3, 2, 1, 6, 6, 4, 2, 0, 2, 3, 3, 4, 4, 5, 5, 4, 6, 2, 6, 6,\n",
      "        3, 2, 2, 2, 1, 6, 4, 2, 4, 1, 1, 3, 4, 3, 4, 3, 3, 6, 3, 2, 3],\n",
      "       device='cuda:0')\n",
      "Loss: 1.452 | Acc: 75.826% (2594/3421), B. Acc: 0.758%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238442.625,\ttarget: 3.472 \tR_feature_loss scaled:\t 238439.078\n",
      "It 100\t Losses: total: 35218.715,\ttarget: 5.253 \tR_feature_loss scaled:\t 35213.422\n",
      "It 200\t Losses: total: 28531.752,\ttarget: 5.802 \tR_feature_loss scaled:\t 28525.920\n",
      "It 300\t Losses: total: 19804.721,\ttarget: 6.042 \tR_feature_loss scaled:\t 19798.650\n",
      "It 400\t Losses: total: 14729.616,\ttarget: 5.851 \tR_feature_loss scaled:\t 14723.739\n",
      "It 500\t Losses: total: 18800.996,\ttarget: 6.143 \tR_feature_loss scaled:\t 18794.828\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([4, 3, 6, 3, 3, 6, 2, 2, 2, 3, 1, 5, 6, 3, 4, 5, 0, 6, 6, 4, 0, 3, 6, 3,\n",
      "        2, 3, 2, 6, 2, 6, 3, 5, 5, 1, 1, 1, 2, 0, 2, 2, 5, 3, 3, 2, 6, 2, 2, 5,\n",
      "        2, 1, 3, 3, 5, 5, 3, 2, 1, 5, 1, 5, 3, 6, 1, 2, 6, 1, 6, 5, 6, 4, 3, 3,\n",
      "        6, 1, 4, 6, 1, 2, 4, 3, 1, 0, 4, 6, 2, 6, 2, 1, 3, 1, 0, 6, 2],\n",
      "       device='cuda:0')\n",
      "Loss: 1.463 | Acc: 75.533% (2584/3421), B. Acc: 0.759%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238139.078,\ttarget: 3.471 \tR_feature_loss scaled:\t 238135.531\n",
      "It 100\t Losses: total: 35854.336,\ttarget: 5.251 \tR_feature_loss scaled:\t 35849.047\n",
      "It 200\t Losses: total: 20852.805,\ttarget: 6.066 \tR_feature_loss scaled:\t 20846.707\n",
      "It 300\t Losses: total: 12588.438,\ttarget: 5.967 \tR_feature_loss scaled:\t 12582.443\n",
      "It 400\t Losses: total: 17145.930,\ttarget: 6.001 \tR_feature_loss scaled:\t 17139.902\n",
      "It 500\t Losses: total: 11344.417,\ttarget: 5.936 \tR_feature_loss scaled:\t 11338.455\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([6, 3, 4, 6, 6, 3, 6, 2, 3, 3, 3, 5, 2, 2, 1, 3, 2, 5, 6, 1, 5, 6, 3, 4,\n",
      "        5, 3, 2, 2, 5, 2, 6, 1, 5, 6, 1, 2, 6, 2, 1, 4, 5, 1, 4, 5, 2, 3, 4, 2,\n",
      "        1, 2, 1, 3, 5, 4, 2, 2, 2, 1, 3, 0, 4, 3, 1, 6, 5, 5, 2, 4, 2, 6, 1, 6,\n",
      "        6, 3, 5, 6, 1, 6, 4, 6, 1, 4, 6, 6, 3, 3, 6, 2, 1, 0, 5, 5, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 1.484 | Acc: 73.897% (2528/3421), B. Acc: 0.748%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238115.703,\ttarget: 3.419 \tR_feature_loss scaled:\t 238112.203\n",
      "It 100\t Losses: total: 35482.664,\ttarget: 5.325 \tR_feature_loss scaled:\t 35477.301\n",
      "It 200\t Losses: total: 22511.062,\ttarget: 5.287 \tR_feature_loss scaled:\t 22505.744\n",
      "It 300\t Losses: total: 13026.993,\ttarget: 5.715 \tR_feature_loss scaled:\t 13021.250\n",
      "It 400\t Losses: total: 19873.449,\ttarget: 5.881 \tR_feature_loss scaled:\t 19867.541\n",
      "It 500\t Losses: total: 11177.574,\ttarget: 5.945 \tR_feature_loss scaled:\t 11171.604\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([6, 3, 1, 1, 1, 2, 6, 5, 1, 2, 5, 4, 4, 1, 2, 2, 2, 6, 6, 0, 2, 6, 1, 6,\n",
      "        3, 1, 4, 3, 5, 6, 5, 5, 3, 2, 5, 2, 6, 2, 2, 6, 5, 1, 1, 1, 6, 2, 1, 6,\n",
      "        3, 4, 5, 1, 5, 2, 6, 5, 5, 5, 2, 3, 2, 1, 1, 2, 1, 3, 0, 1, 2, 1, 6, 2,\n",
      "        4, 2, 1, 5, 2, 1, 1, 4, 4, 0, 1, 1, 2, 2, 2, 3, 3, 2, 5, 3, 3],\n",
      "       device='cuda:0')\n",
      "Loss: 1.459 | Acc: 75.621% (2587/3421), B. Acc: 0.764%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238304.703,\ttarget: 3.459 \tR_feature_loss scaled:\t 238301.172\n",
      "It 100\t Losses: total: 32859.199,\ttarget: 5.520 \tR_feature_loss scaled:\t 32853.641\n",
      "It 200\t Losses: total: 18460.959,\ttarget: 5.828 \tR_feature_loss scaled:\t 18455.100\n",
      "It 300\t Losses: total: 13927.121,\ttarget: 5.917 \tR_feature_loss scaled:\t 13921.176\n",
      "It 400\t Losses: total: 21076.799,\ttarget: 5.767 \tR_feature_loss scaled:\t 21071.006\n",
      "It 500\t Losses: total: 11949.766,\ttarget: 5.956 \tR_feature_loss scaled:\t 11943.784\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([3, 1, 5, 1, 3, 6, 3, 1, 1, 4, 2, 5, 0, 2, 3, 6, 6, 2, 5, 4, 1, 6, 6, 5,\n",
      "        3, 5, 5, 3, 1, 4, 5, 2, 1, 1, 5, 1, 2, 2, 6, 6, 0, 1, 1, 2, 5, 0, 6, 5,\n",
      "        5, 6, 6, 6, 3, 3, 4, 2, 6, 3, 2, 5, 3, 1, 6, 1, 2, 4, 6, 0, 5, 5, 3, 5,\n",
      "        6, 3, 2, 4, 3, 5, 5, 1, 5, 2, 6, 6, 2, 4, 5, 6, 1, 6, 6, 2, 3],\n",
      "       device='cuda:0')\n",
      "Loss: 1.498 | Acc: 73.926% (2529/3421), B. Acc: 0.755%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238355.984,\ttarget: 3.483 \tR_feature_loss scaled:\t 238352.422\n",
      "It 100\t Losses: total: 34326.914,\ttarget: 5.360 \tR_feature_loss scaled:\t 34321.516\n",
      "It 200\t Losses: total: 22036.551,\ttarget: 5.946 \tR_feature_loss scaled:\t 22030.574\n",
      "It 300\t Losses: total: 22577.139,\ttarget: 5.915 \tR_feature_loss scaled:\t 22571.195\n",
      "It 400\t Losses: total: 15005.905,\ttarget: 5.982 \tR_feature_loss scaled:\t 14999.896\n",
      "It 500\t Losses: total: 16891.992,\ttarget: 5.847 \tR_feature_loss scaled:\t 16886.119\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([2, 3, 5, 2, 2, 6, 1, 2, 3, 2, 6, 2, 2, 2, 6, 4, 1, 1, 3, 4, 6, 2, 2, 5,\n",
      "        2, 1, 2, 3, 5, 6, 0, 5, 1, 3, 6, 6, 3, 1, 1, 1, 3, 1, 1, 6, 3, 1, 1, 1,\n",
      "        1, 1, 6, 5, 2, 5, 6, 6, 5, 5, 4, 2, 3, 5, 2, 6, 1, 4, 3, 2, 0, 5, 2, 1,\n",
      "        1, 2, 1, 5, 4, 6, 2, 2, 1, 5, 6, 3, 2, 1, 6, 1, 2, 3, 6, 5, 5],\n",
      "       device='cuda:0')\n",
      "Loss: 1.498 | Acc: 74.481% (2548/3421), B. Acc: 0.754%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238364.469,\ttarget: 3.441 \tR_feature_loss scaled:\t 238360.953\n",
      "It 100\t Losses: total: 35749.719,\ttarget: 5.669 \tR_feature_loss scaled:\t 35744.012\n",
      "It 200\t Losses: total: 18664.637,\ttarget: 6.099 \tR_feature_loss scaled:\t 18658.506\n",
      "It 300\t Losses: total: 14922.833,\ttarget: 6.208 \tR_feature_loss scaled:\t 14916.597\n",
      "It 400\t Losses: total: 14241.050,\ttarget: 5.847 \tR_feature_loss scaled:\t 14235.176\n",
      "It 500\t Losses: total: 16248.570,\ttarget: 6.009 \tR_feature_loss scaled:\t 16242.535\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([5, 4, 1, 6, 0, 5, 0, 1, 2, 5, 2, 2, 0, 5, 5, 6, 5, 3, 5, 2, 5, 2, 1, 2,\n",
      "        2, 3, 1, 5, 3, 6, 0, 2, 1, 1, 3, 5, 2, 1, 0, 5, 1, 5, 3, 6, 3, 4, 2, 6,\n",
      "        3, 0, 6, 1, 0, 6, 6, 1, 4, 3, 3, 4, 2, 6, 6, 3, 6, 0, 1, 2, 0, 3, 2, 6,\n",
      "        6, 2, 1, 2, 3, 6, 6, 2, 5, 2, 2, 1, 1, 2, 6, 2, 3, 4, 5, 6, 5],\n",
      "       device='cuda:0')\n",
      "Loss: 1.451 | Acc: 75.124% (2570/3421), B. Acc: 0.762%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238451.562,\ttarget: 3.433 \tR_feature_loss scaled:\t 238448.047\n",
      "It 100\t Losses: total: 34157.285,\ttarget: 5.781 \tR_feature_loss scaled:\t 34151.465\n",
      "It 200\t Losses: total: 20021.480,\ttarget: 5.803 \tR_feature_loss scaled:\t 20015.646\n",
      "It 300\t Losses: total: 14335.321,\ttarget: 6.010 \tR_feature_loss scaled:\t 14329.283\n",
      "It 400\t Losses: total: 13636.515,\ttarget: 5.905 \tR_feature_loss scaled:\t 13630.584\n",
      "It 500\t Losses: total: 14416.129,\ttarget: 6.073 \tR_feature_loss scaled:\t 14410.030\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([3, 5, 1, 5, 1, 3, 2, 2, 4, 4, 5, 6, 3, 6, 1, 1, 1, 2, 5, 2, 6, 6, 3, 1,\n",
      "        6, 1, 1, 1, 5, 4, 1, 2, 2, 2, 0, 4, 6, 5, 6, 6, 6, 2, 6, 5, 1, 5, 2, 2,\n",
      "        6, 6, 3, 2, 0, 0, 2, 4, 4, 1, 1, 4, 4, 2, 2, 3, 4, 6, 6, 0, 1, 1, 5, 2,\n",
      "        5, 6, 2, 6, 4, 1, 1, 1, 4, 2, 0, 1, 2, 6, 4, 6, 1, 2, 3, 2, 5],\n",
      "       device='cuda:0')\n",
      "Loss: 1.440 | Acc: 75.475% (2582/3421), B. Acc: 0.765%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238273.688,\ttarget: 3.451 \tR_feature_loss scaled:\t 238270.156\n",
      "It 100\t Losses: total: 33887.543,\ttarget: 5.409 \tR_feature_loss scaled:\t 33882.094\n",
      "It 200\t Losses: total: 26124.084,\ttarget: 5.410 \tR_feature_loss scaled:\t 26118.643\n",
      "It 300\t Losses: total: 21166.908,\ttarget: 5.134 \tR_feature_loss scaled:\t 21161.746\n",
      "It 400\t Losses: total: 15430.113,\ttarget: 5.234 \tR_feature_loss scaled:\t 15424.854\n",
      "It 500\t Losses: total: 14388.874,\ttarget: 5.247 \tR_feature_loss scaled:\t 14383.602\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([6, 4, 6, 6, 4, 2, 4, 0, 5, 4, 6, 3, 3, 1, 6, 2, 3, 5, 0, 6, 2, 6, 0, 6,\n",
      "        0, 6, 5, 1, 3, 6, 1, 1, 0, 5, 5, 1, 1, 3, 2, 2, 6, 3, 2, 4, 2, 6, 1, 2,\n",
      "        3, 3, 2, 4, 3, 6, 3, 5, 3, 5, 1, 3, 2, 2, 3, 1, 2, 2, 3, 3, 1, 2, 1, 6,\n",
      "        4, 1, 5, 2, 0, 4, 1, 3, 6, 1, 6, 4, 3, 3, 0, 1, 3, 1, 2, 1, 2],\n",
      "       device='cuda:0')\n",
      "Loss: 1.469 | Acc: 74.744% (2557/3421), B. Acc: 0.753%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 238342.188,\ttarget: 3.391 \tR_feature_loss scaled:\t 238338.719\n",
      "It 100\t Losses: total: 33218.254,\ttarget: 6.265 \tR_feature_loss scaled:\t 33211.949\n",
      "It 200\t Losses: total: 23473.840,\ttarget: 6.428 \tR_feature_loss scaled:\t 23467.381\n",
      "It 300\t Losses: total: 17255.439,\ttarget: 6.601 \tR_feature_loss scaled:\t 17248.811\n",
      "It 400\t Losses: total: 17281.371,\ttarget: 6.597 \tR_feature_loss scaled:\t 17274.748\n",
      "It 500\t Losses: total: 11797.191,\ttarget: 6.600 \tR_feature_loss scaled:\t 11790.566\n",
      "Training classifier...\n",
      "Num images\n",
      "500\n",
      "Num targets\n",
      "torch.Size([128])\n",
      "Test classifier\n",
      "tensor([2, 0, 1, 6, 5, 6, 2, 1, 2, 2, 3, 3, 1, 5, 1, 2, 1, 4, 2, 1, 2, 6, 6, 2,\n",
      "        3, 1, 6, 6, 5, 6, 1, 2, 5, 1, 4, 0, 6, 5, 1, 0, 2, 2, 5, 2, 6, 4, 5, 3,\n",
      "        6, 1, 3, 6, 6, 2, 4, 5, 0, 1, 2, 3, 3, 2, 4, 4, 3, 5, 2, 4, 1, 2, 0, 2,\n",
      "        3, 6, 5, 2, 2, 5, 6, 2, 6, 4, 6, 4, 2, 3, 5, 2, 2, 6, 5, 4, 2],\n",
      "       device='cuda:0')\n",
      "Loss: 1.488 | Acc: 74.540% (2550/3421), B. Acc: 0.757%\n",
      "Test ensemble...\n",
      "tensor([1, 6, 2, 6, 1, 2, 6, 5, 3, 3, 4, 2, 3, 1, 2, 6, 6, 2, 6, 6, 2, 2, 5, 6,\n",
      "        4, 3, 0, 6, 0, 0, 6, 6, 2, 0, 2, 0, 1, 2, 6, 2, 2, 5, 0, 0, 1, 1, 6, 0,\n",
      "        6, 1, 2, 4, 6, 2, 1, 3, 1, 2, 6, 6, 3, 2, 2, 6, 1, 3, 1, 6, 3, 1, 3, 2,\n",
      "        2, 6, 4, 1, 1, 6, 3, 2, 0, 2, 2, 1, 4, 6, 2, 2, 1, 6, 3, 2, 6],\n",
      "       device='cuda:0')\n",
      "Loss: 1.422 | Acc: 79.801% (2730/3421), B. Acc: 0.809%\n",
      "Test classifier\n",
      "tensor([6, 2, 5, 5, 6, 6, 0, 4, 0, 5, 6, 2, 0, 3, 1, 2, 3, 4, 6, 6, 6, 0, 6, 6,\n",
      "        1, 4, 4, 3, 6, 1, 1, 2, 6, 2, 6, 6, 6, 6, 3, 4, 6, 2, 6, 2, 3, 5, 2, 1,\n",
      "        4, 1, 2, 2, 2, 5, 3, 6, 1, 3, 2, 5, 1, 1, 1, 2, 5, 2, 2, 1, 2, 6, 3, 1,\n",
      "        0, 3, 4, 1, 1, 1, 1, 2, 2, 3, 2, 5, 3, 2, 1, 5, 5, 5, 3, 2, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 1.486 | Acc: 74.540% (2550/3421), B. Acc: 0.757%\n"
     ]
    }
   ],
   "source": [
    "fed_model, noise_adapt = FedBlood.model_inversion(ens_local, large_jtr=5, small_jtr=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2812,
     "status": "ok",
     "timestamp": 1734443807272,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "2mUHRwbJ05XH",
    "outputId": "603c2d0f-0fed-4d80-c377-5d217f158233"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-e8709b84d129>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fed_model = torch.load(\"./blood_exper_dirich/federated/glb_model_last.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 5, 4, 5, 3, 5, 2, 5, 2, 3, 2, 6, 5, 2, 1, 6, 5, 3, 6, 1, 6, 2, 1, 1,\n",
      "        0, 1, 0, 6, 6, 4, 5, 1, 6, 6, 1, 2, 4, 1, 2, 4, 2, 2, 2, 1, 1, 3, 5, 3,\n",
      "        6, 1, 6, 3, 0, 2, 1, 2, 3, 2, 4, 6, 6, 6, 6, 1, 5, 4, 2, 0, 1, 2, 5, 1,\n",
      "        2, 6, 2, 1, 1, 1, 2, 3, 2, 3, 5, 6, 5, 5, 5, 4, 6, 3, 3, 1, 2],\n",
      "       device='cuda:0')\n",
      "Loss: 1.482 | Acc: 74.540% (2550/3421), B. Acc: 0.757%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40.02048659324646, 0.7453960830166618, 0.7572539449632514)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "fed_model = torch.load(\"./blood_exper_dirich/federated/glb_model_last.pth\")\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "test_data = DataClass(split='test', root=\"./\", transform=test_transforms, download=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "FedBlood.test_model(fed_model, test_loader, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1876,
     "status": "ok",
     "timestamp": 1734443856554,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "WCni47l29Q4b",
    "outputId": "d7145101-8981-4c35-b552-e8546f785d05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-d8dc6215a129>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  client_0_weights = torch.load(\"./blood_models_dirich/client_4/best.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 3, 4, 3, 3, 3, 2, 3, 2, 3, 2, 6, 5, 3, 1, 6, 3, 3, 6, 1, 6, 2, 1, 1,\n",
      "        3, 1, 0, 6, 6, 4, 3, 1, 6, 6, 1, 2, 4, 1, 2, 4, 2, 2, 2, 1, 1, 6, 5, 3,\n",
      "        6, 1, 6, 3, 4, 2, 1, 6, 3, 2, 4, 6, 6, 6, 6, 1, 3, 4, 2, 0, 1, 2, 3, 1,\n",
      "        2, 6, 6, 1, 1, 1, 2, 3, 2, 3, 3, 6, 3, 3, 3, 4, 6, 3, 3, 1, 2],\n",
      "       device='cuda:0')\n",
      "Loss: 2.515 | Acc: 74.920% (2563/3421), B. Acc: 0.718%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(67.91163086891174, 0.7491961414790996, 0.7178466353954271)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_0_weights = torch.load(\"./blood_models_dirich/client_4/best.pth\")\n",
    "client_0_model = ResNet18(in_channels=3, num_classes=8)\n",
    "client_0_model.load_state_dict(client_0_weights)\n",
    "client_0_model.to('cuda')\n",
    "FedBlood.test_model(client_0_model, test_loader, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OvTDu1I9nCM"
   },
   "source": [
    "## Pneumonia dirich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1734458213120,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "ffDS1pSc9jJJ",
    "outputId": "879b7e19-637e-4afc-c879-472fb08f16d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./pneumoniamnist.npz\n",
      "Using downloaded and verified file: ./pneumoniamnist.npz\n",
      "Using downloaded and verified file: ./pneumoniamnist.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset PneumoniaMNIST of size 28 (pneumoniamnist)\n",
       "    Number of datapoints: 4708\n",
       "    Root location: ./\n",
       "    Split: train\n",
       "    Task: binary-class\n",
       "    Number of channels: 1\n",
       "    Meaning of labels: {'0': 'normal', '1': 'pneumonia'}\n",
       "    Number of samples: {'train': 4708, 'val': 524, 'test': 624}\n",
       "    Description: The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.\n",
       "    License: CC BY 4.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_flag = 'pneumoniamnist'\n",
    "download = True\n",
    "\n",
    "info = medmnist.INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "glb_train_dataset = DataClass(root=\"./\", split='train', download=download)\n",
    "glb_val_dataset = DataClass(root=\"./\", split='val', download=download)\n",
    "glb_test_dataset = DataClass(root=\"./\", split='test', download=download)\n",
    "info[\"n_samples\"][\"train\"] = glb_train_dataset.imgs.shape[0]\n",
    "info[\"n_samples\"][\"val\"] = glb_val_dataset.imgs.shape[0]\n",
    "info[\"n_samples\"][\"test\"] = glb_test_dataset.imgs.shape[0]\n",
    "glb_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1734458216304,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "VmUN6PjE9jJJ"
   },
   "outputs": [],
   "source": [
    "num_clients = 5\n",
    "num_classes = 2\n",
    "part_dir, list_name, split_ids = utils.fl_partition(glb_train_dataset, data_flag, num_clients, num_classes, iid=False, beta=0.6, val_split = 0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1734458218956,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "4f_5Wpn79jJJ",
    "outputId": "a3fcfeda-e1fe-404d-e1d6-f720996ca4f6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5), sharey=True)\n",
    "\n",
    "for list_ind, ax in enumerate(axes):\n",
    "    f_name = list_name[list_ind]\n",
    "    data = np.load(f_name)\n",
    "    labels = data['train_labels'].flatten().tolist()\n",
    "    label_counts = Counter(labels)\n",
    "    categories = list(label_counts.keys())\n",
    "    counts = list(label_counts.values())\n",
    "\n",
    "    # Plot on the current subplot\n",
    "    ax.bar(categories, counts, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f\"Dataset {list_ind + 1}\", fontsize=14)\n",
    "    ax.set_xlabel('Labels', fontsize=12)\n",
    "    ax.set_xticks(ticks=categories)\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "fig.supylabel('Count', fontsize=14)\n",
    "fig.suptitle('Sample label distribution across hosptials', fontsize=16)\n",
    "fig.subplots_adjust(left=0.05, right=0.95, top=0.85, bottom=0.1, wspace=0.3)\n",
    "#plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 262470,
     "status": "ok",
     "timestamp": 1734458487412,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "FwNWz9bJ9jJK",
    "outputId": "7c2e3594-697b-4328-86ea-f886c56f8c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch num: 1 \n",
      "Train loss: 0.031312 \n",
      "Val loss: 0.003676 \n",
      "Val acc: 0.901754\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 2 \n",
      "Train loss: 0.019240 \n",
      "Val loss: 0.009736 \n",
      "Val acc: 0.901754\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 3 \n",
      "Train loss: 0.009629 \n",
      "Val loss: 0.010799 \n",
      "Val acc: 0.901754\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 4 \n",
      "Train loss: 0.009805 \n",
      "Val loss: 0.003073 \n",
      "Val acc: 0.919298\n",
      "Val balanced acc: 0.589286\n",
      "Epoch num: 5 \n",
      "Train loss: 0.007262 \n",
      "Val loss: 0.000540 \n",
      "Val acc: 0.968421\n",
      "Val balanced acc: 0.902932\n",
      "Epoch num: 6 \n",
      "Train loss: 0.006611 \n",
      "Val loss: 0.002214 \n",
      "Val acc: 0.929825\n",
      "Val balanced acc: 0.642857\n",
      "Epoch num: 7 \n",
      "Train loss: 0.007374 \n",
      "Val loss: 0.003549 \n",
      "Val acc: 0.915789\n",
      "Val balanced acc: 0.571429\n",
      "Epoch num: 8 \n",
      "Train loss: 0.007490 \n",
      "Val loss: 0.000601 \n",
      "Val acc: 0.971930\n",
      "Val balanced acc: 0.873054\n",
      "Epoch num: 9 \n",
      "Train loss: 0.006233 \n",
      "Val loss: 0.001256 \n",
      "Val acc: 0.943860\n",
      "Val balanced acc: 0.937048\n",
      "Epoch num: 10 \n",
      "Train loss: 0.006137 \n",
      "Val loss: 0.000822 \n",
      "Val acc: 0.961404\n",
      "Val balanced acc: 0.962688\n",
      "Epoch num: 11 \n",
      "Train loss: 0.005862 \n",
      "Val loss: 0.000950 \n",
      "Val acc: 0.961404\n",
      "Val balanced acc: 0.803571\n",
      "Epoch num: 12 \n",
      "Train loss: 0.005640 \n",
      "Val loss: 0.002649 \n",
      "Val acc: 0.866667\n",
      "Val balanced acc: 0.926070\n",
      "Epoch num: 13 \n",
      "Train loss: 0.005060 \n",
      "Val loss: 0.000548 \n",
      "Val acc: 0.978947\n",
      "Val balanced acc: 0.924680\n",
      "Epoch num: 14 \n",
      "Train loss: 0.004623 \n",
      "Val loss: 0.008150 \n",
      "Val acc: 0.757895\n",
      "Val balanced acc: 0.865759\n",
      "Epoch num: 15 \n",
      "Train loss: 0.004580 \n",
      "Val loss: 0.008372 \n",
      "Val acc: 0.701754\n",
      "Val balanced acc: 0.834630\n",
      "Epoch num: 16 \n",
      "Train loss: 0.004919 \n",
      "Val loss: 0.004726 \n",
      "Val acc: 0.905263\n",
      "Val balanced acc: 0.517857\n",
      "Epoch num: 17 \n",
      "Train loss: 0.003487 \n",
      "Val loss: 0.000493 \n",
      "Val acc: 0.982456\n",
      "Val balanced acc: 0.910714\n",
      "Epoch num: 18 \n",
      "Train loss: 0.003458 \n",
      "Val loss: 0.000498 \n",
      "Val acc: 0.982456\n",
      "Val balanced acc: 0.974361\n",
      "Epoch num: 19 \n",
      "Train loss: 0.003053 \n",
      "Val loss: 0.000412 \n",
      "Val acc: 0.989474\n",
      "Val balanced acc: 0.978252\n",
      "Epoch num: 20 \n",
      "Train loss: 0.002770 \n",
      "Val loss: 0.000493 \n",
      "Val acc: 0.982456\n",
      "Val balanced acc: 0.926626\n",
      "Epoch num: 21 \n",
      "Train loss: 0.003273 \n",
      "Val loss: 0.000464 \n",
      "Val acc: 0.982456\n",
      "Val balanced acc: 0.926626\n",
      "Epoch num: 22 \n",
      "Train loss: 0.002327 \n",
      "Val loss: 0.000394 \n",
      "Val acc: 0.989474\n",
      "Val balanced acc: 0.962340\n",
      "Epoch num: 23 \n",
      "Train loss: 0.002725 \n",
      "Val loss: 0.000420 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.944483\n",
      "Epoch num: 24 \n",
      "Train loss: 0.002531 \n",
      "Val loss: 0.000442 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.960395\n",
      "Epoch num: 25 \n",
      "Train loss: 0.002234 \n",
      "Val loss: 0.000474 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.976306\n",
      "Epoch num: 26 \n",
      "Train loss: 0.002716 \n",
      "Val loss: 0.000484 \n",
      "Val acc: 0.982456\n",
      "Val balanced acc: 0.958449\n",
      "Epoch num: 27 \n",
      "Train loss: 0.002832 \n",
      "Val loss: 0.000405 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.944483\n",
      "Epoch num: 28 \n",
      "Train loss: 0.002396 \n",
      "Val loss: 0.000474 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.944483\n",
      "Epoch num: 29 \n",
      "Train loss: 0.002240 \n",
      "Val loss: 0.000384 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.960395\n",
      "Epoch num: 30 \n",
      "Train loss: 0.002138 \n",
      "Val loss: 0.000407 \n",
      "Val acc: 0.989474\n",
      "Val balanced acc: 0.978252\n",
      "Epoch num: 31 \n",
      "Train loss: 0.001983 \n",
      "Val loss: 0.000560 \n",
      "Val acc: 0.982456\n",
      "Val balanced acc: 0.910714\n",
      "Epoch num: 32 \n",
      "Train loss: 0.002645 \n",
      "Val loss: 0.000415 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.944483\n",
      "Epoch num: 33 \n",
      "Train loss: 0.001596 \n",
      "Val loss: 0.000337 \n",
      "Val acc: 0.992982\n",
      "Val balanced acc: 0.980197\n",
      "Epoch num: 34 \n",
      "Train loss: 0.001690 \n",
      "Val loss: 0.000398 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.928571\n",
      "Epoch num: 35 \n",
      "Train loss: 0.001947 \n",
      "Val loss: 0.000439 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.928571\n",
      "Epoch num: 36 \n",
      "Train loss: 0.001714 \n",
      "Val loss: 0.000429 \n",
      "Val acc: 0.982456\n",
      "Val balanced acc: 0.910714\n",
      "Epoch num: 37 \n",
      "Train loss: 0.001919 \n",
      "Val loss: 0.000320 \n",
      "Val acc: 0.996491\n",
      "Val balanced acc: 0.982143\n",
      "Epoch num: 38 \n",
      "Train loss: 0.001801 \n",
      "Val loss: 0.000422 \n",
      "Val acc: 0.982456\n",
      "Val balanced acc: 0.942538\n",
      "Epoch num: 39 \n",
      "Train loss: 0.001507 \n",
      "Val loss: 0.000680 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.976306\n",
      "Epoch num: 40 \n",
      "Train loss: 0.001357 \n",
      "Val loss: 0.000409 \n",
      "Val acc: 0.989474\n",
      "Val balanced acc: 0.978252\n",
      "Epoch num: 41 \n",
      "Train loss: 0.001485 \n",
      "Val loss: 0.000798 \n",
      "Val acc: 0.975439\n",
      "Val balanced acc: 0.875000\n",
      "Epoch num: 42 \n",
      "Train loss: 0.001413 \n",
      "Val loss: 0.000427 \n",
      "Val acc: 0.982456\n",
      "Val balanced acc: 0.910714\n",
      "Epoch num: 43 \n",
      "Train loss: 0.001452 \n",
      "Val loss: 0.000394 \n",
      "Val acc: 0.989474\n",
      "Val balanced acc: 0.946429\n",
      "Epoch num: 44 \n",
      "Train loss: 0.001157 \n",
      "Val loss: 0.000613 \n",
      "Val acc: 0.982456\n",
      "Val balanced acc: 0.910714\n",
      "Epoch num: 45 \n",
      "Train loss: 0.001323 \n",
      "Val loss: 0.000618 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.976306\n",
      "Epoch num: 46 \n",
      "Train loss: 0.001574 \n",
      "Val loss: 0.000467 \n",
      "Val acc: 0.989474\n",
      "Val balanced acc: 0.978252\n",
      "Epoch num: 47 \n",
      "Train loss: 0.001544 \n",
      "Val loss: 0.000502 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.976306\n",
      "Epoch num: 48 \n",
      "Train loss: 0.001283 \n",
      "Val loss: 0.000823 \n",
      "Val acc: 0.978947\n",
      "Val balanced acc: 0.892857\n",
      "Epoch num: 49 \n",
      "Train loss: 0.001780 \n",
      "Val loss: 0.000583 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.928571\n",
      "Epoch num: 50 \n",
      "Train loss: 0.001555 \n",
      "Val loss: 0.000479 \n",
      "Val acc: 0.985965\n",
      "Val balanced acc: 0.944483\n",
      "All done!\n",
      "Epoch num: 1 \n",
      "Train loss: 0.058910 \n",
      "Val loss: 0.006239 \n",
      "Val acc: 0.394958\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 2 \n",
      "Train loss: 0.022817 \n",
      "Val loss: 0.033035 \n",
      "Val acc: 0.579832\n",
      "Val balanced acc: 0.482861\n",
      "Epoch num: 3 \n",
      "Train loss: 0.014642 \n",
      "Val loss: 0.028501 \n",
      "Val acc: 0.588235\n",
      "Val balanced acc: 0.486111\n",
      "Epoch num: 4 \n",
      "Train loss: 0.010500 \n",
      "Val loss: 0.028328 \n",
      "Val acc: 0.605042\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 5 \n",
      "Train loss: 0.009167 \n",
      "Val loss: 0.029300 \n",
      "Val acc: 0.605042\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 6 \n",
      "Train loss: 0.007396 \n",
      "Val loss: 0.033721 \n",
      "Val acc: 0.605042\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 7 \n",
      "Train loss: 0.007718 \n",
      "Val loss: 0.025928 \n",
      "Val acc: 0.605042\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 8 \n",
      "Train loss: 0.007969 \n",
      "Val loss: 0.024391 \n",
      "Val acc: 0.621849\n",
      "Val balanced acc: 0.521277\n",
      "Epoch num: 9 \n",
      "Train loss: 0.008240 \n",
      "Val loss: 0.020422 \n",
      "Val acc: 0.655462\n",
      "Val balanced acc: 0.563830\n",
      "Epoch num: 10 \n",
      "Train loss: 0.006362 \n",
      "Val loss: 0.013455 \n",
      "Val acc: 0.647059\n",
      "Val balanced acc: 0.553191\n",
      "Epoch num: 11 \n",
      "Train loss: 0.005861 \n",
      "Val loss: 0.003343 \n",
      "Val acc: 0.823529\n",
      "Val balanced acc: 0.787677\n",
      "Epoch num: 12 \n",
      "Train loss: 0.006039 \n",
      "Val loss: 0.002138 \n",
      "Val acc: 0.924370\n",
      "Val balanced acc: 0.930112\n",
      "Epoch num: 13 \n",
      "Train loss: 0.005090 \n",
      "Val loss: 0.002671 \n",
      "Val acc: 0.882353\n",
      "Val balanced acc: 0.895390\n",
      "Epoch num: 14 \n",
      "Train loss: 0.004904 \n",
      "Val loss: 0.004705 \n",
      "Val acc: 0.815126\n",
      "Val balanced acc: 0.836141\n",
      "Epoch num: 15 \n",
      "Train loss: 0.004397 \n",
      "Val loss: 0.005030 \n",
      "Val acc: 0.781513\n",
      "Val balanced acc: 0.727098\n",
      "Epoch num: 16 \n",
      "Train loss: 0.004531 \n",
      "Val loss: 0.003220 \n",
      "Val acc: 0.865546\n",
      "Val balanced acc: 0.877807\n",
      "Epoch num: 17 \n",
      "Train loss: 0.004404 \n",
      "Val loss: 0.002648 \n",
      "Val acc: 0.915966\n",
      "Val balanced acc: 0.901005\n",
      "Epoch num: 18 \n",
      "Train loss: 0.004852 \n",
      "Val loss: 0.002498 \n",
      "Val acc: 0.890756\n",
      "Val balanced acc: 0.902335\n",
      "Epoch num: 19 \n",
      "Train loss: 0.003910 \n",
      "Val loss: 0.004529 \n",
      "Val acc: 0.831933\n",
      "Val balanced acc: 0.850030\n",
      "Epoch num: 20 \n",
      "Train loss: 0.003854 \n",
      "Val loss: 0.002886 \n",
      "Val acc: 0.865546\n",
      "Val balanced acc: 0.885195\n",
      "Epoch num: 21 \n",
      "Train loss: 0.003970 \n",
      "Val loss: 0.002244 \n",
      "Val acc: 0.890756\n",
      "Val balanced acc: 0.902335\n",
      "Epoch num: 22 \n",
      "Train loss: 0.003849 \n",
      "Val loss: 0.002622 \n",
      "Val acc: 0.915966\n",
      "Val balanced acc: 0.904699\n",
      "Epoch num: 23 \n",
      "Train loss: 0.003741 \n",
      "Val loss: 0.002564 \n",
      "Val acc: 0.899160\n",
      "Val balanced acc: 0.905585\n",
      "Epoch num: 24 \n",
      "Train loss: 0.003523 \n",
      "Val loss: 0.002492 \n",
      "Val acc: 0.907563\n",
      "Val balanced acc: 0.916223\n",
      "Epoch num: 25 \n",
      "Train loss: 0.003385 \n",
      "Val loss: 0.002395 \n",
      "Val acc: 0.907563\n",
      "Val balanced acc: 0.916223\n",
      "Epoch num: 26 \n",
      "Train loss: 0.002622 \n",
      "Val loss: 0.002369 \n",
      "Val acc: 0.890756\n",
      "Val balanced acc: 0.902335\n",
      "Epoch num: 27 \n",
      "Train loss: 0.002988 \n",
      "Val loss: 0.002491 \n",
      "Val acc: 0.899160\n",
      "Val balanced acc: 0.912973\n",
      "Epoch num: 28 \n",
      "Train loss: 0.003313 \n",
      "Val loss: 0.002416 \n",
      "Val acc: 0.899160\n",
      "Val balanced acc: 0.912973\n",
      "Epoch num: 29 \n",
      "Train loss: 0.002730 \n",
      "Val loss: 0.002545 \n",
      "Val acc: 0.890756\n",
      "Val balanced acc: 0.906028\n",
      "Epoch num: 30 \n",
      "Train loss: 0.002706 \n",
      "Val loss: 0.002750 \n",
      "Val acc: 0.890756\n",
      "Val balanced acc: 0.906028\n",
      "Epoch num: 31 \n",
      "Train loss: 0.002258 \n",
      "Val loss: 0.002575 \n",
      "Val acc: 0.890756\n",
      "Val balanced acc: 0.906028\n",
      "Epoch num: 32 \n",
      "Train loss: 0.002622 \n",
      "Val loss: 0.002218 \n",
      "Val acc: 0.907563\n",
      "Val balanced acc: 0.919917\n",
      "Epoch num: 33 \n",
      "Train loss: 0.002988 \n",
      "Val loss: 0.002140 \n",
      "Val acc: 0.915966\n",
      "Val balanced acc: 0.926862\n",
      "Epoch num: 34 \n",
      "Train loss: 0.002904 \n",
      "Val loss: 0.002273 \n",
      "Val acc: 0.907563\n",
      "Val balanced acc: 0.919917\n",
      "Epoch num: 35 \n",
      "Train loss: 0.002800 \n",
      "Val loss: 0.002180 \n",
      "Val acc: 0.907563\n",
      "Val balanced acc: 0.916223\n",
      "Epoch num: 36 \n",
      "Train loss: 0.002654 \n",
      "Val loss: 0.002153 \n",
      "Val acc: 0.907563\n",
      "Val balanced acc: 0.916223\n",
      "Epoch num: 37 \n",
      "Train loss: 0.003140 \n",
      "Val loss: 0.002123 \n",
      "Val acc: 0.915966\n",
      "Val balanced acc: 0.923168\n",
      "Epoch num: 38 \n",
      "Train loss: 0.002305 \n",
      "Val loss: 0.002110 \n",
      "Val acc: 0.915966\n",
      "Val balanced acc: 0.923168\n",
      "Epoch num: 39 \n",
      "Train loss: 0.002465 \n",
      "Val loss: 0.002104 \n",
      "Val acc: 0.915966\n",
      "Val balanced acc: 0.923168\n",
      "Epoch num: 40 \n",
      "Train loss: 0.002675 \n",
      "Val loss: 0.002076 \n",
      "Val acc: 0.915966\n",
      "Val balanced acc: 0.923168\n",
      "Epoch num: 41 \n",
      "Train loss: 0.002398 \n",
      "Val loss: 0.002092 \n",
      "Val acc: 0.915966\n",
      "Val balanced acc: 0.923168\n",
      "Epoch num: 42 \n",
      "Train loss: 0.003047 \n",
      "Val loss: 0.002075 \n",
      "Val acc: 0.915966\n",
      "Val balanced acc: 0.923168\n",
      "Epoch num: 43 \n",
      "Train loss: 0.002878 \n",
      "Val loss: 0.002054 \n",
      "Val acc: 0.915966\n",
      "Val balanced acc: 0.923168\n",
      "Epoch num: 44 \n",
      "Train loss: 0.002327 \n",
      "Val loss: 0.002077 \n",
      "Val acc: 0.915966\n",
      "Val balanced acc: 0.923168\n",
      "Epoch num: 45 \n",
      "Train loss: 0.002978 \n",
      "Val loss: 0.002071 \n",
      "Val acc: 0.915966\n",
      "Val balanced acc: 0.923168\n",
      "Epoch num: 46 \n",
      "Train loss: 0.002729 \n",
      "Val loss: 0.002079 \n",
      "Val acc: 0.907563\n",
      "Val balanced acc: 0.916223\n",
      "Epoch num: 47 \n",
      "Train loss: 0.002601 \n",
      "Val loss: 0.002102 \n",
      "Val acc: 0.907563\n",
      "Val balanced acc: 0.916223\n",
      "Epoch num: 48 \n",
      "Train loss: 0.002886 \n",
      "Val loss: 0.002141 \n",
      "Val acc: 0.907563\n",
      "Val balanced acc: 0.916223\n",
      "Epoch num: 49 \n",
      "Train loss: 0.002504 \n",
      "Val loss: 0.002175 \n",
      "Val acc: 0.907563\n",
      "Val balanced acc: 0.916223\n",
      "Epoch num: 50 \n",
      "Train loss: 0.002626 \n",
      "Val loss: 0.002190 \n",
      "Val acc: 0.899160\n",
      "Val balanced acc: 0.909279\n",
      "All done!\n",
      "Epoch num: 1 \n",
      "Train loss: 0.015802 \n",
      "Val loss: 0.005252 \n",
      "Val acc: 0.950000\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 2 \n",
      "Train loss: 0.003946 \n",
      "Val loss: 0.002524 \n",
      "Val acc: 0.950000\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 3 \n",
      "Train loss: 0.003851 \n",
      "Val loss: 0.003734 \n",
      "Val acc: 0.950000\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 4 \n",
      "Train loss: 0.007089 \n",
      "Val loss: 0.004731 \n",
      "Val acc: 0.950000\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 5 \n",
      "Train loss: 0.004990 \n",
      "Val loss: 0.004460 \n",
      "Val acc: 0.950000\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 6 \n",
      "Train loss: 0.004394 \n",
      "Val loss: 0.004037 \n",
      "Val acc: 0.950000\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 7 \n",
      "Train loss: 0.002406 \n",
      "Val loss: 0.027804 \n",
      "Val acc: 0.950000\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 8 \n",
      "Train loss: 0.002074 \n",
      "Val loss: 0.011895 \n",
      "Val acc: 0.950000\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 9 \n",
      "Train loss: 0.001377 \n",
      "Val loss: 0.013879 \n",
      "Val acc: 0.950000\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 10 \n",
      "Train loss: 0.002346 \n",
      "Val loss: 0.004993 \n",
      "Val acc: 0.950000\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 11 \n",
      "Train loss: 0.002393 \n",
      "Val loss: 0.004313 \n",
      "Val acc: 0.862500\n",
      "Val balanced acc: 0.690789\n",
      "Epoch num: 12 \n",
      "Train loss: 0.001609 \n",
      "Val loss: 0.007032 \n",
      "Val acc: 0.925000\n",
      "Val balanced acc: 0.486842\n",
      "Epoch num: 13 \n",
      "Train loss: 0.001724 \n",
      "Val loss: 0.007596 \n",
      "Val acc: 0.950000\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 14 \n",
      "Train loss: 0.001767 \n",
      "Val loss: 0.002618 \n",
      "Val acc: 0.950000\n",
      "Val balanced acc: 0.618421\n",
      "Epoch num: 15 \n",
      "Train loss: 0.001210 \n",
      "Val loss: 0.001289 \n",
      "Val acc: 0.962500\n",
      "Val balanced acc: 0.743421\n",
      "Epoch num: 16 \n",
      "Train loss: 0.001806 \n",
      "Val loss: 0.000773 \n",
      "Val acc: 0.962500\n",
      "Val balanced acc: 0.743421\n",
      "Epoch num: 17 \n",
      "Train loss: 0.001188 \n",
      "Val loss: 0.000630 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 18 \n",
      "Train loss: 0.001557 \n",
      "Val loss: 0.000595 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 19 \n",
      "Train loss: 0.001313 \n",
      "Val loss: 0.000472 \n",
      "Val acc: 0.987500\n",
      "Val balanced acc: 0.875000\n",
      "Epoch num: 20 \n",
      "Train loss: 0.001819 \n",
      "Val loss: 0.000463 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 21 \n",
      "Train loss: 0.000972 \n",
      "Val loss: 0.000477 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 22 \n",
      "Train loss: 0.001027 \n",
      "Val loss: 0.000547 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 23 \n",
      "Train loss: 0.001065 \n",
      "Val loss: 0.000608 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 24 \n",
      "Train loss: 0.000915 \n",
      "Val loss: 0.000543 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 25 \n",
      "Train loss: 0.001058 \n",
      "Val loss: 0.000407 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 26 \n",
      "Train loss: 0.001053 \n",
      "Val loss: 0.000309 \n",
      "Val acc: 0.987500\n",
      "Val balanced acc: 0.875000\n",
      "Epoch num: 27 \n",
      "Train loss: 0.001216 \n",
      "Val loss: 0.000294 \n",
      "Val acc: 0.987500\n",
      "Val balanced acc: 0.875000\n",
      "Epoch num: 28 \n",
      "Train loss: 0.001036 \n",
      "Val loss: 0.000363 \n",
      "Val acc: 0.987500\n",
      "Val balanced acc: 0.875000\n",
      "Epoch num: 29 \n",
      "Train loss: 0.000880 \n",
      "Val loss: 0.000498 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 30 \n",
      "Train loss: 0.001084 \n",
      "Val loss: 0.000656 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 31 \n",
      "Train loss: 0.000828 \n",
      "Val loss: 0.000764 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 32 \n",
      "Train loss: 0.000981 \n",
      "Val loss: 0.000715 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 33 \n",
      "Train loss: 0.000696 \n",
      "Val loss: 0.000512 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 34 \n",
      "Train loss: 0.000972 \n",
      "Val loss: 0.000316 \n",
      "Val acc: 1.000000\n",
      "Val balanced acc: 1.000000\n",
      "Epoch num: 35 \n",
      "Train loss: 0.000802 \n",
      "Val loss: 0.000294 \n",
      "Val acc: 1.000000\n",
      "Val balanced acc: 1.000000\n",
      "Epoch num: 36 \n",
      "Train loss: 0.001476 \n",
      "Val loss: 0.000302 \n",
      "Val acc: 1.000000\n",
      "Val balanced acc: 1.000000\n",
      "Epoch num: 37 \n",
      "Train loss: 0.001044 \n",
      "Val loss: 0.000239 \n",
      "Val acc: 1.000000\n",
      "Val balanced acc: 1.000000\n",
      "Epoch num: 38 \n",
      "Train loss: 0.000770 \n",
      "Val loss: 0.000229 \n",
      "Val acc: 1.000000\n",
      "Val balanced acc: 1.000000\n",
      "Epoch num: 39 \n",
      "Train loss: 0.000886 \n",
      "Val loss: 0.000318 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 40 \n",
      "Train loss: 0.001022 \n",
      "Val loss: 0.000529 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 41 \n",
      "Train loss: 0.000594 \n",
      "Val loss: 0.000596 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 42 \n",
      "Train loss: 0.000820 \n",
      "Val loss: 0.000521 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 43 \n",
      "Train loss: 0.000704 \n",
      "Val loss: 0.000446 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 44 \n",
      "Train loss: 0.000756 \n",
      "Val loss: 0.000341 \n",
      "Val acc: 0.987500\n",
      "Val balanced acc: 0.875000\n",
      "Epoch num: 45 \n",
      "Train loss: 0.000676 \n",
      "Val loss: 0.000256 \n",
      "Val acc: 0.987500\n",
      "Val balanced acc: 0.875000\n",
      "Epoch num: 46 \n",
      "Train loss: 0.000504 \n",
      "Val loss: 0.000265 \n",
      "Val acc: 0.987500\n",
      "Val balanced acc: 0.875000\n",
      "Epoch num: 47 \n",
      "Train loss: 0.000721 \n",
      "Val loss: 0.000357 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 48 \n",
      "Train loss: 0.000860 \n",
      "Val loss: 0.000361 \n",
      "Val acc: 0.975000\n",
      "Val balanced acc: 0.750000\n",
      "Epoch num: 49 \n",
      "Train loss: 0.000844 \n",
      "Val loss: 0.000258 \n",
      "Val acc: 0.987500\n",
      "Val balanced acc: 0.875000\n",
      "Epoch num: 50 \n",
      "Train loss: 0.000812 \n",
      "Val loss: 0.000260 \n",
      "Val acc: 0.987500\n",
      "Val balanced acc: 0.875000\n",
      "All done!\n",
      "Epoch num: 1 \n",
      "Train loss: 0.063236 \n",
      "Val loss: 0.027354 \n",
      "Val acc: 0.533333\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 2 \n",
      "Train loss: 0.027409 \n",
      "Val loss: 0.148139 \n",
      "Val acc: 0.533333\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 3 \n",
      "Train loss: 0.014914 \n",
      "Val loss: 0.102042 \n",
      "Val acc: 0.533333\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 4 \n",
      "Train loss: 0.016538 \n",
      "Val loss: 0.039598 \n",
      "Val acc: 0.533333\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 5 \n",
      "Train loss: 0.013548 \n",
      "Val loss: 0.013342 \n",
      "Val acc: 0.641026\n",
      "Val balanced acc: 0.615385\n",
      "Epoch num: 6 \n",
      "Train loss: 0.011721 \n",
      "Val loss: 0.001717 \n",
      "Val acc: 0.943590\n",
      "Val balanced acc: 0.942308\n",
      "Epoch num: 7 \n",
      "Train loss: 0.015223 \n",
      "Val loss: 0.001716 \n",
      "Val acc: 0.917949\n",
      "Val balanced acc: 0.921016\n",
      "Epoch num: 8 \n",
      "Train loss: 0.016966 \n",
      "Val loss: 0.001910 \n",
      "Val acc: 0.943590\n",
      "Val balanced acc: 0.940934\n",
      "Epoch num: 9 \n",
      "Train loss: 0.011631 \n",
      "Val loss: 0.002454 \n",
      "Val acc: 0.902564\n",
      "Val balanced acc: 0.902473\n",
      "Epoch num: 10 \n",
      "Train loss: 0.012487 \n",
      "Val loss: 0.006030 \n",
      "Val acc: 0.810256\n",
      "Val balanced acc: 0.821429\n",
      "Epoch num: 11 \n",
      "Train loss: 0.010300 \n",
      "Val loss: 0.001394 \n",
      "Val acc: 0.964103\n",
      "Val balanced acc: 0.964973\n",
      "Epoch num: 12 \n",
      "Train loss: 0.008348 \n",
      "Val loss: 0.006860 \n",
      "Val acc: 0.779487\n",
      "Val balanced acc: 0.793269\n",
      "Epoch num: 13 \n",
      "Train loss: 0.009203 \n",
      "Val loss: 0.015787 \n",
      "Val acc: 0.651282\n",
      "Val balanced acc: 0.673077\n",
      "Epoch num: 14 \n",
      "Train loss: 0.007857 \n",
      "Val loss: 0.008889 \n",
      "Val acc: 0.733333\n",
      "Val balanced acc: 0.715659\n",
      "Epoch num: 15 \n",
      "Train loss: 0.010032 \n",
      "Val loss: 0.001851 \n",
      "Val acc: 0.912821\n",
      "Val balanced acc: 0.913462\n",
      "Epoch num: 16 \n",
      "Train loss: 0.008868 \n",
      "Val loss: 0.002119 \n",
      "Val acc: 0.917949\n",
      "Val balanced acc: 0.913462\n",
      "Epoch num: 17 \n",
      "Train loss: 0.007986 \n",
      "Val loss: 0.001259 \n",
      "Val acc: 0.953846\n",
      "Val balanced acc: 0.953984\n",
      "Epoch num: 18 \n",
      "Train loss: 0.007492 \n",
      "Val loss: 0.001409 \n",
      "Val acc: 0.928205\n",
      "Val balanced acc: 0.929945\n",
      "Epoch num: 19 \n",
      "Train loss: 0.007503 \n",
      "Val loss: 0.002047 \n",
      "Val acc: 0.912821\n",
      "Val balanced acc: 0.908654\n",
      "Epoch num: 20 \n",
      "Train loss: 0.006895 \n",
      "Val loss: 0.001128 \n",
      "Val acc: 0.958974\n",
      "Val balanced acc: 0.960165\n",
      "Epoch num: 21 \n",
      "Train loss: 0.006063 \n",
      "Val loss: 0.001653 \n",
      "Val acc: 0.943590\n",
      "Val balanced acc: 0.940934\n",
      "Epoch num: 22 \n",
      "Train loss: 0.009942 \n",
      "Val loss: 0.001346 \n",
      "Val acc: 0.953846\n",
      "Val balanced acc: 0.952610\n",
      "Epoch num: 23 \n",
      "Train loss: 0.007276 \n",
      "Val loss: 0.001258 \n",
      "Val acc: 0.953846\n",
      "Val balanced acc: 0.952610\n",
      "Epoch num: 24 \n",
      "Train loss: 0.006928 \n",
      "Val loss: 0.001355 \n",
      "Val acc: 0.948718\n",
      "Val balanced acc: 0.946429\n",
      "Epoch num: 25 \n",
      "Train loss: 0.006808 \n",
      "Val loss: 0.001607 \n",
      "Val acc: 0.938462\n",
      "Val balanced acc: 0.940934\n",
      "Epoch num: 26 \n",
      "Train loss: 0.006418 \n",
      "Val loss: 0.003995 \n",
      "Val acc: 0.887179\n",
      "Val balanced acc: 0.894231\n",
      "Epoch num: 27 \n",
      "Train loss: 0.007171 \n",
      "Val loss: 0.001917 \n",
      "Val acc: 0.917949\n",
      "Val balanced acc: 0.913462\n",
      "Epoch num: 28 \n",
      "Train loss: 0.008542 \n",
      "Val loss: 0.001935 \n",
      "Val acc: 0.923077\n",
      "Val balanced acc: 0.927198\n",
      "Epoch num: 29 \n",
      "Train loss: 0.006821 \n",
      "Val loss: 0.001158 \n",
      "Val acc: 0.938462\n",
      "Val balanced acc: 0.939560\n",
      "Epoch num: 30 \n",
      "Train loss: 0.006956 \n",
      "Val loss: 0.001575 \n",
      "Val acc: 0.938462\n",
      "Val balanced acc: 0.941621\n",
      "Epoch num: 31 \n",
      "Train loss: 0.005277 \n",
      "Val loss: 0.001289 \n",
      "Val acc: 0.953846\n",
      "Val balanced acc: 0.953297\n",
      "Epoch num: 32 \n",
      "Train loss: 0.007810 \n",
      "Val loss: 0.001175 \n",
      "Val acc: 0.958974\n",
      "Val balanced acc: 0.958104\n",
      "Epoch num: 33 \n",
      "Train loss: 0.004413 \n",
      "Val loss: 0.001258 \n",
      "Val acc: 0.948718\n",
      "Val balanced acc: 0.950549\n",
      "Epoch num: 34 \n",
      "Train loss: 0.004965 \n",
      "Val loss: 0.001144 \n",
      "Val acc: 0.948718\n",
      "Val balanced acc: 0.949863\n",
      "Epoch num: 35 \n",
      "Train loss: 0.004479 \n",
      "Val loss: 0.001060 \n",
      "Val acc: 0.958974\n",
      "Val balanced acc: 0.958104\n",
      "Epoch num: 36 \n",
      "Train loss: 0.004442 \n",
      "Val loss: 0.001108 \n",
      "Val acc: 0.964103\n",
      "Val balanced acc: 0.962912\n",
      "Epoch num: 37 \n",
      "Train loss: 0.005563 \n",
      "Val loss: 0.001133 \n",
      "Val acc: 0.958974\n",
      "Val balanced acc: 0.958104\n",
      "Epoch num: 38 \n",
      "Train loss: 0.004347 \n",
      "Val loss: 0.001198 \n",
      "Val acc: 0.953846\n",
      "Val balanced acc: 0.954670\n",
      "Epoch num: 39 \n",
      "Train loss: 0.004763 \n",
      "Val loss: 0.001106 \n",
      "Val acc: 0.958974\n",
      "Val balanced acc: 0.958791\n",
      "Epoch num: 40 \n",
      "Train loss: 0.004163 \n",
      "Val loss: 0.001085 \n",
      "Val acc: 0.964103\n",
      "Val balanced acc: 0.963599\n",
      "Epoch num: 41 \n",
      "Train loss: 0.003997 \n",
      "Val loss: 0.001084 \n",
      "Val acc: 0.964103\n",
      "Val balanced acc: 0.963599\n",
      "Epoch num: 42 \n",
      "Train loss: 0.004200 \n",
      "Val loss: 0.001153 \n",
      "Val acc: 0.964103\n",
      "Val balanced acc: 0.962912\n",
      "Epoch num: 43 \n",
      "Train loss: 0.006257 \n",
      "Val loss: 0.001156 \n",
      "Val acc: 0.958974\n",
      "Val balanced acc: 0.958791\n",
      "Epoch num: 44 \n",
      "Train loss: 0.005684 \n",
      "Val loss: 0.001182 \n",
      "Val acc: 0.964103\n",
      "Val balanced acc: 0.962912\n",
      "Epoch num: 45 \n",
      "Train loss: 0.005474 \n",
      "Val loss: 0.001106 \n",
      "Val acc: 0.958974\n",
      "Val balanced acc: 0.958791\n",
      "Epoch num: 46 \n",
      "Train loss: 0.008899 \n",
      "Val loss: 0.001107 \n",
      "Val acc: 0.953846\n",
      "Val balanced acc: 0.954670\n",
      "Epoch num: 47 \n",
      "Train loss: 0.004071 \n",
      "Val loss: 0.001047 \n",
      "Val acc: 0.964103\n",
      "Val balanced acc: 0.964286\n",
      "Epoch num: 48 \n",
      "Train loss: 0.004208 \n",
      "Val loss: 0.001039 \n",
      "Val acc: 0.964103\n",
      "Val balanced acc: 0.963599\n",
      "Epoch num: 49 \n",
      "Train loss: 0.006280 \n",
      "Val loss: 0.001042 \n",
      "Val acc: 0.958974\n",
      "Val balanced acc: 0.958104\n",
      "Epoch num: 50 \n",
      "Train loss: 0.005138 \n",
      "Val loss: 0.001030 \n",
      "Val acc: 0.964103\n",
      "Val balanced acc: 0.964286\n",
      "All done!\n",
      "Epoch num: 1 \n",
      "Train loss: 0.015839 \n",
      "Val loss: 0.001586 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 2 \n",
      "Train loss: 0.004639 \n",
      "Val loss: 0.004599 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 3 \n",
      "Train loss: 0.003697 \n",
      "Val loss: 0.001454 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 4 \n",
      "Train loss: 0.003662 \n",
      "Val loss: 0.001399 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 5 \n",
      "Train loss: 0.004485 \n",
      "Val loss: 0.000991 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 6 \n",
      "Train loss: 0.003909 \n",
      "Val loss: 0.000422 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 7 \n",
      "Train loss: 0.008863 \n",
      "Val loss: 0.000224 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 8 \n",
      "Train loss: 0.004558 \n",
      "Val loss: 0.000275 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 9 \n",
      "Train loss: 0.005476 \n",
      "Val loss: 0.000340 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 10 \n",
      "Train loss: 0.004355 \n",
      "Val loss: 0.000236 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 11 \n",
      "Train loss: 0.004232 \n",
      "Val loss: 0.000354 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 12 \n",
      "Train loss: 0.003614 \n",
      "Val loss: 0.000247 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 13 \n",
      "Train loss: 0.004261 \n",
      "Val loss: 0.000274 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 14 \n",
      "Train loss: 0.003461 \n",
      "Val loss: 0.000437 \n",
      "Val acc: 0.973077\n",
      "Val balanced acc: 0.490310\n",
      "Epoch num: 15 \n",
      "Train loss: 0.007863 \n",
      "Val loss: 0.000268 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 16 \n",
      "Train loss: 0.003709 \n",
      "Val loss: 0.001794 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 17 \n",
      "Train loss: 0.004825 \n",
      "Val loss: 0.000341 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 18 \n",
      "Train loss: 0.004921 \n",
      "Val loss: 0.000300 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 19 \n",
      "Train loss: 0.003336 \n",
      "Val loss: 0.000441 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 20 \n",
      "Train loss: 0.003516 \n",
      "Val loss: 0.000341 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 21 \n",
      "Train loss: 0.002770 \n",
      "Val loss: 0.000286 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 22 \n",
      "Train loss: 0.002613 \n",
      "Val loss: 0.000264 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 23 \n",
      "Train loss: 0.002620 \n",
      "Val loss: 0.000239 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 24 \n",
      "Train loss: 0.002556 \n",
      "Val loss: 0.000257 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 25 \n",
      "Train loss: 0.002619 \n",
      "Val loss: 0.000259 \n",
      "Val acc: 0.992308\n",
      "Val balanced acc: 0.500000\n",
      "Epoch num: 26 \n",
      "Train loss: 0.003027 \n",
      "Val loss: 0.000272 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 27 \n",
      "Train loss: 0.002577 \n",
      "Val loss: 0.000359 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 28 \n",
      "Train loss: 0.002348 \n",
      "Val loss: 0.000346 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 29 \n",
      "Train loss: 0.002318 \n",
      "Val loss: 0.000279 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 30 \n",
      "Train loss: 0.002213 \n",
      "Val loss: 0.000287 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 31 \n",
      "Train loss: 0.002714 \n",
      "Val loss: 0.000282 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 32 \n",
      "Train loss: 0.002757 \n",
      "Val loss: 0.000284 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 33 \n",
      "Train loss: 0.002143 \n",
      "Val loss: 0.000297 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 34 \n",
      "Train loss: 0.002321 \n",
      "Val loss: 0.000290 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 35 \n",
      "Train loss: 0.002169 \n",
      "Val loss: 0.000302 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 36 \n",
      "Train loss: 0.003831 \n",
      "Val loss: 0.000287 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 37 \n",
      "Train loss: 0.002486 \n",
      "Val loss: 0.000297 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 38 \n",
      "Train loss: 0.002616 \n",
      "Val loss: 0.000287 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 39 \n",
      "Train loss: 0.002161 \n",
      "Val loss: 0.000292 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 40 \n",
      "Train loss: 0.002126 \n",
      "Val loss: 0.000311 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 41 \n",
      "Train loss: 0.002305 \n",
      "Val loss: 0.000298 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 42 \n",
      "Train loss: 0.002381 \n",
      "Val loss: 0.000282 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 43 \n",
      "Train loss: 0.002055 \n",
      "Val loss: 0.000304 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 44 \n",
      "Train loss: 0.002214 \n",
      "Val loss: 0.000302 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 45 \n",
      "Train loss: 0.002245 \n",
      "Val loss: 0.000293 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 46 \n",
      "Train loss: 0.002092 \n",
      "Val loss: 0.000307 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 47 \n",
      "Train loss: 0.002163 \n",
      "Val loss: 0.000288 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 48 \n",
      "Train loss: 0.002085 \n",
      "Val loss: 0.000297 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 49 \n",
      "Train loss: 0.002991 \n",
      "Val loss: 0.000287 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "Epoch num: 50 \n",
      "Train loss: 0.002200 \n",
      "Val loss: 0.000294 \n",
      "Val acc: 0.988462\n",
      "Val balanced acc: 0.498062\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "data_flag = 'pneumoniamnist'\n",
    "\n",
    "info = medmnist.INFO[data_flag].copy()\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "for i in range(num_clients):\n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "    root = os.path.join(part_dir, \"client_\" + str(i))\n",
    "    train_dataset = DataClass(root=os.path.join(part_dir, \"client_\" + str(i)), split='train', download=False)\n",
    "    val_dataset = DataClass(root=os.path.join(part_dir, \"client_\" + str(i)), split='val', download=False)\n",
    "    info = medmnist.INFO[data_flag].copy()\n",
    "    train_dataset.info = info\n",
    "    val_dataset.info = info\n",
    "    train_dataset.info[\"n_samples\"][\"train\"] = train_dataset.imgs.shape[0]\n",
    "    val_dataset.info[\"n_samples\"][\"val\"] = val_dataset.imgs.shape[0]\n",
    "    #TODO NEED TO FIX THIS INFO SAMPLE SIZE ISSUE\n",
    "    # train_dataset.info[\"n_samples\"][\"test\"] = 0\n",
    "\n",
    "    aug_list = []\n",
    "    aug_list.append(transforms.RandomCrop(28, padding=4))\n",
    "    aug_list.append(transforms.RandomHorizontalFlip())\n",
    "    preprocess_list = [transforms.ToTensor(), transforms.Normalize(mean=[.5], std=[.5])]\n",
    "\n",
    "    loc_trainer_test = LocalTrainer(ResNet18(in_channels=n_channels, num_classes=n_classes), DataClass, os.path.join(\"pneumonia_models_dirich\", \"client_\" + str(i)), n_classes, root=root, epochs=50, lr_adj = [25, 40], aug=aug_list, preprocess=preprocess_list, seed=i)\n",
    "    loc_trainer_test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1866,
     "status": "ok",
     "timestamp": 1734447145716,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "PgutoF8_9jJK",
    "outputId": "b6534ef9-7066-4758-c8b1-24a6b032609e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python_class': 'PneumoniaMNIST', 'description': 'The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.', 'url': 'https://zenodo.org/records/10519652/files/pneumoniamnist.npz?download=1', 'MD5': '28209eda62fecd6e6a2d98b1501bb15f', 'url_64': 'https://zenodo.org/records/10519652/files/pneumoniamnist_64.npz?download=1', 'MD5_64': '8f4eceb4ccffa70c672198ea285246c6', 'url_128': 'https://zenodo.org/records/10519652/files/pneumoniamnist_128.npz?download=1', 'MD5_128': '05b46931834c231683c68f40c47b2971', 'url_224': 'https://zenodo.org/records/10519652/files/pneumoniamnist_224.npz?download=1', 'MD5_224': 'd6a3c71de1b945ea11211b03746c1fe1', 'task': 'binary-class', 'label': {'0': 'normal', '1': 'pneumonia'}, 'n_channels': 1, 'n_samples': {'train': 4708, 'val': 524, 'test': 624}, 'license': 'CC BY 4.0'}\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "FedPneu = fedisca.FedISCA(test_data_dir=\"./\", model_weights_dir=\"./pneumonia_models_dirich\", exper_dir=\"./pneumonnia_exper_dirich/federated\", input_size=28, in_channels=n_channels, num_classes=n_classes, batch_size=64, epochs=25, iter_mi=500, lr_steps=[16, 20], log_freq=100, is_medmnist=True, medmnist=\"pneumoniamnist\")\n",
    "ens_local = FedPneu.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4050817,
     "status": "ok",
     "timestamp": 1734456340274,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "b04ad5Va9jJK",
    "outputId": "cb2aaea5-16db-49dd-e9a6-3bf5419abb7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], device='cuda:0')\n",
      "Dataset PneumoniaMNIST of size 28 (pneumoniamnist)\n",
      "    Number of datapoints: 624\n",
      "    Root location: .\n",
      "    Split: test\n",
      "    Task: binary-class\n",
      "    Number of channels: 1\n",
      "    Meaning of labels: {'0': 'normal', '1': 'pneumonia'}\n",
      "    Number of samples: {'train': 4708, 'val': 524, 'test': 624}\n",
      "    Description: The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.\n",
      "    License: CC BY 4.0\n",
      "Generating images...\n",
      "It 0\t Losses: total: 386226.812,\ttarget: 1.175 \tR_feature_loss scaled:\t 386225.594\n",
      "It 100\t Losses: total: 38947.594,\ttarget: 2.255 \tR_feature_loss scaled:\t 38945.328\n",
      "It 200\t Losses: total: 10597.987,\ttarget: 2.807 \tR_feature_loss scaled:\t 10595.174\n",
      "It 300\t Losses: total: 7089.175,\ttarget: 2.522 \tR_feature_loss scaled:\t 7086.646\n",
      "It 400\t Losses: total: 5540.379,\ttarget: 2.516 \tR_feature_loss scaled:\t 5537.857\n",
      "It 500\t Losses: total: 6194.674,\ttarget: 2.604 \tR_feature_loss scaled:\t 6192.063\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.946 | Acc: 75.962% (474/624), B. Acc: 0.682%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 397349.906,\ttarget: 1.165 \tR_feature_loss scaled:\t 397348.719\n",
      "It 100\t Losses: total: 34796.105,\ttarget: 2.253 \tR_feature_loss scaled:\t 34793.844\n",
      "It 200\t Losses: total: 8675.224,\ttarget: 1.736 \tR_feature_loss scaled:\t 8673.480\n",
      "It 300\t Losses: total: 6781.208,\ttarget: 1.896 \tR_feature_loss scaled:\t 6779.307\n",
      "It 400\t Losses: total: 5173.787,\ttarget: 1.721 \tR_feature_loss scaled:\t 5172.060\n",
      "It 500\t Losses: total: 6065.598,\ttarget: 1.625 \tR_feature_loss scaled:\t 6063.967\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.685 | Acc: 79.327% (495/624), B. Acc: 0.727%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 396159.281,\ttarget: 1.118 \tR_feature_loss scaled:\t 396158.125\n",
      "It 100\t Losses: total: 41504.586,\ttarget: 2.228 \tR_feature_loss scaled:\t 41502.348\n",
      "It 200\t Losses: total: 8479.849,\ttarget: 2.204 \tR_feature_loss scaled:\t 8477.639\n",
      "It 300\t Losses: total: 8281.161,\ttarget: 2.293 \tR_feature_loss scaled:\t 8278.862\n",
      "It 400\t Losses: total: 6352.332,\ttarget: 2.016 \tR_feature_loss scaled:\t 6350.310\n",
      "It 500\t Losses: total: 5505.201,\ttarget: 2.338 \tR_feature_loss scaled:\t 5502.857\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.643 | Acc: 79.327% (495/624), B. Acc: 0.729%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 391536.219,\ttarget: 1.171 \tR_feature_loss scaled:\t 391535.031\n",
      "It 100\t Losses: total: 34993.660,\ttarget: 2.323 \tR_feature_loss scaled:\t 34991.328\n",
      "It 200\t Losses: total: 9067.222,\ttarget: 2.224 \tR_feature_loss scaled:\t 9064.991\n",
      "It 300\t Losses: total: 7965.519,\ttarget: 1.923 \tR_feature_loss scaled:\t 7963.590\n",
      "It 400\t Losses: total: 5587.392,\ttarget: 2.227 \tR_feature_loss scaled:\t 5585.159\n",
      "It 500\t Losses: total: 3697.347,\ttarget: 2.167 \tR_feature_loss scaled:\t 3695.173\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.726 | Acc: 74.038% (462/624), B. Acc: 0.656%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 392927.688,\ttarget: 1.145 \tR_feature_loss scaled:\t 392926.500\n",
      "It 100\t Losses: total: 36238.750,\ttarget: 2.815 \tR_feature_loss scaled:\t 36235.926\n",
      "It 200\t Losses: total: 8980.517,\ttarget: 3.055 \tR_feature_loss scaled:\t 8977.455\n",
      "It 300\t Losses: total: 12372.202,\ttarget: 2.451 \tR_feature_loss scaled:\t 12369.745\n",
      "It 400\t Losses: total: 5661.399,\ttarget: 2.621 \tR_feature_loss scaled:\t 5658.772\n",
      "It 500\t Losses: total: 5771.745,\ttarget: 2.525 \tR_feature_loss scaled:\t 5769.214\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.776 | Acc: 75.160% (469/624), B. Acc: 0.671%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 391150.094,\ttarget: 1.152 \tR_feature_loss scaled:\t 391148.906\n",
      "It 100\t Losses: total: 39312.246,\ttarget: 2.900 \tR_feature_loss scaled:\t 39309.336\n",
      "It 200\t Losses: total: 9769.986,\ttarget: 2.634 \tR_feature_loss scaled:\t 9767.346\n",
      "It 300\t Losses: total: 6580.636,\ttarget: 2.551 \tR_feature_loss scaled:\t 6578.079\n",
      "It 400\t Losses: total: 7341.952,\ttarget: 2.577 \tR_feature_loss scaled:\t 7339.369\n",
      "It 500\t Losses: total: 4547.834,\ttarget: 2.337 \tR_feature_loss scaled:\t 4545.491\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.520 | Acc: 82.692% (516/624), B. Acc: 0.774%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 389191.750,\ttarget: 1.129 \tR_feature_loss scaled:\t 389190.594\n",
      "It 100\t Losses: total: 39491.043,\ttarget: 2.682 \tR_feature_loss scaled:\t 39488.352\n",
      "It 200\t Losses: total: 9507.323,\ttarget: 2.308 \tR_feature_loss scaled:\t 9505.009\n",
      "It 300\t Losses: total: 5458.326,\ttarget: 2.303 \tR_feature_loss scaled:\t 5456.016\n",
      "It 400\t Losses: total: 6921.405,\ttarget: 2.645 \tR_feature_loss scaled:\t 6918.755\n",
      "It 500\t Losses: total: 5937.960,\ttarget: 2.401 \tR_feature_loss scaled:\t 5935.554\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.634 | Acc: 77.564% (484/624), B. Acc: 0.705%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 398387.781,\ttarget: 1.135 \tR_feature_loss scaled:\t 398386.625\n",
      "It 100\t Losses: total: 39245.156,\ttarget: 2.311 \tR_feature_loss scaled:\t 39242.836\n",
      "It 200\t Losses: total: 9193.332,\ttarget: 2.602 \tR_feature_loss scaled:\t 9190.724\n",
      "It 300\t Losses: total: 7562.448,\ttarget: 2.415 \tR_feature_loss scaled:\t 7560.026\n",
      "It 400\t Losses: total: 10939.397,\ttarget: 2.672 \tR_feature_loss scaled:\t 10936.720\n",
      "It 500\t Losses: total: 5297.100,\ttarget: 2.353 \tR_feature_loss scaled:\t 5294.741\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.684 | Acc: 75.801% (473/624), B. Acc: 0.680%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 393066.344,\ttarget: 1.142 \tR_feature_loss scaled:\t 393065.156\n",
      "It 100\t Losses: total: 38087.164,\ttarget: 2.503 \tR_feature_loss scaled:\t 38084.652\n",
      "It 200\t Losses: total: 11388.502,\ttarget: 2.455 \tR_feature_loss scaled:\t 11386.040\n",
      "It 300\t Losses: total: 7878.006,\ttarget: 2.693 \tR_feature_loss scaled:\t 7875.307\n",
      "It 400\t Losses: total: 7379.311,\ttarget: 2.289 \tR_feature_loss scaled:\t 7377.016\n",
      "It 500\t Losses: total: 6272.455,\ttarget: 2.077 \tR_feature_loss scaled:\t 6270.372\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.805 | Acc: 73.077% (456/624), B. Acc: 0.644%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 385338.844,\ttarget: 1.146 \tR_feature_loss scaled:\t 385337.656\n",
      "It 100\t Losses: total: 36191.516,\ttarget: 2.807 \tR_feature_loss scaled:\t 36188.699\n",
      "It 200\t Losses: total: 11582.349,\ttarget: 2.585 \tR_feature_loss scaled:\t 11579.757\n",
      "It 300\t Losses: total: 7124.329,\ttarget: 2.486 \tR_feature_loss scaled:\t 7121.836\n",
      "It 400\t Losses: total: 6877.042,\ttarget: 2.524 \tR_feature_loss scaled:\t 6874.513\n",
      "It 500\t Losses: total: 8518.848,\ttarget: 2.383 \tR_feature_loss scaled:\t 8516.459\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.724 | Acc: 75.641% (472/624), B. Acc: 0.676%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 398998.375,\ttarget: 1.148 \tR_feature_loss scaled:\t 398997.188\n",
      "It 100\t Losses: total: 39226.918,\ttarget: 1.738 \tR_feature_loss scaled:\t 39225.172\n",
      "It 200\t Losses: total: 8953.679,\ttarget: 2.224 \tR_feature_loss scaled:\t 8951.448\n",
      "It 300\t Losses: total: 7918.434,\ttarget: 2.162 \tR_feature_loss scaled:\t 7916.266\n",
      "It 400\t Losses: total: 5368.094,\ttarget: 2.254 \tR_feature_loss scaled:\t 5365.834\n",
      "It 500\t Losses: total: 8440.005,\ttarget: 2.372 \tR_feature_loss scaled:\t 8437.627\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.549 | Acc: 81.891% (511/624), B. Acc: 0.761%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 391376.312,\ttarget: 1.159 \tR_feature_loss scaled:\t 391375.125\n",
      "It 100\t Losses: total: 42995.500,\ttarget: 2.488 \tR_feature_loss scaled:\t 42993.000\n",
      "It 200\t Losses: total: 10438.430,\ttarget: 2.078 \tR_feature_loss scaled:\t 10436.346\n",
      "It 300\t Losses: total: 7407.555,\ttarget: 2.168 \tR_feature_loss scaled:\t 7405.381\n",
      "It 400\t Losses: total: 4982.619,\ttarget: 2.036 \tR_feature_loss scaled:\t 4980.576\n",
      "It 500\t Losses: total: 5488.703,\ttarget: 2.086 \tR_feature_loss scaled:\t 5486.612\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.564 | Acc: 79.808% (498/624), B. Acc: 0.734%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 392359.281,\ttarget: 1.174 \tR_feature_loss scaled:\t 392358.062\n",
      "It 100\t Losses: total: 34469.348,\ttarget: 2.706 \tR_feature_loss scaled:\t 34466.633\n",
      "It 200\t Losses: total: 10430.132,\ttarget: 2.240 \tR_feature_loss scaled:\t 10427.885\n",
      "It 300\t Losses: total: 7306.982,\ttarget: 2.210 \tR_feature_loss scaled:\t 7304.766\n",
      "It 400\t Losses: total: 5175.970,\ttarget: 2.257 \tR_feature_loss scaled:\t 5173.707\n",
      "It 500\t Losses: total: 8534.260,\ttarget: 2.343 \tR_feature_loss scaled:\t 8531.911\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.467 | Acc: 83.013% (518/624), B. Acc: 0.777%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 398100.969,\ttarget: 1.138 \tR_feature_loss scaled:\t 398099.812\n",
      "It 100\t Losses: total: 38246.082,\ttarget: 2.408 \tR_feature_loss scaled:\t 38243.664\n",
      "It 200\t Losses: total: 12324.976,\ttarget: 1.748 \tR_feature_loss scaled:\t 12323.221\n",
      "It 300\t Losses: total: 7198.600,\ttarget: 1.998 \tR_feature_loss scaled:\t 7196.596\n",
      "It 400\t Losses: total: 6387.256,\ttarget: 2.030 \tR_feature_loss scaled:\t 6385.220\n",
      "It 500\t Losses: total: 5366.712,\ttarget: 1.805 \tR_feature_loss scaled:\t 5364.901\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.606 | Acc: 80.288% (501/624), B. Acc: 0.739%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 393265.969,\ttarget: 1.141 \tR_feature_loss scaled:\t 393264.812\n",
      "It 100\t Losses: total: 37967.203,\ttarget: 2.459 \tR_feature_loss scaled:\t 37964.734\n",
      "It 200\t Losses: total: 9099.914,\ttarget: 2.446 \tR_feature_loss scaled:\t 9097.461\n",
      "It 300\t Losses: total: 7275.671,\ttarget: 2.472 \tR_feature_loss scaled:\t 7273.194\n",
      "It 400\t Losses: total: 6044.810,\ttarget: 2.461 \tR_feature_loss scaled:\t 6042.343\n",
      "It 500\t Losses: total: 6986.127,\ttarget: 2.258 \tR_feature_loss scaled:\t 6983.862\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "       device='cuda:0')\n",
      "Loss: 0.605 | Acc: 78.045% (487/624), B. Acc: 0.710%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 385133.031,\ttarget: 1.146 \tR_feature_loss scaled:\t 385131.844\n",
      "It 100\t Losses: total: 38226.164,\ttarget: 1.990 \tR_feature_loss scaled:\t 38224.164\n",
      "It 200\t Losses: total: 9103.924,\ttarget: 1.977 \tR_feature_loss scaled:\t 9101.939\n",
      "It 300\t Losses: total: 7132.085,\ttarget: 2.134 \tR_feature_loss scaled:\t 7129.945\n",
      "It 400\t Losses: total: 6944.238,\ttarget: 2.147 \tR_feature_loss scaled:\t 6942.085\n",
      "It 500\t Losses: total: 4495.552,\ttarget: 2.074 \tR_feature_loss scaled:\t 4493.473\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       device='cuda:0')\n",
      "Loss: 0.587 | Acc: 78.205% (488/624), B. Acc: 0.711%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 385328.281,\ttarget: 1.185 \tR_feature_loss scaled:\t 385327.062\n",
      "It 100\t Losses: total: 34661.113,\ttarget: 2.180 \tR_feature_loss scaled:\t 34658.922\n",
      "It 200\t Losses: total: 9461.086,\ttarget: 2.206 \tR_feature_loss scaled:\t 9458.873\n",
      "It 300\t Losses: total: 5733.039,\ttarget: 2.282 \tR_feature_loss scaled:\t 5730.751\n",
      "It 400\t Losses: total: 5256.955,\ttarget: 2.271 \tR_feature_loss scaled:\t 5254.678\n",
      "It 500\t Losses: total: 6296.507,\ttarget: 2.154 \tR_feature_loss scaled:\t 6294.348\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.496 | Acc: 82.692% (516/624), B. Acc: 0.774%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 393745.781,\ttarget: 1.108 \tR_feature_loss scaled:\t 393744.656\n",
      "It 100\t Losses: total: 37685.137,\ttarget: 2.122 \tR_feature_loss scaled:\t 37683.004\n",
      "It 200\t Losses: total: 8810.360,\ttarget: 2.301 \tR_feature_loss scaled:\t 8808.053\n",
      "It 300\t Losses: total: 4816.345,\ttarget: 2.243 \tR_feature_loss scaled:\t 4814.096\n",
      "It 400\t Losses: total: 6829.272,\ttarget: 1.950 \tR_feature_loss scaled:\t 6827.317\n",
      "It 500\t Losses: total: 8205.587,\ttarget: 2.028 \tR_feature_loss scaled:\t 8203.553\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.552 | Acc: 81.250% (507/624), B. Acc: 0.754%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 403389.219,\ttarget: 1.108 \tR_feature_loss scaled:\t 403388.094\n",
      "It 100\t Losses: total: 34907.957,\ttarget: 2.763 \tR_feature_loss scaled:\t 34905.184\n",
      "It 200\t Losses: total: 10167.831,\ttarget: 2.592 \tR_feature_loss scaled:\t 10165.232\n",
      "It 300\t Losses: total: 6785.108,\ttarget: 2.471 \tR_feature_loss scaled:\t 6782.631\n",
      "It 400\t Losses: total: 7010.724,\ttarget: 2.446 \tR_feature_loss scaled:\t 7008.271\n",
      "It 500\t Losses: total: 8780.822,\ttarget: 2.368 \tR_feature_loss scaled:\t 8778.448\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.524 | Acc: 81.571% (509/624), B. Acc: 0.759%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 387445.656,\ttarget: 1.132 \tR_feature_loss scaled:\t 387444.500\n",
      "It 100\t Losses: total: 38928.102,\ttarget: 2.425 \tR_feature_loss scaled:\t 38925.668\n",
      "It 200\t Losses: total: 10263.022,\ttarget: 2.270 \tR_feature_loss scaled:\t 10260.746\n",
      "It 300\t Losses: total: 6519.521,\ttarget: 2.418 \tR_feature_loss scaled:\t 6517.098\n",
      "It 400\t Losses: total: 4741.197,\ttarget: 2.291 \tR_feature_loss scaled:\t 4738.900\n",
      "It 500\t Losses: total: 5028.137,\ttarget: 2.360 \tR_feature_loss scaled:\t 5025.771\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.483 | Acc: 83.173% (519/624), B. Acc: 0.780%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 390673.719,\ttarget: 1.169 \tR_feature_loss scaled:\t 390672.531\n",
      "It 100\t Losses: total: 36047.152,\ttarget: 2.128 \tR_feature_loss scaled:\t 36045.016\n",
      "It 200\t Losses: total: 8511.409,\ttarget: 2.280 \tR_feature_loss scaled:\t 8509.123\n",
      "It 300\t Losses: total: 7317.801,\ttarget: 2.516 \tR_feature_loss scaled:\t 7315.279\n",
      "It 400\t Losses: total: 6064.569,\ttarget: 2.374 \tR_feature_loss scaled:\t 6062.188\n",
      "It 500\t Losses: total: 6949.703,\ttarget: 2.420 \tR_feature_loss scaled:\t 6947.277\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.475 | Acc: 83.013% (518/624), B. Acc: 0.778%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 390320.531,\ttarget: 1.158 \tR_feature_loss scaled:\t 390319.344\n",
      "It 100\t Losses: total: 39046.223,\ttarget: 1.907 \tR_feature_loss scaled:\t 39044.305\n",
      "It 200\t Losses: total: 10087.521,\ttarget: 1.848 \tR_feature_loss scaled:\t 10085.665\n",
      "It 300\t Losses: total: 5452.838,\ttarget: 1.553 \tR_feature_loss scaled:\t 5451.279\n",
      "It 400\t Losses: total: 6049.649,\ttarget: 1.579 \tR_feature_loss scaled:\t 6048.064\n",
      "It 500\t Losses: total: 5466.083,\ttarget: 1.804 \tR_feature_loss scaled:\t 5464.272\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.486 | Acc: 82.692% (516/624), B. Acc: 0.774%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 389265.906,\ttarget: 1.136 \tR_feature_loss scaled:\t 389264.750\n",
      "It 100\t Losses: total: 36741.445,\ttarget: 2.173 \tR_feature_loss scaled:\t 36739.262\n",
      "It 200\t Losses: total: 10213.431,\ttarget: 2.461 \tR_feature_loss scaled:\t 10210.963\n",
      "It 300\t Losses: total: 8353.328,\ttarget: 2.396 \tR_feature_loss scaled:\t 8350.926\n",
      "It 400\t Losses: total: 5349.533,\ttarget: 2.143 \tR_feature_loss scaled:\t 5347.384\n",
      "It 500\t Losses: total: 5104.760,\ttarget: 2.064 \tR_feature_loss scaled:\t 5102.690\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.483 | Acc: 82.853% (517/624), B. Acc: 0.776%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 394074.938,\ttarget: 1.116 \tR_feature_loss scaled:\t 394073.781\n",
      "It 100\t Losses: total: 37449.504,\ttarget: 1.948 \tR_feature_loss scaled:\t 37447.547\n",
      "It 200\t Losses: total: 10141.698,\ttarget: 1.842 \tR_feature_loss scaled:\t 10139.849\n",
      "It 300\t Losses: total: 6064.777,\ttarget: 1.971 \tR_feature_loss scaled:\t 6062.800\n",
      "It 400\t Losses: total: 7396.052,\ttarget: 2.016 \tR_feature_loss scaled:\t 7394.030\n",
      "It 500\t Losses: total: 7612.857,\ttarget: 2.225 \tR_feature_loss scaled:\t 7610.626\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.531 | Acc: 81.250% (507/624), B. Acc: 0.753%\n",
      "Generating images...\n",
      "It 0\t Losses: total: 398241.281,\ttarget: 1.128 \tR_feature_loss scaled:\t 398240.125\n",
      "It 100\t Losses: total: 37800.836,\ttarget: 2.465 \tR_feature_loss scaled:\t 37798.359\n",
      "It 200\t Losses: total: 11481.701,\ttarget: 2.481 \tR_feature_loss scaled:\t 11479.213\n",
      "It 300\t Losses: total: 6316.301,\ttarget: 2.215 \tR_feature_loss scaled:\t 6314.080\n",
      "It 400\t Losses: total: 6465.903,\ttarget: 2.350 \tR_feature_loss scaled:\t 6463.547\n",
      "It 500\t Losses: total: 6373.676,\ttarget: 1.975 \tR_feature_loss scaled:\t 6371.695\n",
      "Training classifier...\n",
      "Test classifier\n",
      "tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0],\n",
      "       device='cuda:0')\n",
      "Loss: 0.468 | Acc: 83.333% (520/624), B. Acc: 0.783%\n",
      "Test ensemble...\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0],\n",
      "       device='cuda:0')\n",
      "Loss: 0.336 | Acc: 87.500% (546/624), B. Acc: 0.844%\n",
      "Test classifier\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1],\n",
      "       device='cuda:0')\n",
      "Loss: 0.471 | Acc: 83.333% (520/624), B. Acc: 0.783%\n"
     ]
    }
   ],
   "source": [
    "fed_model, noise_adapt = FedPneu.model_inversion(ens_local, large_jtr=5, small_jtr=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1734446862502,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "UwrD1JEjFplc"
   },
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "test_data = DataClass(split='test', root=\"./\", transform=test_transforms, download=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1734461699558,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "r5lObZXU9jJK",
    "outputId": "b6050f2b-c8df-4fc6-d0e1-756637d7b183"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-103-9122802a47ef>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fed_model = torch.load(\"pneumonnia_exper_dirich/federated/best.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.473 | Acc: 83.333% (520/624), B. Acc: 78.291%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.3642348647117615, 0.8333333333333334, 0.782905982905983)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "fed_model = torch.load(\"pneumonnia_exper_dirich/federated/best.pth\")\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "test_data = DataClass(split='test', root=\"./\", transform=test_transforms, download=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "FedPneu.test_model(fed_model, test_loader, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 666,
     "status": "ok",
     "timestamp": 1734461747751,
     "user": {
      "displayName": "Caleb Chin",
      "userId": "07678352097565562723"
     },
     "user_tz": 300
    },
    "id": "mgNhSYuw9jJK",
    "outputId": "348d21d2-ebda-497f-ff31-725f3f0276e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-106-afad4c0a4322>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  client_0_weights = torch.load(\"./pneumonia_models_dirich/client_1/best.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.340 | Acc: 88.301% (551/624), B. Acc: 87.051%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.7001046538352966, 0.8830128205128205, 0.8705128205128205)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_0_weights = torch.load(\"./pneumonia_models_dirich/client_1/best.pth\")\n",
    "client_0_model = ResNet18(in_channels=1, num_classes=2)\n",
    "client_0_model.load_state_dict(client_0_weights)\n",
    "client_0_model.to('cuda')\n",
    "FedPneu.test_model(client_0_model, test_loader, nn.CrossEntropyLoss())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "LW2FrWPTUH1A",
    "fnqGSRKeGS3y",
    "RTu_0MbdUqT0"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
